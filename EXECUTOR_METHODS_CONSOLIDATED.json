{
  "metadata": {
    "generated_at": "2025-12-12T03:51:35.282683",
    "source_contracts": "executor_contracts/specialized/*.v3.json",
    "source_signatures": "src/methods_dispensary/*.py",
    "total_contracts": 300,
    "total_unique_classes_in_contracts": 30,
    "total_unique_methods_in_contracts": 240
  },
  "classes": {
    "TextMiningEngine": {
      "file_path": "src/methods_dispensary/analyzer_one.py",
      "line_number": 1609,
      "class_docstring": "Advanced text mining for critical diagnosis.",
      "total_usage_in_300_contracts": 60,
      "unique_questions": 50,
      "questions": [
        "Q001",
        "Q005",
        "Q011",
        "Q021",
        "Q030",
        "Q031",
        "Q035",
        "Q041",
        "Q051",
        "Q060",
        "Q061",
        "Q065",
        "Q071",
        "Q081",
        "Q090",
        "Q091",
        "Q095",
        "Q101",
        "Q111",
        "Q120",
        "Q121",
        "Q125",
        "Q131",
        "Q141",
        "Q150",
        "Q151",
        "Q155",
        "Q161",
        "Q171",
        "Q180",
        "Q181",
        "Q185",
        "Q191",
        "Q201",
        "Q210",
        "Q211",
        "Q215",
        "Q221",
        "Q231",
        "Q240",
        "Q241",
        "Q245",
        "Q251",
        "Q261",
        "Q270",
        "Q271",
        "Q275",
        "Q281",
        "Q291",
        "Q300"
      ],
      "base_slots": [
        "D1-Q1",
        "D1-Q5",
        "D3-Q1",
        "D5-Q1",
        "D6-Q5"
      ],
      "policy_areas": [
        "PA01",
        "PA02",
        "PA03",
        "PA04",
        "PA05",
        "PA06",
        "PA07",
        "PA08",
        "PA09",
        "PA10"
      ],
      "dimensions": [
        "DIM01",
        "DIM03",
        "DIM05",
        "DIM06"
      ],
      "methods": {
        "diagnose_critical_links": {
          "canonical_abbreviation": "text_mining",
          "provides": "text_mining.diagnose_critical_links",
          "role": "diagnose_critical_links_diagnosis",
          "usage_count_in_300": 30,
          "questions": [
            "Q001",
            "Q011",
            "Q030",
            "Q031",
            "Q041",
            "Q060",
            "Q061",
            "Q071",
            "Q090",
            "Q091",
            "Q101",
            "Q120",
            "Q121",
            "Q131",
            "Q150",
            "Q151",
            "Q161",
            "Q180",
            "Q181",
            "Q191",
            "Q210",
            "Q211",
            "Q221",
            "Q240",
            "Q241",
            "Q251",
            "Q270",
            "Q271",
            "Q281",
            "Q300"
          ],
          "base_slots": [
            "D1-Q1",
            "D3-Q1",
            "D6-Q5"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM01",
            "DIM03",
            "DIM06"
          ],
          "signature": {
            "line_number": 1630,
            "is_async": false,
            "parameters": [
              {
                "name": "semantic_cube",
                "type": "dict[str, Any]"
              },
              {
                "name": "performance_analysis",
                "type": "dict[str, Any]"
              }
            ],
            "return_type": "dict[str, Any]",
            "docstring": "Diagnose critical value chain links."
          },
          "epistemological_foundation": {
            "paradigm": "Critical text mining with causal link detection",
            "ontological_basis": "Texts contain latent causal structures that can be detected through linguistic patterns indicating causal relationships (because, therefore, leads to, etc.)",
            "epistemological_stance": "Empirical-interpretive: Knowledge about policy mechanisms emerges from detecting linguistic markers of causality in policy documents",
            "theoretical_framework": [
              "Causal discourse analysis: Texts reveal causal beliefs through specific linguistic constructions (Fairclough, 2003)",
              "Theory of change reconstruction: Policy documents implicitly encode theories of change that can be extracted via causal link detection (Weiss, 1995)"
            ],
            "justification": "Diagnosing critical causal links in baseline diagnostics reveals whether policymakers understand the causal pathways between gender inequalities and their determinants"
          },
          "technical_approach": {
            "method_type": "pattern_based_causal_link_extraction",
            "algorithm": "Multi-pattern regex matching with context window analysis",
            "steps": [
              {
                "step": 1,
                "description": "Identify causal connectors (porque, por lo tanto, conduce a, genera, resulta en)"
              },
              {
                "step": 2,
                "description": "Extract entities before and after connector (cause → effect)"
              },
              {
                "step": 3,
                "description": "Classify link criticality based on proximity to gender indicators"
              }
            ],
            "assumptions": [
              "Causal language reflects causal understanding",
              "Critical links mention gender-related outcomes"
            ],
            "limitations": [
              "Cannot detect implicit causality without linguistic markers",
              "May miss causal relationships expressed across distant sentences"
            ],
            "complexity": "O(n*p) where n=sentences, p=causal patterns"
          },
          "output_interpretation": {
            "output_structure": {
              "critical_links": "List of detected causal links with source/target entities",
              "link_scores": "Criticality scores (0-1) based on relevance to gender outcomes"
            },
            "interpretation_guide": {
              "high_criticality": "≥0.8: Link directly connects to gender inequality outcomes",
              "medium_criticality": "0.5-0.79: Link relates to intermediate factors",
              "low_criticality": "<0.5: Peripheral causal relationship"
            },
            "actionable_insights": [
              "If few critical links found: Diagnosis lacks causal depth, may be purely descriptive",
              "If many links but low criticality: Diagnosis discusses tangential issues, not core gender inequalities"
            ]
          }
        },
        "_analyze_link_text": {
          "canonical_abbreviation": "text_mining",
          "provides": "text_mining.analyze_link_text",
          "role": "_analyze_link_text_analysis",
          "usage_count_in_300": 20,
          "questions": [
            "Q001",
            "Q005",
            "Q031",
            "Q035",
            "Q061",
            "Q065",
            "Q091",
            "Q095",
            "Q121",
            "Q125",
            "Q151",
            "Q155",
            "Q181",
            "Q185",
            "Q211",
            "Q215",
            "Q241",
            "Q245",
            "Q271",
            "Q275"
          ],
          "base_slots": [
            "D1-Q1",
            "D1-Q5"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM01"
          ],
          "signature": {
            "line_number": 1695,
            "is_async": false,
            "parameters": [
              {
                "name": "segments",
                "type": "list[dict]"
              }
            ],
            "return_type": "dict[str, Any]",
            "docstring": "Analyze text content for a link."
          },
          "epistemological_foundation": {
            "paradigm": "Contextual semantic analysis",
            "ontological_basis": "The meaning and validity of a causal claim depends on its textual context (surrounding sentences, semantic coherence)",
            "epistemological_stance": "Coherentist: A causal link is epistemically justified if it coheres with surrounding textual evidence",
            "theoretical_framework": [
              "Semantic coherence theory: Valid claims are embedded in coherent semantic contexts",
              "Evidential reasoning: Context provides supporting or undermining evidence for causal claims"
            ],
            "justification": "Detecting a causal connector alone is insufficient; analyzing surrounding text validates whether the link is substantive or superficial"
          },
          "technical_approach": {
            "method_type": "context_window_semantic_analysis",
            "algorithm": "Extract ±N sentences around link, analyze semantic consistency",
            "steps": [
              {
                "step": 1,
                "description": "Extract context window (±3 sentences) around detected causal link"
              },
              {
                "step": 2,
                "description": "Calculate semantic similarity between link and context using embeddings"
              },
              {
                "step": 3,
                "description": "Identify supporting/contradicting evidence in context"
              }
            ],
            "assumptions": [
              "Coherent contexts indicate valid causal claims",
              "Context window of ±3 sentences captures relevant information"
            ],
            "limitations": [
              "May miss long-range dependencies beyond context window",
              "Semantic similarity doesn't guarantee logical validity"
            ],
            "complexity": "O(k*w) where k=links, w=context window size"
          },
          "output_interpretation": {
            "output_structure": {
              "context_coherence_score": "Semantic coherence metric (0-1)",
              "supporting_evidence": "Contextual sentences that support the link",
              "contradicting_evidence": "Contextual sentences that contradict the link"
            },
            "interpretation_guide": {
              "high_coherence": "≥0.7: Link is well-supported by context",
              "low_coherence": "<0.5: Link may be spurious or poorly contextualized"
            },
            "actionable_insights": [
              "Low coherence + many links: Document has scattered causal claims without substantiation",
              "High coherence: Causal claims are evidence-based and well-argued"
            ]
          }
        },
        "_generate_interventions": {
          "canonical_abbreviation": "text_mining",
          "provides": "text_mining.generate_interventions",
          "role": "_generate_interventions_execution",
          "usage_count_in_300": 10,
          "questions": [
            "Q021",
            "Q051",
            "Q081",
            "Q111",
            "Q141",
            "Q171",
            "Q201",
            "Q231",
            "Q261",
            "Q291"
          ],
          "base_slots": [
            "D5-Q1"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM05"
          ],
          "signature": {
            "line_number": 1762,
            "is_async": false,
            "parameters": [
              {
                "name": "link_name",
                "type": "str"
              },
              {
                "name": "risk_assessment",
                "type": "dict[str, Any]"
              },
              {
                "name": "text_analysis",
                "type": "dict[str, Any]"
              }
            ],
            "return_type": "list[dict[str, str]]",
            "docstring": "Generate intervention recommendations."
          },
          "epistemological_foundation": {
            "paradigm": "TextMiningEngine analytical paradigm",
            "ontological_basis": "Analysis via TextMiningEngine._generate_interventions",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _generate_interventions implements structured analysis for D5-Q1"
            ],
            "justification": "This method contributes to D5-Q1 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "TextMiningEngine._generate_interventions algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _generate_interventions"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _generate_interventions"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _generate_interventions results for downstream analysis"
            ]
          }
        }
      }
    },
    "IndustrialPolicyProcessor": {
      "file_path": "src/methods_dispensary/policy_processor.py",
      "line_number": 662,
      "class_docstring": "\n    State-of-the-art policy plan processor implementing rigorous causal\n    framework analysis with Bayesian evidence scoring and graph-theoretic\n    validation for Colombian local development plans.\n\n    This processor provides core analysis capabilities for policy documents.\n\n    DEPRECATION NOTE: The questionnaire_path parameter is deprecated.\n    Modern pipelines use SPC (Smart Policy Chunks) ingestion which handles\n    questionnaire integration separately.\n    ",
      "total_usage_in_300_contracts": 190,
      "unique_questions": 90,
      "questions": [
        "Q001",
        "Q005",
        "Q007",
        "Q011",
        "Q012",
        "Q013",
        "Q015",
        "Q020",
        "Q022",
        "Q031",
        "Q035",
        "Q037",
        "Q041",
        "Q042",
        "Q043",
        "Q045",
        "Q050",
        "Q052",
        "Q061",
        "Q065",
        "Q067",
        "Q071",
        "Q072",
        "Q073",
        "Q075",
        "Q080",
        "Q082",
        "Q091",
        "Q095",
        "Q097",
        "Q101",
        "Q102",
        "Q103",
        "Q105",
        "Q110",
        "Q112",
        "Q121",
        "Q125",
        "Q127",
        "Q131",
        "Q132",
        "Q133",
        "Q135",
        "Q140",
        "Q142",
        "Q151",
        "Q155",
        "Q157",
        "Q161",
        "Q162",
        "Q163",
        "Q165",
        "Q170",
        "Q172",
        "Q181",
        "Q185",
        "Q187",
        "Q191",
        "Q192",
        "Q193",
        "Q195",
        "Q200",
        "Q202",
        "Q211",
        "Q215",
        "Q217",
        "Q221",
        "Q222",
        "Q223",
        "Q225",
        "Q230",
        "Q232",
        "Q241",
        "Q245",
        "Q247",
        "Q251",
        "Q252",
        "Q253",
        "Q255",
        "Q260",
        "Q262",
        "Q271",
        "Q275",
        "Q277",
        "Q281",
        "Q282",
        "Q283",
        "Q285",
        "Q290",
        "Q292"
      ],
      "base_slots": [
        "D1-Q1",
        "D1-Q5",
        "D2-Q2",
        "D3-Q1",
        "D3-Q2",
        "D3-Q3",
        "D3-Q5",
        "D4-Q5",
        "D5-Q2"
      ],
      "policy_areas": [
        "PA01",
        "PA02",
        "PA03",
        "PA04",
        "PA05",
        "PA06",
        "PA07",
        "PA08",
        "PA09",
        "PA10"
      ],
      "dimensions": [
        "DIM01",
        "DIM02",
        "DIM03",
        "DIM04",
        "DIM05"
      ],
      "methods": {
        "process": {
          "canonical_abbreviation": "industrial_policy",
          "provides": "industrial_policy.process",
          "role": "process_processing",
          "usage_count_in_300": 10,
          "questions": [
            "Q001",
            "Q031",
            "Q061",
            "Q091",
            "Q121",
            "Q151",
            "Q181",
            "Q211",
            "Q241",
            "Q271"
          ],
          "base_slots": [
            "D1-Q1"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM01"
          ],
          "signature": {
            "line_number": 830,
            "is_async": false,
            "parameters": [
              {
                "name": "raw_text",
                "type": "str"
              },
              {
                "name": "**kwargs",
                "type": "Any"
              }
            ],
            "return_type": "dict[str, Any]",
            "docstring": "\n        Execute comprehensive policy plan analysis.\n\n        Args:\n            raw_text: Sanitized policy document text\n            **kwargs: Additional optional parameters (e.g., text, sentences, tables) for compatibility\n\n        Returns:\n            Structured analysis results with evidence bundles and confidence scores\n        "
          },
          "epistemological_foundation": {
            "paradigm": "Structured policy analysis with industrial rigor",
            "ontological_basis": "Policy documents follow recognizable structural patterns (policy pillars, strategic objectives, action lines) that can be systematically extracted",
            "epistemological_stance": "Structuralism: Policy knowledge is organized according to hierarchical structures that determine meaning",
            "theoretical_framework": [
              "Policy architecture theory: Effective policies have clear structural organization",
              "Industrial-grade analysis: Systematic, replicable methods ensure consistency across documents"
            ],
            "justification": "Gender policy baselines must be extracted systematically to ensure comparability across municipalities and policy cycles"
          },
          "technical_approach": {
            "method_type": "hierarchical_pattern_extraction",
            "algorithm": "Multi-level pattern matching with structural validation",
            "steps": [
              {
                "step": 1,
                "description": "Identify policy structure markers (pillar headers, objective numbering)"
              },
              {
                "step": 2,
                "description": "Extract content within each structural segment"
              },
              {
                "step": 3,
                "description": "Match predefined policy patterns (baseline data, targets, indicators)"
              },
              {
                "step": 4,
                "description": "Validate structural completeness and coherence"
              }
            ],
            "assumptions": [
              "Policy documents follow standard Colombian municipal planning structures",
              "Structural markers are consistently used"
            ],
            "limitations": [
              "Fails on highly non-standard document structures",
              "Cannot process purely narrative documents without structural markers"
            ],
            "complexity": "O(n*m) where n=document sections, m=pattern types"
          },
          "output_interpretation": {
            "output_structure": {
              "extracted_segments": "Hierarchical dict of policy structure → content",
              "pattern_matches": "Dict of pattern_type → matched instances",
              "structural_completeness": "Score indicating presence of expected structural elements"
            },
            "interpretation_guide": {
              "complete_structure": "≥0.8: Document has well-defined policy architecture",
              "incomplete_structure": "<0.5: Document lacks standard structure, may be disorganized"
            },
            "actionable_insights": [
              "Incomplete structure: Municipality may lack technical capacity for formal policy design",
              "Complete structure but no baseline data: Structural compliance without substantive content"
            ]
          }
        },
        "_match_patterns_in_sentences": {
          "canonical_abbreviation": "industrial_policy",
          "provides": "industrial_policy.match_patterns_in_sentences",
          "role": "_match_patterns_in_sentences_matching",
          "usage_count_in_300": 10,
          "questions": [
            "Q001",
            "Q031",
            "Q061",
            "Q091",
            "Q121",
            "Q151",
            "Q181",
            "Q211",
            "Q241",
            "Q271"
          ],
          "base_slots": [
            "D1-Q1"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM01"
          ],
          "signature": {
            "line_number": 923,
            "is_async": false,
            "parameters": [
              {
                "name": "compiled_patterns",
                "type": "list"
              },
              {
                "name": "relevant_sentences",
                "type": "list[str]"
              },
              {
                "name": "**kwargs",
                "type": "Any"
              }
            ],
            "return_type": "tuple[list[str], list[int]]",
            "docstring": "\n        Execute pattern matching across relevant sentences and collect matches with positions.\n\n        Args:\n            compiled_patterns: List of compiled regex patterns to match\n            relevant_sentences: Filtered sentences to search within\n            **kwargs: Additional optional parameters for compatibility\n\n        Returns:\n            Tuple of (matched_strings, match_positions)\n        "
          },
          "epistemological_foundation": {
            "paradigm": "Micro-level pattern detection",
            "ontological_basis": "Policy commitments and evidence are encoded at sentence level through specific linguistic patterns",
            "epistemological_stance": "Linguistic realism: Specific phrases reliably indicate policy elements (e.g., 'línea base' indicates baseline data)",
            "theoretical_framework": [
              "Information extraction theory: Structured information can be extracted from unstructured text via pattern recognition"
            ],
            "justification": "Sentence-level granularity ensures precise localization of policy elements for traceability and validation"
          },
          "technical_approach": {
            "method_type": "regex_pattern_matching_per_sentence",
            "algorithm": "Apply pattern registry to each sentence, collect matches with positions",
            "steps": [
              {
                "step": 1,
                "description": "Segment document into sentences"
              },
              {
                "step": 2,
                "description": "For each sentence, apply all relevant regex patterns"
              },
              {
                "step": 3,
                "description": "Record matches with sentence ID, start/end positions, matched text"
              }
            ],
            "assumptions": [
              "Sentence boundaries are correctly detected",
              "Patterns comprehensively cover expected phrasings"
            ],
            "limitations": [
              "Misses patterns spanning multiple sentences",
              "Regex cannot handle complex syntactic variations"
            ],
            "complexity": "O(s*p) where s=sentences, p=patterns"
          },
          "output_interpretation": {
            "output_structure": {
              "sentence_matches": "List of {sentence_id, pattern_id, matched_text, position}"
            },
            "interpretation_guide": {
              "dense_matches": "Many matches per sentence: Rich informational content",
              "sparse_matches": "Few matches: Document may lack expected policy elements"
            },
            "actionable_insights": [
              "Sentences with multiple pattern types (e.g., baseline + source + indicator): High-quality evidence",
              "Matches concentrated in few sentences: Evidence is localized, not systemic"
            ]
          }
        },
        "_extract_point_evidence": {
          "canonical_abbreviation": "industrial_policy",
          "provides": "industrial_policy.extract_point_evidence",
          "role": "_extract_point_evidence_extraction",
          "usage_count_in_300": 10,
          "questions": [
            "Q001",
            "Q031",
            "Q061",
            "Q091",
            "Q121",
            "Q151",
            "Q181",
            "Q211",
            "Q241",
            "Q271"
          ],
          "base_slots": [
            "D1-Q1"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM01"
          ],
          "signature": {
            "line_number": 1178,
            "is_async": false,
            "parameters": [
              {
                "name": "text",
                "type": "str"
              },
              {
                "name": "sentences",
                "type": "list[str]"
              },
              {
                "name": "point_code",
                "type": "str"
              }
            ],
            "return_type": "dict[str, Any]",
            "docstring": "Extract evidence for a specific policy point across all dimensions."
          },
          "epistemological_foundation": {
            "paradigm": "Evidence atomization",
            "ontological_basis": "Complex policy evidence can be decomposed into atomic 'points' (individual data claims, sources, indicators)",
            "epistemological_stance": "Logical atomism: Understanding complex evidence requires breaking it into elementary propositions",
            "theoretical_framework": [
              "Evidence granularity theory: Fine-grained evidence units enable precise validation and reuse"
            ],
            "justification": "Extracting point-level evidence allows detailed auditing and prevents conflating distinct claims"
          },
          "technical_approach": {
            "method_type": "atomic_evidence_extraction",
            "algorithm": "Identify and isolate individual evidence points (numbers, sources, indicators)",
            "steps": [
              {
                "step": 1,
                "description": "Detect evidence units (numeric values, entity names, temporal references)"
              },
              {
                "step": 2,
                "description": "Extract unit with minimal context (subject, predicate)"
              },
              {
                "step": 3,
                "description": "Classify unit type (quantitative_indicator, official_source, temporal_marker)"
              }
            ],
            "assumptions": [
              "Evidence points can be meaningfully isolated from context",
              "Each point has a primary classification"
            ],
            "limitations": [
              "Decontextualization may lose nuance",
              "Complex evidence may not fit atomic model"
            ],
            "complexity": "O(n) where n=detected evidence units"
          },
          "output_interpretation": {
            "output_structure": {
              "evidence_points": "List of {type, value, context_snippet, confidence}"
            },
            "interpretation_guide": {
              "high_point_count": "Rich evidence base",
              "low_point_count": "Sparse evidence"
            },
            "actionable_insights": [
              "Many quantitative points + few sources: Data without provenance",
              "Balanced distribution: Comprehensive evidence base"
            ]
          }
        },
        "_analyze_causal_dimensions": {
          "canonical_abbreviation": "industrial_policy",
          "provides": "industrial_policy.analyze_causal_dimensions",
          "role": "_analyze_causal_dimensions_analysis",
          "usage_count_in_300": 30,
          "questions": [
            "Q005",
            "Q007",
            "Q020",
            "Q035",
            "Q037",
            "Q050",
            "Q065",
            "Q067",
            "Q080",
            "Q095",
            "Q097",
            "Q110",
            "Q125",
            "Q127",
            "Q140",
            "Q155",
            "Q157",
            "Q170",
            "Q185",
            "Q187",
            "Q200",
            "Q215",
            "Q217",
            "Q230",
            "Q245",
            "Q247",
            "Q260",
            "Q275",
            "Q277",
            "Q290"
          ],
          "base_slots": [
            "D1-Q5",
            "D2-Q2",
            "D4-Q5"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM01",
            "DIM02",
            "DIM04"
          ],
          "signature": {
            "line_number": 1217,
            "is_async": false,
            "parameters": [
              {
                "name": "text",
                "type": "str"
              },
              {
                "name": "sentences",
                "type": "list[str] | None",
                "default": "None"
              }
            ],
            "return_type": "dict[str, Any]",
            "docstring": "\n        Perform global analysis of causal dimensions across entire document.\n\n        Args:\n            text: Full document text\n            sentences: Optional pre-segmented sentences. If not provided, will be\n                      automatically extracted from text using the text processor.\n\n        Returns:\n            Dictionary containing dimension scores and confidence metrics\n\n        Note:\n            This function requires 'sentences' for optimal performance. If not provided,\n          "
          },
          "epistemological_foundation": {
            "paradigm": "IndustrialPolicyProcessor analytical paradigm",
            "ontological_basis": "Analysis via IndustrialPolicyProcessor._analyze_causal_dimensions",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _analyze_causal_dimensions implements structured analysis for D1-Q5"
            ],
            "justification": "This method contributes to D1-Q5 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "IndustrialPolicyProcessor._analyze_causal_dimensions algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _analyze_causal_dimensions"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _analyze_causal_dimensions"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _analyze_causal_dimensions results for downstream analysis"
            ]
          }
        },
        "_extract_metadata": {
          "canonical_abbreviation": "industrial_policy",
          "provides": "industrial_policy.extract_metadata",
          "role": "_extract_metadata_extraction",
          "usage_count_in_300": 20,
          "questions": [
            "Q005",
            "Q011",
            "Q035",
            "Q041",
            "Q065",
            "Q071",
            "Q095",
            "Q101",
            "Q125",
            "Q131",
            "Q155",
            "Q161",
            "Q185",
            "Q191",
            "Q215",
            "Q221",
            "Q245",
            "Q251",
            "Q275",
            "Q281"
          ],
          "base_slots": [
            "D1-Q5",
            "D3-Q1"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM01",
            "DIM03"
          ],
          "signature": {
            "line_number": 1281,
            "is_async": false,
            "parameters": [
              {
                "name": "text",
                "type": "str"
              }
            ],
            "return_type": "dict[str, Any]",
            "docstring": "Extract key metadata from policy document header."
          },
          "epistemological_foundation": {
            "paradigm": "IndustrialPolicyProcessor analytical paradigm",
            "ontological_basis": "Analysis via IndustrialPolicyProcessor._extract_metadata",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _extract_metadata implements structured analysis for D1-Q5"
            ],
            "justification": "This method contributes to D1-Q5 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "IndustrialPolicyProcessor._extract_metadata algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _extract_metadata"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _extract_metadata"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _extract_metadata results for downstream analysis"
            ]
          }
        },
        "_calculate_quality_score": {
          "canonical_abbreviation": "industrial_policy",
          "provides": "industrial_policy.calculate_quality_score",
          "role": "_calculate_quality_score_calculation",
          "usage_count_in_300": 20,
          "questions": [
            "Q011",
            "Q012",
            "Q041",
            "Q042",
            "Q071",
            "Q072",
            "Q101",
            "Q102",
            "Q131",
            "Q132",
            "Q161",
            "Q162",
            "Q191",
            "Q192",
            "Q221",
            "Q222",
            "Q251",
            "Q252",
            "Q281",
            "Q282"
          ],
          "base_slots": [
            "D3-Q1",
            "D3-Q2"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM03"
          ],
          "signature": {
            "line_number": 1110,
            "is_async": false,
            "parameters": [
              {
                "name": "dimension_analysis",
                "type": "dict[str, Any]"
              },
              {
                "name": "contradiction_bundle",
                "type": "dict[str, Any]"
              },
              {
                "name": "performance_analysis",
                "type": "dict[str, Any]"
              }
            ],
            "return_type": "QualityScore",
            "docstring": "Aggregate key indicators into a structured QualityScore dataclass."
          },
          "epistemological_foundation": {
            "paradigm": "IndustrialPolicyProcessor analytical paradigm",
            "ontological_basis": "Analysis via IndustrialPolicyProcessor._calculate_quality_score",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _calculate_quality_score implements structured analysis for D3-Q1"
            ],
            "justification": "This method contributes to D3-Q1 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "IndustrialPolicyProcessor._calculate_quality_score algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _calculate_quality_score"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _calculate_quality_score"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _calculate_quality_score results for downstream analysis"
            ]
          }
        },
        "_compile_pattern_registry": {
          "canonical_abbreviation": "industrial_policy",
          "provides": "industrial_policy.compile_pattern_registry",
          "role": "_compile_pattern_registry_execution",
          "usage_count_in_300": 10,
          "questions": [
            "Q012",
            "Q042",
            "Q072",
            "Q102",
            "Q132",
            "Q162",
            "Q192",
            "Q222",
            "Q252",
            "Q282"
          ],
          "base_slots": [
            "D3-Q2"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM03"
          ],
          "signature": {
            "line_number": 772,
            "is_async": false,
            "parameters": [],
            "return_type": "dict[CausalDimension, dict[str, list[re.Pattern]]]",
            "docstring": "Compile all causal patterns into efficient regex objects."
          },
          "epistemological_foundation": {
            "paradigm": "IndustrialPolicyProcessor analytical paradigm",
            "ontological_basis": "Analysis via IndustrialPolicyProcessor._compile_pattern_registry",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _compile_pattern_registry implements structured analysis for D3-Q2"
            ],
            "justification": "This method contributes to D3-Q2 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "IndustrialPolicyProcessor._compile_pattern_registry algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _compile_pattern_registry"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _compile_pattern_registry"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _compile_pattern_registry results for downstream analysis"
            ]
          }
        },
        "_build_point_patterns": {
          "canonical_abbreviation": "industrial_policy",
          "provides": "industrial_policy.build_point_patterns",
          "role": "_build_point_patterns_execution",
          "usage_count_in_300": 10,
          "questions": [
            "Q012",
            "Q042",
            "Q072",
            "Q102",
            "Q132",
            "Q162",
            "Q192",
            "Q222",
            "Q252",
            "Q282"
          ],
          "base_slots": [
            "D3-Q2"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM03"
          ],
          "signature": {
            "line_number": 784,
            "is_async": false,
            "parameters": [],
            "return_type": "None",
            "docstring": "\n        LEGACY: Pattern building from questionnaire disabled.\n\n        This method is kept for backward compatibility but does nothing.\n        Modern SPC pipeline handles question-aware chunking separately.\n        "
          },
          "epistemological_foundation": {
            "paradigm": "IndustrialPolicyProcessor analytical paradigm",
            "ontological_basis": "Analysis via IndustrialPolicyProcessor._build_point_patterns",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _build_point_patterns implements structured analysis for D3-Q2"
            ],
            "justification": "This method contributes to D3-Q2 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "IndustrialPolicyProcessor._build_point_patterns algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _build_point_patterns"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _build_point_patterns"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _build_point_patterns results for downstream analysis"
            ]
          }
        },
        "_empty_result": {
          "canonical_abbreviation": "industrial_policy",
          "provides": "industrial_policy.empty_result",
          "role": "_empty_result_execution",
          "usage_count_in_300": 10,
          "questions": [
            "Q012",
            "Q042",
            "Q072",
            "Q102",
            "Q132",
            "Q162",
            "Q192",
            "Q222",
            "Q252",
            "Q282"
          ],
          "base_slots": [
            "D3-Q2"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM03"
          ],
          "signature": {
            "line_number": 1322,
            "is_async": false,
            "parameters": [],
            "return_type": "dict[str, Any]",
            "docstring": "Return structure for failed/empty processing."
          },
          "epistemological_foundation": {
            "paradigm": "IndustrialPolicyProcessor analytical paradigm",
            "ontological_basis": "Analysis via IndustrialPolicyProcessor._empty_result",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _empty_result implements structured analysis for D3-Q2"
            ],
            "justification": "This method contributes to D3-Q2 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "IndustrialPolicyProcessor._empty_result algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _empty_result"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _empty_result"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _empty_result results for downstream analysis"
            ]
          }
        },
        "_compute_evidence_confidence": {
          "canonical_abbreviation": "industrial_policy",
          "provides": "industrial_policy.compute_evidence_confidence",
          "role": "_compute_evidence_confidence_execution",
          "usage_count_in_300": 20,
          "questions": [
            "Q013",
            "Q022",
            "Q043",
            "Q052",
            "Q073",
            "Q082",
            "Q103",
            "Q112",
            "Q133",
            "Q142",
            "Q163",
            "Q172",
            "Q193",
            "Q202",
            "Q223",
            "Q232",
            "Q253",
            "Q262",
            "Q283",
            "Q292"
          ],
          "base_slots": [
            "D3-Q3",
            "D5-Q2"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM03",
            "DIM05"
          ],
          "signature": {
            "line_number": 948,
            "is_async": false,
            "parameters": [
              {
                "name": "matches",
                "type": "list[str]"
              },
              {
                "name": "text_length",
                "type": "int"
              },
              {
                "name": "pattern_specificity",
                "type": "float"
              },
              {
                "name": "**kwargs",
                "type": "Any"
              }
            ],
            "return_type": "float",
            "docstring": "\n        Calculate confidence score for evidence based on pattern matches and contextual factors.\n\n        Args:\n            matches: List of matched pattern strings\n            text_length: Total length of the document text\n            pattern_specificity: Specificity coefficient for pattern weighting\n            **kwargs: Additional optional parameters for compatibility\n\n        Returns:\n            Computed confidence score\n        "
          },
          "epistemological_foundation": {
            "paradigm": "IndustrialPolicyProcessor analytical paradigm",
            "ontological_basis": "Analysis via IndustrialPolicyProcessor._compute_evidence_confidence",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _compute_evidence_confidence implements structured analysis for D3-Q3"
            ],
            "justification": "This method contributes to D3-Q3 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "IndustrialPolicyProcessor._compute_evidence_confidence algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _compute_evidence_confidence"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _compute_evidence_confidence"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _compute_evidence_confidence results for downstream analysis"
            ]
          }
        },
        "_load_questionnaire": {
          "canonical_abbreviation": "industrial_policy",
          "provides": "industrial_policy.load_questionnaire",
          "role": "_load_questionnaire_execution",
          "usage_count_in_300": 10,
          "questions": [
            "Q015",
            "Q045",
            "Q075",
            "Q105",
            "Q135",
            "Q165",
            "Q195",
            "Q225",
            "Q255",
            "Q285"
          ],
          "base_slots": [
            "D3-Q5"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM03"
          ],
          "signature": {
            "line_number": 758,
            "is_async": false,
            "parameters": [],
            "return_type": "dict[str, Any]",
            "docstring": "\n        LEGACY: Questionnaire loading disabled.\n\n        This method is kept for backward compatibility but returns empty data.\n        Modern SPC pipeline handles questionnaire injection separately.\n        "
          },
          "epistemological_foundation": {
            "paradigm": "IndustrialPolicyProcessor analytical paradigm",
            "ontological_basis": "Analysis via IndustrialPolicyProcessor._load_questionnaire",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _load_questionnaire implements structured analysis for D3-Q5"
            ],
            "justification": "This method contributes to D3-Q5 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "IndustrialPolicyProcessor._load_questionnaire algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _load_questionnaire"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _load_questionnaire"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _load_questionnaire results for downstream analysis"
            ]
          }
        },
        "_compute_avg_confidence": {
          "canonical_abbreviation": "industrial_policy",
          "provides": "industrial_policy.compute_avg_confidence",
          "role": "_compute_avg_confidence_execution",
          "usage_count_in_300": 10,
          "questions": [
            "Q022",
            "Q052",
            "Q082",
            "Q112",
            "Q142",
            "Q172",
            "Q202",
            "Q232",
            "Q262",
            "Q292"
          ],
          "base_slots": [
            "D5-Q2"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM05"
          ],
          "signature": {
            "line_number": 1312,
            "is_async": false,
            "parameters": [
              {
                "name": "dimension_analysis",
                "type": "dict[str, Any]"
              }
            ],
            "return_type": "float",
            "docstring": "Calculate average confidence across all dimensions."
          },
          "epistemological_foundation": {
            "paradigm": "IndustrialPolicyProcessor analytical paradigm",
            "ontological_basis": "Analysis via IndustrialPolicyProcessor._compute_avg_confidence",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _compute_avg_confidence implements structured analysis for D5-Q2"
            ],
            "justification": "This method contributes to D5-Q2 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "IndustrialPolicyProcessor._compute_avg_confidence algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _compute_avg_confidence"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _compute_avg_confidence"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _compute_avg_confidence results for downstream analysis"
            ]
          }
        },
        "_construct_evidence_bundle": {
          "canonical_abbreviation": "industrial_policy",
          "provides": "industrial_policy.construct_evidence_bundle",
          "role": "_construct_evidence_bundle_execution",
          "usage_count_in_300": 10,
          "questions": [
            "Q022",
            "Q052",
            "Q082",
            "Q112",
            "Q142",
            "Q172",
            "Q202",
            "Q232",
            "Q262",
            "Q292"
          ],
          "base_slots": [
            "D5-Q2"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM05"
          ],
          "signature": {
            "line_number": 968,
            "is_async": false,
            "parameters": [
              {
                "name": "dimension",
                "type": "CausalDimension"
              },
              {
                "name": "category",
                "type": "str"
              },
              {
                "name": "matches",
                "type": "list[str]"
              },
              {
                "name": "positions",
                "type": "list[int]"
              },
              {
                "name": "confidence",
                "type": "float"
              },
              {
                "name": "**kwargs",
                "type": "Any"
              }
            ],
            "return_type": "dict[str, Any]",
            "docstring": "\n        Assemble evidence bundle from matched patterns and computed confidence.\n\n        Args:\n            dimension: Causal dimension classification\n            category: Specific category within dimension\n            matches: List of matched pattern strings\n            positions: List of match positions in text\n            confidence: Computed confidence score\n            **kwargs: Additional optional parameters for compatibility\n\n        Returns:\n            Serialized evidence bundle dictio"
          },
          "epistemological_foundation": {
            "paradigm": "IndustrialPolicyProcessor analytical paradigm",
            "ontological_basis": "Analysis via IndustrialPolicyProcessor._construct_evidence_bundle",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _construct_evidence_bundle implements structured analysis for D5-Q2"
            ],
            "justification": "This method contributes to D5-Q2 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "IndustrialPolicyProcessor._construct_evidence_bundle algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _construct_evidence_bundle"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _construct_evidence_bundle"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _construct_evidence_bundle results for downstream analysis"
            ]
          }
        },
        "export_results": {
          "canonical_abbreviation": "industrial_policy",
          "provides": "industrial_policy.export_results",
          "role": "export_results_execution",
          "usage_count_in_300": 10,
          "questions": [
            "Q022",
            "Q052",
            "Q082",
            "Q112",
            "Q142",
            "Q172",
            "Q202",
            "Q232",
            "Q262",
            "Q292"
          ],
          "base_slots": [
            "D5-Q2"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM05"
          ],
          "signature": {
            "line_number": 1338,
            "is_async": false,
            "parameters": [
              {
                "name": "results",
                "type": "dict[str, Any]"
              },
              {
                "name": "output_path",
                "type": "str | Path"
              }
            ],
            "return_type": "None",
            "docstring": "Export analysis results to JSON with formatted output."
          },
          "epistemological_foundation": {
            "paradigm": "IndustrialPolicyProcessor analytical paradigm",
            "ontological_basis": "Analysis via IndustrialPolicyProcessor.export_results",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method export_results implements structured analysis for D5-Q2"
            ],
            "justification": "This method contributes to D5-Q2 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "IndustrialPolicyProcessor.export_results algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute export_results"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from export_results"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use export_results results for downstream analysis"
            ]
          }
        }
      }
    },
    "CausalExtractor": {
      "file_path": "src/methods_dispensary/derek_beach.py",
      "line_number": 967,
      "class_docstring": "Extract and structure causal chains from text",
      "total_usage_in_300_contracts": 250,
      "unique_questions": 110,
      "questions": [
        "Q001",
        "Q005",
        "Q007",
        "Q008",
        "Q015",
        "Q016",
        "Q017",
        "Q021",
        "Q023",
        "Q026",
        "Q030",
        "Q031",
        "Q035",
        "Q037",
        "Q038",
        "Q045",
        "Q046",
        "Q047",
        "Q051",
        "Q053",
        "Q056",
        "Q060",
        "Q061",
        "Q065",
        "Q067",
        "Q068",
        "Q075",
        "Q076",
        "Q077",
        "Q081",
        "Q083",
        "Q086",
        "Q090",
        "Q091",
        "Q095",
        "Q097",
        "Q098",
        "Q105",
        "Q106",
        "Q107",
        "Q111",
        "Q113",
        "Q116",
        "Q120",
        "Q121",
        "Q125",
        "Q127",
        "Q128",
        "Q135",
        "Q136",
        "Q137",
        "Q141",
        "Q143",
        "Q146",
        "Q150",
        "Q151",
        "Q155",
        "Q157",
        "Q158",
        "Q165",
        "Q166",
        "Q167",
        "Q171",
        "Q173",
        "Q176",
        "Q180",
        "Q181",
        "Q185",
        "Q187",
        "Q188",
        "Q195",
        "Q196",
        "Q197",
        "Q201",
        "Q203",
        "Q206",
        "Q210",
        "Q211",
        "Q215",
        "Q217",
        "Q218",
        "Q225",
        "Q226",
        "Q227",
        "Q231",
        "Q233",
        "Q236",
        "Q240",
        "Q241",
        "Q245",
        "Q247",
        "Q248",
        "Q255",
        "Q256",
        "Q257",
        "Q261",
        "Q263",
        "Q266",
        "Q270",
        "Q271",
        "Q275",
        "Q277",
        "Q278",
        "Q285",
        "Q286",
        "Q287",
        "Q291",
        "Q293",
        "Q296",
        "Q300"
      ],
      "base_slots": [
        "D1-Q1",
        "D1-Q5",
        "D2-Q2",
        "D2-Q3",
        "D3-Q5",
        "D4-Q1",
        "D4-Q2",
        "D5-Q1",
        "D5-Q3",
        "D6-Q1",
        "D6-Q5"
      ],
      "policy_areas": [
        "PA01",
        "PA02",
        "PA03",
        "PA04",
        "PA05",
        "PA06",
        "PA07",
        "PA08",
        "PA09",
        "PA10"
      ],
      "dimensions": [
        "DIM01",
        "DIM02",
        "DIM03",
        "DIM04",
        "DIM05",
        "DIM06"
      ],
      "methods": {
        "_extract_goals": {
          "canonical_abbreviation": "causal_extraction",
          "provides": "causal_extraction.extract_goals",
          "role": "_extract_goals_extraction",
          "usage_count_in_300": 20,
          "questions": [
            "Q001",
            "Q016",
            "Q031",
            "Q046",
            "Q061",
            "Q076",
            "Q091",
            "Q106",
            "Q121",
            "Q136",
            "Q151",
            "Q166",
            "Q181",
            "Q196",
            "Q211",
            "Q226",
            "Q241",
            "Q256",
            "Q271",
            "Q286"
          ],
          "base_slots": [
            "D1-Q1",
            "D4-Q1"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM01",
            "DIM04"
          ],
          "signature": {
            "line_number": 999,
            "is_async": false,
            "parameters": [
              {
                "name": "text",
                "type": "str"
              }
            ],
            "return_type": "list[MetaNode]",
            "docstring": "Extract all goals from text"
          },
          "epistemological_foundation": {
            "paradigm": "Teleological policy analysis",
            "ontological_basis": "Policies are goal-directed interventions; goals are explicitly or implicitly stated desired outcomes",
            "epistemological_stance": "Intentionalism: Understanding policy requires identifying intended goals",
            "theoretical_framework": [
              "Means-ends rationality: Policies are structured around goals (ends) and actions (means)",
              "Theory of change: Goals are nodes in causal pathways from inputs to impacts"
            ],
            "justification": "Baseline diagnostics should connect data to goals (e.g., 'reduce VBG by X%'); extracting goals enables evaluating this connection"
          },
          "technical_approach": {
            "method_type": "goal_phrase_extraction",
            "algorithm": "Identify goal markers (reducir, aumentar, lograr) + target entity + quantifier",
            "steps": [
              {
                "step": 1,
                "description": "Detect goal verbs (reducir, incrementar, alcanzar)"
              },
              {
                "step": 2,
                "description": "Extract object of goal verb (what is to be reduced/increased)"
              },
              {
                "step": 3,
                "description": "Extract quantifier if present (percentage, absolute number)"
              }
            ],
            "assumptions": [
              "Goals are expressed with standard verbs",
              "Goals have identifiable objects and quantifiers"
            ],
            "limitations": [
              "Misses implicit goals",
              "May confuse aspirational language with concrete goals"
            ],
            "complexity": "O(n) where n=sentences"
          },
          "output_interpretation": {
            "output_structure": {
              "goals": "List of {goal_verb, target_entity, quantifier, sentence}"
            },
            "interpretation_guide": {
              "quantified_goals": "Goals with numbers are measurable",
              "vague_goals": "Goals without quantifiers are non-measurable"
            },
            "actionable_insights": [
              "Goals without baseline data: Targets lack empirical foundation",
              "Goals aligned with baseline indicators: Evidence-based planning"
            ]
          }
        },
        "_parse_goal_context": {
          "canonical_abbreviation": "causal_extraction",
          "provides": "causal_extraction.parse_goal_context",
          "role": "_parse_goal_context_parsing",
          "usage_count_in_300": 20,
          "questions": [
            "Q001",
            "Q016",
            "Q031",
            "Q046",
            "Q061",
            "Q076",
            "Q091",
            "Q106",
            "Q121",
            "Q136",
            "Q151",
            "Q166",
            "Q181",
            "Q196",
            "Q211",
            "Q226",
            "Q241",
            "Q256",
            "Q271",
            "Q286"
          ],
          "base_slots": [
            "D1-Q1",
            "D4-Q1"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM01",
            "DIM04"
          ],
          "signature": {
            "line_number": 1022,
            "is_async": false,
            "parameters": [
              {
                "name": "goal_id",
                "type": "str"
              },
              {
                "name": "context",
                "type": "str"
              }
            ],
            "return_type": "MetaNode | None",
            "docstring": "Parse goal context to extract structured information"
          },
          "epistemological_foundation": {
            "paradigm": "Contextual goal interpretation",
            "ontological_basis": "Goals are situated in policy contexts that specify conditions, timeframes, and responsible actors",
            "epistemological_stance": "Contextualism: Goal meaning depends on context",
            "theoretical_framework": [
              "Situated policy analysis: Goals must be understood in their institutional and temporal context"
            ],
            "justification": "A goal like 'reduce VBG' is ambiguous without context (by how much? by when? who is responsible?)"
          },
          "technical_approach": {
            "method_type": "context_parsing_around_goals",
            "algorithm": "Extract temporal, spatial, and actor context around identified goals",
            "steps": [
              {
                "step": 1,
                "description": "For each goal, extract surrounding context window"
              },
              {
                "step": 2,
                "description": "Identify temporal markers (plazo, 2024-2027)"
              },
              {
                "step": 3,
                "description": "Identify responsible actors (Secretaría de...)"
              },
              {
                "step": 4,
                "description": "Identify spatial scope (municipal, rural, etc.)"
              }
            ],
            "assumptions": [
              "Context is proximal to goal statement",
              "Context markers use standard terminology"
            ],
            "limitations": [
              "May miss context stated in distant document sections",
              "Cannot infer implicit context"
            ],
            "complexity": "O(g*w) where g=goals, w=context window"
          },
          "output_interpretation": {
            "output_structure": {
              "goal_contexts": "Dict mapping goal_id → {temporal, spatial, actor} context"
            },
            "interpretation_guide": {
              "complete_context": "All three dimensions present: Well-specified goal",
              "incomplete_context": "Missing dimensions: Ambiguous goal"
            },
            "actionable_insights": [
              "Goals without temporal context: No deadline, low accountability",
              "Goals without responsible actors: Diffused responsibility"
            ]
          }
        },
        "_assess_temporal_coherence": {
          "canonical_abbreviation": "causal_extraction",
          "provides": "causal_extraction.assess_temporal_coherence",
          "role": "_assess_temporal_coherence_execution",
          "usage_count_in_300": 30,
          "questions": [
            "Q005",
            "Q021",
            "Q030",
            "Q035",
            "Q051",
            "Q060",
            "Q065",
            "Q081",
            "Q090",
            "Q095",
            "Q111",
            "Q120",
            "Q125",
            "Q141",
            "Q150",
            "Q155",
            "Q171",
            "Q180",
            "Q185",
            "Q201",
            "Q210",
            "Q215",
            "Q231",
            "Q240",
            "Q245",
            "Q261",
            "Q270",
            "Q275",
            "Q291",
            "Q300"
          ],
          "base_slots": [
            "D1-Q5",
            "D5-Q1",
            "D6-Q5"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM01",
            "DIM05",
            "DIM06"
          ],
          "signature": {
            "line_number": 1471,
            "is_async": false,
            "parameters": [
              {
                "name": "source",
                "type": "str"
              },
              {
                "name": "target",
                "type": "str"
              }
            ],
            "return_type": "float",
            "docstring": "Assess temporal coherence based on verb sequences"
          },
          "epistemological_foundation": {
            "paradigm": "CausalExtractor analytical paradigm",
            "ontological_basis": "Analysis via CausalExtractor._assess_temporal_coherence",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _assess_temporal_coherence implements structured analysis for D1-Q5"
            ],
            "justification": "This method contributes to D1-Q5 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "CausalExtractor._assess_temporal_coherence algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _assess_temporal_coherence"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _assess_temporal_coherence"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _assess_temporal_coherence results for downstream analysis"
            ]
          }
        },
        "extract_causal_hierarchy": {
          "canonical_abbreviation": "causal_extraction",
          "provides": "causal_extraction.extract_causal_hierarchy",
          "role": "extract_causal_hierarchy_extraction",
          "usage_count_in_300": 30,
          "questions": [
            "Q007",
            "Q017",
            "Q026",
            "Q037",
            "Q047",
            "Q056",
            "Q067",
            "Q077",
            "Q086",
            "Q097",
            "Q107",
            "Q116",
            "Q127",
            "Q137",
            "Q146",
            "Q157",
            "Q167",
            "Q176",
            "Q187",
            "Q197",
            "Q206",
            "Q217",
            "Q227",
            "Q236",
            "Q247",
            "Q257",
            "Q266",
            "Q277",
            "Q287",
            "Q296"
          ],
          "base_slots": [
            "D2-Q2",
            "D4-Q2",
            "D6-Q1"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM02",
            "DIM04",
            "DIM06"
          ],
          "signature": {
            "line_number": 979,
            "is_async": false,
            "parameters": [
              {
                "name": "text",
                "type": "str"
              }
            ],
            "return_type": "nx.DiGraph",
            "docstring": "Extract complete causal hierarchy from text"
          },
          "epistemological_foundation": {
            "paradigm": "CausalExtractor analytical paradigm",
            "ontological_basis": "Analysis via CausalExtractor.extract_causal_hierarchy",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method extract_causal_hierarchy implements structured analysis for D2-Q2"
            ],
            "justification": "This method contributes to D2-Q2 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "CausalExtractor.extract_causal_hierarchy algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute extract_causal_hierarchy"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from extract_causal_hierarchy"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use extract_causal_hierarchy results for downstream analysis"
            ]
          }
        },
        "_extract_causal_links": {
          "canonical_abbreviation": "causal_extraction",
          "provides": "causal_extraction.extract_causal_links",
          "role": "_extract_causal_links_extraction",
          "usage_count_in_300": 10,
          "questions": [
            "Q008",
            "Q038",
            "Q068",
            "Q098",
            "Q128",
            "Q158",
            "Q188",
            "Q218",
            "Q248",
            "Q278"
          ],
          "base_slots": [
            "D2-Q3"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM02"
          ],
          "signature": {
            "line_number": 1120,
            "is_async": false,
            "parameters": [
              {
                "name": "text",
                "type": "str"
              }
            ],
            "return_type": "None",
            "docstring": "\n        AGUJA I: El Prior Informado Adaptativo\n        Extract causal links using Bayesian inference with adaptive priors\n        "
          },
          "epistemological_foundation": {
            "paradigm": "CausalExtractor analytical paradigm",
            "ontological_basis": "Analysis via CausalExtractor._extract_causal_links",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _extract_causal_links implements structured analysis for D2-Q3"
            ],
            "justification": "This method contributes to D2-Q3 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "CausalExtractor._extract_causal_links algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _extract_causal_links"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _extract_causal_links"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _extract_causal_links results for downstream analysis"
            ]
          }
        },
        "_calculate_composite_likelihood": {
          "canonical_abbreviation": "causal_extraction",
          "provides": "causal_extraction.calculate_composite_likelihood",
          "role": "_calculate_composite_likelihood_calculation",
          "usage_count_in_300": 20,
          "questions": [
            "Q008",
            "Q016",
            "Q038",
            "Q046",
            "Q068",
            "Q076",
            "Q098",
            "Q106",
            "Q128",
            "Q136",
            "Q158",
            "Q166",
            "Q188",
            "Q196",
            "Q218",
            "Q226",
            "Q248",
            "Q256",
            "Q278",
            "Q286"
          ],
          "base_slots": [
            "D2-Q3",
            "D4-Q1"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM02",
            "DIM04"
          ],
          "signature": {
            "line_number": 1570,
            "is_async": false,
            "parameters": [
              {
                "name": "evidence",
                "type": "dict[str, Any]"
              }
            ],
            "return_type": "float",
            "docstring": "Calculate composite likelihood from multiple evidence components\n\n        Enhanced with:\n        - Nonlinear transformation rewarding triangulation\n        - Evidence diversity verification across analytical domains\n        "
          },
          "epistemological_foundation": {
            "paradigm": "CausalExtractor analytical paradigm",
            "ontological_basis": "Analysis via CausalExtractor._calculate_composite_likelihood",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _calculate_composite_likelihood implements structured analysis for D2-Q3"
            ],
            "justification": "This method contributes to D2-Q3 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "CausalExtractor._calculate_composite_likelihood algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _calculate_composite_likelihood"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _calculate_composite_likelihood"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _calculate_composite_likelihood results for downstream analysis"
            ]
          }
        },
        "_initialize_prior": {
          "canonical_abbreviation": "causal_extraction",
          "provides": "causal_extraction.initialize_prior",
          "role": "_initialize_prior_execution",
          "usage_count_in_300": 10,
          "questions": [
            "Q008",
            "Q038",
            "Q068",
            "Q098",
            "Q128",
            "Q158",
            "Q188",
            "Q218",
            "Q248",
            "Q278"
          ],
          "base_slots": [
            "D2-Q3"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM02"
          ],
          "signature": {
            "line_number": 1551,
            "is_async": false,
            "parameters": [
              {
                "name": "source",
                "type": "str"
              },
              {
                "name": "target",
                "type": "str"
              }
            ],
            "return_type": "tuple[float, float, float]",
            "docstring": "Initialize prior distribution for causal link"
          },
          "epistemological_foundation": {
            "paradigm": "CausalExtractor analytical paradigm",
            "ontological_basis": "Analysis via CausalExtractor._initialize_prior",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _initialize_prior implements structured analysis for D2-Q3"
            ],
            "justification": "This method contributes to D2-Q3 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "CausalExtractor._initialize_prior algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _initialize_prior"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _initialize_prior"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _initialize_prior results for downstream analysis"
            ]
          }
        },
        "_calculate_type_transition_prior": {
          "canonical_abbreviation": "causal_extraction",
          "provides": "causal_extraction.calculate_type_transition_prior",
          "role": "_calculate_type_transition_prior_calculation",
          "usage_count_in_300": 20,
          "questions": [
            "Q008",
            "Q015",
            "Q038",
            "Q045",
            "Q068",
            "Q075",
            "Q098",
            "Q105",
            "Q128",
            "Q135",
            "Q158",
            "Q165",
            "Q188",
            "Q195",
            "Q218",
            "Q225",
            "Q248",
            "Q255",
            "Q278",
            "Q285"
          ],
          "base_slots": [
            "D2-Q3",
            "D3-Q5"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM02",
            "DIM03"
          ],
          "signature": {
            "line_number": 1295,
            "is_async": false,
            "parameters": [
              {
                "name": "source",
                "type": "str"
              },
              {
                "name": "target",
                "type": "str"
              }
            ],
            "return_type": "float",
            "docstring": "Calculate prior based on historical transition frequencies between goal types"
          },
          "epistemological_foundation": {
            "paradigm": "CausalExtractor analytical paradigm",
            "ontological_basis": "Analysis via CausalExtractor._calculate_type_transition_prior",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _calculate_type_transition_prior implements structured analysis for D2-Q3"
            ],
            "justification": "This method contributes to D2-Q3 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "CausalExtractor._calculate_type_transition_prior algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _calculate_type_transition_prior"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _calculate_type_transition_prior"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _calculate_type_transition_prior results for downstream analysis"
            ]
          }
        },
        "_assess_financial_consistency": {
          "canonical_abbreviation": "causal_extraction",
          "provides": "causal_extraction.assess_financial_consistency",
          "role": "_assess_financial_consistency_execution",
          "usage_count_in_300": 10,
          "questions": [
            "Q015",
            "Q045",
            "Q075",
            "Q105",
            "Q135",
            "Q165",
            "Q195",
            "Q225",
            "Q255",
            "Q285"
          ],
          "base_slots": [
            "D3-Q5"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM03"
          ],
          "signature": {
            "line_number": 1503,
            "is_async": false,
            "parameters": [
              {
                "name": "source",
                "type": "str"
              },
              {
                "name": "target",
                "type": "str"
              }
            ],
            "return_type": "float",
            "docstring": "Assess financial alignment between connected nodes"
          },
          "epistemological_foundation": {
            "paradigm": "CausalExtractor analytical paradigm",
            "ontological_basis": "Analysis via CausalExtractor._assess_financial_consistency",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _assess_financial_consistency implements structured analysis for D3-Q5"
            ],
            "justification": "This method contributes to D3-Q5 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "CausalExtractor._assess_financial_consistency algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _assess_financial_consistency"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _assess_financial_consistency"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _assess_financial_consistency results for downstream analysis"
            ]
          }
        },
        "_build_type_hierarchy": {
          "canonical_abbreviation": "causal_extraction",
          "provides": "causal_extraction.build_type_hierarchy",
          "role": "_build_type_hierarchy_execution",
          "usage_count_in_300": 10,
          "questions": [
            "Q015",
            "Q045",
            "Q075",
            "Q105",
            "Q135",
            "Q165",
            "Q195",
            "Q225",
            "Q255",
            "Q285"
          ],
          "base_slots": [
            "D3-Q5"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM03"
          ],
          "signature": {
            "line_number": 1630,
            "is_async": false,
            "parameters": [],
            "return_type": "None",
            "docstring": "Build hierarchy based on goal types"
          },
          "epistemological_foundation": {
            "paradigm": "CausalExtractor analytical paradigm",
            "ontological_basis": "Analysis via CausalExtractor._build_type_hierarchy",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _build_type_hierarchy implements structured analysis for D3-Q5"
            ],
            "justification": "This method contributes to D3-Q5 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "CausalExtractor._build_type_hierarchy algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _build_type_hierarchy"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _build_type_hierarchy"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _build_type_hierarchy results for downstream analysis"
            ]
          }
        },
        "_check_structural_violation": {
          "canonical_abbreviation": "causal_extraction",
          "provides": "causal_extraction.check_structural_violation",
          "role": "_check_structural_violation_execution",
          "usage_count_in_300": 10,
          "questions": [
            "Q015",
            "Q045",
            "Q075",
            "Q105",
            "Q135",
            "Q165",
            "Q195",
            "Q225",
            "Q255",
            "Q285"
          ],
          "base_slots": [
            "D3-Q5"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM03"
          ],
          "signature": {
            "line_number": 1319,
            "is_async": false,
            "parameters": [
              {
                "name": "source",
                "type": "str"
              },
              {
                "name": "target",
                "type": "str"
              }
            ],
            "return_type": "str | None",
            "docstring": "\n        AUDIT POINT 2.1: Structural Veto (D6-Q2)\n\n        Check if causal link violates structural hierarchy based on TeoriaCambio axioms.\n        Implements set-theoretic constraints per Goertz & Mahoney 2012.\n\n        Returns:\n            None if link is valid, otherwise a string describing the violation\n        "
          },
          "epistemological_foundation": {
            "paradigm": "CausalExtractor analytical paradigm",
            "ontological_basis": "Analysis via CausalExtractor._check_structural_violation",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _check_structural_violation implements structured analysis for D3-Q5"
            ],
            "justification": "This method contributes to D3-Q5 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "CausalExtractor._check_structural_violation algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _check_structural_violation"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _check_structural_violation"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _check_structural_violation results for downstream analysis"
            ]
          }
        },
        "_calculate_textual_proximity": {
          "canonical_abbreviation": "causal_extraction",
          "provides": "causal_extraction.calculate_textual_proximity",
          "role": "_calculate_textual_proximity_calculation",
          "usage_count_in_300": 10,
          "questions": [
            "Q015",
            "Q045",
            "Q075",
            "Q105",
            "Q135",
            "Q165",
            "Q195",
            "Q225",
            "Q255",
            "Q285"
          ],
          "base_slots": [
            "D3-Q5"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM03"
          ],
          "signature": {
            "line_number": 1528,
            "is_async": false,
            "parameters": [
              {
                "name": "source",
                "type": "str"
              },
              {
                "name": "target",
                "type": "str"
              },
              {
                "name": "text",
                "type": "str"
              }
            ],
            "return_type": "float",
            "docstring": "Calculate how often node IDs appear together in text windows"
          },
          "epistemological_foundation": {
            "paradigm": "CausalExtractor analytical paradigm",
            "ontological_basis": "Analysis via CausalExtractor._calculate_textual_proximity",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _calculate_textual_proximity implements structured analysis for D3-Q5"
            ],
            "justification": "This method contributes to D3-Q5 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "CausalExtractor._calculate_textual_proximity algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _calculate_textual_proximity"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _calculate_textual_proximity"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _calculate_textual_proximity results for downstream analysis"
            ]
          }
        },
        "_calculate_language_specificity": {
          "canonical_abbreviation": "causal_extraction",
          "provides": "causal_extraction.calculate_language_specificity",
          "role": "_calculate_language_specificity_calculation",
          "usage_count_in_300": 20,
          "questions": [
            "Q016",
            "Q030",
            "Q046",
            "Q060",
            "Q076",
            "Q090",
            "Q106",
            "Q120",
            "Q136",
            "Q150",
            "Q166",
            "Q180",
            "Q196",
            "Q210",
            "Q226",
            "Q240",
            "Q256",
            "Q270",
            "Q286",
            "Q300"
          ],
          "base_slots": [
            "D4-Q1",
            "D6-Q5"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM04",
            "DIM06"
          ],
          "signature": {
            "line_number": 1360,
            "is_async": false,
            "parameters": [
              {
                "name": "keyword",
                "type": "str"
              },
              {
                "name": "policy_area",
                "type": "str | None",
                "default": "None"
              },
              {
                "name": "context",
                "type": "str | None",
                "default": "None"
              }
            ],
            "return_type": "float",
            "docstring": "Assess specificity of causal language (epistemic certainty)\n\n        Harmonic Front 3 - Enhancement 4: Language Specificity Assessment\n        Enhanced to check policy-specific vocabulary (patrones_verificacion) for current\n        Policy Area (P1–P10), not just generic causal keywords.\n\n        For D6-Q5 (Contextual/Differential Focus): rewards use of specialized terminology\n        that anchors intervention in social/cultural context (e.g., \"catastro multipropósito\",\n        \"reparación integr"
          },
          "epistemological_foundation": {
            "paradigm": "CausalExtractor analytical paradigm",
            "ontological_basis": "Analysis via CausalExtractor._calculate_language_specificity",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _calculate_language_specificity implements structured analysis for D4-Q1"
            ],
            "justification": "This method contributes to D4-Q1 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "CausalExtractor._calculate_language_specificity algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _calculate_language_specificity"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _calculate_language_specificity"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _calculate_language_specificity results for downstream analysis"
            ]
          }
        },
        "_calculate_semantic_distance": {
          "canonical_abbreviation": "causal_extraction",
          "provides": "causal_extraction.calculate_semantic_distance",
          "role": "_calculate_semantic_distance_calculation",
          "usage_count_in_300": 20,
          "questions": [
            "Q016",
            "Q023",
            "Q046",
            "Q053",
            "Q076",
            "Q083",
            "Q106",
            "Q113",
            "Q136",
            "Q143",
            "Q166",
            "Q173",
            "Q196",
            "Q203",
            "Q226",
            "Q233",
            "Q256",
            "Q263",
            "Q286",
            "Q293"
          ],
          "base_slots": [
            "D4-Q1",
            "D5-Q3"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM04",
            "DIM05"
          ],
          "signature": {
            "line_number": 1256,
            "is_async": false,
            "parameters": [
              {
                "name": "source",
                "type": "str"
              },
              {
                "name": "target",
                "type": "str"
              }
            ],
            "return_type": "float",
            "docstring": "\n        Calculate semantic distance between nodes using spaCy embeddings\n\n        PERFORMANCE NOTE: This method can be optimized with:\n        1. Vectorized operations using numpy for batch processing\n        2. Embedding caching to avoid recomputing spaCy vectors\n        3. Async processing for large documents with many nodes\n        4. Alternative: BERT/transformer embeddings for higher fidelity (SOTA)\n\n        Current implementation prioritizes determinism over speed.\n        Enable performa"
          },
          "epistemological_foundation": {
            "paradigm": "CausalExtractor analytical paradigm",
            "ontological_basis": "Analysis via CausalExtractor._calculate_semantic_distance",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _calculate_semantic_distance implements structured analysis for D4-Q1"
            ],
            "justification": "This method contributes to D4-Q1 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "CausalExtractor._calculate_semantic_distance algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _calculate_semantic_distance"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _calculate_semantic_distance"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _calculate_semantic_distance results for downstream analysis"
            ]
          }
        },
        "_classify_goal_type": {
          "canonical_abbreviation": "causal_extraction",
          "provides": "causal_extraction.classify_goal_type",
          "role": "_classify_goal_type_execution",
          "usage_count_in_300": 10,
          "questions": [
            "Q016",
            "Q046",
            "Q076",
            "Q106",
            "Q136",
            "Q166",
            "Q196",
            "Q226",
            "Q256",
            "Q286"
          ],
          "base_slots": [
            "D4-Q1"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM04"
          ],
          "signature": {
            "line_number": 1688,
            "is_async": false,
            "parameters": [
              {
                "name": "text",
                "type": "str"
              }
            ],
            "return_type": "str",
            "docstring": "\n        Classify the type of a goal based on its text.\n\n        Args:\n            text: Goal text to classify\n\n        Returns:\n            Goal type (programa, producto, resultado, impacto)\n        "
          },
          "epistemological_foundation": {
            "paradigm": "CausalExtractor analytical paradigm",
            "ontological_basis": "Analysis via CausalExtractor._classify_goal_type",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _classify_goal_type implements structured analysis for D4-Q1"
            ],
            "justification": "This method contributes to D4-Q1 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "CausalExtractor._classify_goal_type algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _classify_goal_type"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _classify_goal_type"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _classify_goal_type results for downstream analysis"
            ]
          }
        }
      }
    },
    "FinancialAuditor": {
      "file_path": "src/methods_dispensary/derek_beach.py",
      "line_number": 1908,
      "class_docstring": "Financial traceability and auditing",
      "total_usage_in_300_contracts": 140,
      "unique_questions": 80,
      "questions": [
        "Q001",
        "Q002",
        "Q003",
        "Q006",
        "Q012",
        "Q013",
        "Q019",
        "Q022",
        "Q031",
        "Q032",
        "Q033",
        "Q036",
        "Q042",
        "Q043",
        "Q049",
        "Q052",
        "Q061",
        "Q062",
        "Q063",
        "Q066",
        "Q072",
        "Q073",
        "Q079",
        "Q082",
        "Q091",
        "Q092",
        "Q093",
        "Q096",
        "Q102",
        "Q103",
        "Q109",
        "Q112",
        "Q121",
        "Q122",
        "Q123",
        "Q126",
        "Q132",
        "Q133",
        "Q139",
        "Q142",
        "Q151",
        "Q152",
        "Q153",
        "Q156",
        "Q162",
        "Q163",
        "Q169",
        "Q172",
        "Q181",
        "Q182",
        "Q183",
        "Q186",
        "Q192",
        "Q193",
        "Q199",
        "Q202",
        "Q211",
        "Q212",
        "Q213",
        "Q216",
        "Q222",
        "Q223",
        "Q229",
        "Q232",
        "Q241",
        "Q242",
        "Q243",
        "Q246",
        "Q252",
        "Q253",
        "Q259",
        "Q262",
        "Q271",
        "Q272",
        "Q273",
        "Q276",
        "Q282",
        "Q283",
        "Q289",
        "Q292"
      ],
      "base_slots": [
        "D1-Q1",
        "D1-Q2",
        "D1-Q3",
        "D2-Q1",
        "D3-Q2",
        "D3-Q3",
        "D4-Q4",
        "D5-Q2"
      ],
      "policy_areas": [
        "PA01",
        "PA02",
        "PA03",
        "PA04",
        "PA05",
        "PA06",
        "PA07",
        "PA08",
        "PA09",
        "PA10"
      ],
      "dimensions": [
        "DIM01",
        "DIM02",
        "DIM03",
        "DIM04",
        "DIM05"
      ],
      "methods": {
        "_parse_amount": {
          "canonical_abbreviation": "financial_audit",
          "provides": "financial_audit.parse_amount",
          "role": "_parse_amount_parsing",
          "usage_count_in_300": 10,
          "questions": [
            "Q001",
            "Q031",
            "Q061",
            "Q091",
            "Q121",
            "Q151",
            "Q181",
            "Q211",
            "Q241",
            "Q271"
          ],
          "base_slots": [
            "D1-Q1"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM01"
          ],
          "signature": {
            "line_number": 2024,
            "is_async": false,
            "parameters": [
              {
                "name": "value",
                "type": "Any"
              }
            ],
            "return_type": "float | None",
            "docstring": "Parse monetary amount from various formats"
          },
          "epistemological_foundation": {
            "paradigm": "Financial data extraction and normalization",
            "ontological_basis": "Financial commitments are expressed as monetary amounts in various formats (text, numbers, currencies)",
            "epistemological_stance": "Representationalism: Textual representations of amounts map to objective financial realities",
            "theoretical_framework": [
              "Financial accountability: Budget transparency requires extracting and verifying financial allocations"
            ],
            "justification": "Gender policies require budget backing; extracting financial amounts enables assessing resource adequacy"
          },
          "technical_approach": {
            "method_type": "numeric_parsing_with_normalization",
            "algorithm": "Detect numeric patterns, parse to float, normalize units (thousands, millions)",
            "steps": [
              {
                "step": 1,
                "description": "Detect amount patterns ($ X.XXX.XXX, X millones)"
              },
              {
                "step": 2,
                "description": "Parse numeric component to float"
              },
              {
                "step": 3,
                "description": "Identify unit (millones, mil millones) and multiply"
              },
              {
                "step": 4,
                "description": "Normalize to standard unit (e.g., COP)"
              }
            ],
            "assumptions": [
              "Amounts follow Colombian formatting conventions (. as thousands separator)",
              "Units are explicitly stated or inferable from context"
            ],
            "limitations": [
              "May fail on non-standard formats",
              "Cannot verify accuracy of stated amounts"
            ],
            "complexity": "O(n) where n=detected amount patterns"
          },
          "output_interpretation": {
            "output_structure": {
              "parsed_amounts": "List of {raw_text, normalized_value, currency, confidence}"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.9: Amount clearly stated and parsed",
              "low_confidence": "<0.7: Ambiguous format, may be misparsed"
            },
            "actionable_insights": [
              "Many amounts but low confidence: Financial data is poorly formatted",
              "No amounts found: Budget information missing from diagnosis"
            ]
          }
        },
        "_detect_allocation_gaps": {
          "canonical_abbreviation": "financial_audit",
          "provides": "financial_audit.detect_allocation_gaps",
          "role": "_detect_allocation_gaps_detection",
          "usage_count_in_300": 20,
          "questions": [
            "Q002",
            "Q019",
            "Q032",
            "Q049",
            "Q062",
            "Q079",
            "Q092",
            "Q109",
            "Q122",
            "Q139",
            "Q152",
            "Q169",
            "Q182",
            "Q199",
            "Q212",
            "Q229",
            "Q242",
            "Q259",
            "Q272",
            "Q289"
          ],
          "base_slots": [
            "D1-Q2",
            "D4-Q4"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM01",
            "DIM04"
          ],
          "signature": {
            "line_number": 2209,
            "is_async": false,
            "parameters": [
              {
                "name": "nodes",
                "type": "dict[str, MetaNode]"
              }
            ],
            "return_type": "list[dict[str, Any]]",
            "docstring": "\n        Detect gaps in financial allocations.\n\n        Args:\n            nodes: Dictionary of nodes\n\n        Returns:\n            List of detected gaps\n        "
          },
          "epistemological_foundation": {
            "paradigm": "FinancialAuditor analytical paradigm",
            "ontological_basis": "Analysis via FinancialAuditor._detect_allocation_gaps",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _detect_allocation_gaps implements structured analysis for D1-Q2"
            ],
            "justification": "This method contributes to D1-Q2 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "FinancialAuditor._detect_allocation_gaps algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _detect_allocation_gaps"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _detect_allocation_gaps"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _detect_allocation_gaps results for downstream analysis"
            ]
          }
        },
        "trace_financial_allocation": {
          "canonical_abbreviation": "financial_audit",
          "provides": "financial_audit.trace_financial_allocation",
          "role": "trace_financial_allocation_execution",
          "usage_count_in_300": 10,
          "questions": [
            "Q003",
            "Q033",
            "Q063",
            "Q093",
            "Q123",
            "Q153",
            "Q183",
            "Q213",
            "Q243",
            "Q273"
          ],
          "base_slots": [
            "D1-Q3"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM01"
          ],
          "signature": {
            "line_number": 1921,
            "is_async": false,
            "parameters": [
              {
                "name": "tables",
                "type": "list[pd.DataFrame]"
              },
              {
                "name": "nodes",
                "type": "dict[str, MetaNode]"
              },
              {
                "name": "graph",
                "type": "nx.DiGraph | None",
                "default": "None"
              }
            ],
            "return_type": "dict[str, float]",
            "docstring": "Trace financial allocations to programs/goals\n\n        Harmonic Front 3 - Enhancement 5: Single-Case Counterfactual Budget Check\n        Incorporates logic from single-case counterfactuals to test minimal sufficiency.\n        For D3-Q3 (Traceability/Resources): checks if resource X (BPIN code) were removed,\n        would the mechanism (Product) still execute? Only boosts budget traceability score\n        if allocation is tied to a specific project.\n        "
          },
          "epistemological_foundation": {
            "paradigm": "FinancialAuditor analytical paradigm",
            "ontological_basis": "Analysis via FinancialAuditor.trace_financial_allocation",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method trace_financial_allocation implements structured analysis for D1-Q3"
            ],
            "justification": "This method contributes to D1-Q3 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "FinancialAuditor.trace_financial_allocation algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute trace_financial_allocation"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from trace_financial_allocation"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use trace_financial_allocation results for downstream analysis"
            ]
          }
        },
        "_process_financial_table": {
          "canonical_abbreviation": "financial_audit",
          "provides": "financial_audit.process_financial_table",
          "role": "_process_financial_table_processing",
          "usage_count_in_300": 20,
          "questions": [
            "Q003",
            "Q006",
            "Q033",
            "Q036",
            "Q063",
            "Q066",
            "Q093",
            "Q096",
            "Q123",
            "Q126",
            "Q153",
            "Q156",
            "Q183",
            "Q186",
            "Q213",
            "Q216",
            "Q243",
            "Q246",
            "Q273",
            "Q276"
          ],
          "base_slots": [
            "D1-Q3",
            "D2-Q1"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM01",
            "DIM02"
          ],
          "signature": {
            "line_number": 1952,
            "is_async": false,
            "parameters": [
              {
                "name": "table",
                "type": "pd.DataFrame"
              },
              {
                "name": "nodes",
                "type": "dict[str, MetaNode]"
              }
            ],
            "return_type": "None",
            "docstring": "Process a single financial table"
          },
          "epistemological_foundation": {
            "paradigm": "FinancialAuditor analytical paradigm",
            "ontological_basis": "Analysis via FinancialAuditor._process_financial_table",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _process_financial_table implements structured analysis for D1-Q3"
            ],
            "justification": "This method contributes to D1-Q3 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "FinancialAuditor._process_financial_table algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _process_financial_table"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _process_financial_table"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _process_financial_table results for downstream analysis"
            ]
          }
        },
        "_match_program_to_node": {
          "canonical_abbreviation": "financial_audit",
          "provides": "financial_audit.match_program_to_node",
          "role": "_match_program_to_node_matching",
          "usage_count_in_300": 20,
          "questions": [
            "Q003",
            "Q013",
            "Q033",
            "Q043",
            "Q063",
            "Q073",
            "Q093",
            "Q103",
            "Q123",
            "Q133",
            "Q153",
            "Q163",
            "Q183",
            "Q193",
            "Q213",
            "Q223",
            "Q243",
            "Q253",
            "Q273",
            "Q283"
          ],
          "base_slots": [
            "D1-Q3",
            "D3-Q3"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM01",
            "DIM03"
          ],
          "signature": {
            "line_number": 2040,
            "is_async": false,
            "parameters": [
              {
                "name": "program_id",
                "type": "str"
              },
              {
                "name": "nodes",
                "type": "dict[str, MetaNode]"
              }
            ],
            "return_type": "str | None",
            "docstring": "Match program ID to existing node using fuzzy matching\n\n        Enhanced for D1-Q3 / D3-Q3 Financial Traceability:\n        - Implements confidence penalty if fuzzy match ratio < 100\n        - Reduces node.financial_allocation confidence by 15% for imperfect matches\n        - Tracks match quality for overall financial traceability scoring\n        "
          },
          "epistemological_foundation": {
            "paradigm": "FinancialAuditor analytical paradigm",
            "ontological_basis": "Analysis via FinancialAuditor._match_program_to_node",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _match_program_to_node implements structured analysis for D1-Q3"
            ],
            "justification": "This method contributes to D1-Q3 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "FinancialAuditor._match_program_to_node algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _match_program_to_node"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _match_program_to_node"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _match_program_to_node results for downstream analysis"
            ]
          }
        },
        "_match_goal_to_budget": {
          "canonical_abbreviation": "financial_audit",
          "provides": "financial_audit.match_goal_to_budget",
          "role": "_match_goal_to_budget_matching",
          "usage_count_in_300": 20,
          "questions": [
            "Q003",
            "Q013",
            "Q033",
            "Q043",
            "Q063",
            "Q073",
            "Q093",
            "Q103",
            "Q123",
            "Q133",
            "Q153",
            "Q163",
            "Q183",
            "Q193",
            "Q213",
            "Q223",
            "Q243",
            "Q253",
            "Q273",
            "Q283"
          ],
          "base_slots": [
            "D1-Q3",
            "D3-Q3"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM01",
            "DIM03"
          ],
          "signature": {
            "line_number": 2251,
            "is_async": false,
            "parameters": [
              {
                "name": "goal_text",
                "type": "str"
              },
              {
                "name": "budget_entries",
                "type": "list[dict[str, Any]]"
              }
            ],
            "return_type": "dict[str, Any] | None",
            "docstring": "\n        Match a goal to budget entries.\n\n        Args:\n            goal_text: Goal text to match\n            budget_entries: List of budget entries\n\n        Returns:\n            Best matching budget entry or None\n        "
          },
          "epistemological_foundation": {
            "paradigm": "FinancialAuditor analytical paradigm",
            "ontological_basis": "Analysis via FinancialAuditor._match_goal_to_budget",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _match_goal_to_budget implements structured analysis for D1-Q3"
            ],
            "justification": "This method contributes to D1-Q3 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "FinancialAuditor._match_goal_to_budget algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _match_goal_to_budget"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _match_goal_to_budget"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _match_goal_to_budget results for downstream analysis"
            ]
          }
        },
        "_perform_counterfactual_budget_check": {
          "canonical_abbreviation": "financial_audit",
          "provides": "financial_audit.perform_counterfactual_budget_check",
          "role": "_perform_counterfactual_budget_check_execution",
          "usage_count_in_300": 10,
          "questions": [
            "Q003",
            "Q033",
            "Q063",
            "Q093",
            "Q123",
            "Q153",
            "Q183",
            "Q213",
            "Q243",
            "Q273"
          ],
          "base_slots": [
            "D1-Q3"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM01"
          ],
          "signature": {
            "line_number": 2096,
            "is_async": false,
            "parameters": [
              {
                "name": "nodes",
                "type": "dict[str, MetaNode]"
              },
              {
                "name": "graph",
                "type": "nx.DiGraph"
              }
            ],
            "return_type": "None",
            "docstring": "\n        Harmonic Front 3 - Enhancement 5: Counterfactual Sufficiency Test for D3-Q3\n\n        Tests minimal sufficiency: if resource X (BPIN code) were removed, would the\n        mechanism (Product) still execute? Only boosts budget traceability score if\n        allocation is tied to a specific project.\n\n        For D3-Q3 (Traceability/Resources): ensures funding is necessary for the mechanism\n        and prevents false positives from generic or disconnected budget entries.\n        "
          },
          "epistemological_foundation": {
            "paradigm": "FinancialAuditor analytical paradigm",
            "ontological_basis": "Analysis via FinancialAuditor._perform_counterfactual_budget_check",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _perform_counterfactual_budget_check implements structured analysis for D1-Q3"
            ],
            "justification": "This method contributes to D1-Q3 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "FinancialAuditor._perform_counterfactual_budget_check algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _perform_counterfactual_budget_check"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _perform_counterfactual_budget_check"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _perform_counterfactual_budget_check results for downstream analysis"
            ]
          }
        },
        "_calculate_sufficiency": {
          "canonical_abbreviation": "financial_audit",
          "provides": "financial_audit.calculate_sufficiency",
          "role": "_calculate_sufficiency_calculation",
          "usage_count_in_300": 30,
          "questions": [
            "Q003",
            "Q012",
            "Q022",
            "Q033",
            "Q042",
            "Q052",
            "Q063",
            "Q072",
            "Q082",
            "Q093",
            "Q102",
            "Q112",
            "Q123",
            "Q132",
            "Q142",
            "Q153",
            "Q162",
            "Q172",
            "Q183",
            "Q192",
            "Q202",
            "Q213",
            "Q222",
            "Q232",
            "Q243",
            "Q252",
            "Q262",
            "Q273",
            "Q282",
            "Q292"
          ],
          "base_slots": [
            "D1-Q3",
            "D3-Q2",
            "D5-Q2"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM01",
            "DIM03",
            "DIM05"
          ],
          "signature": {
            "line_number": 2187,
            "is_async": false,
            "parameters": [
              {
                "name": "allocation",
                "type": "float"
              },
              {
                "name": "target",
                "type": "float"
              }
            ],
            "return_type": "float",
            "docstring": "\n        Calculate if financial allocation is sufficient for target.\n\n        Args:\n            allocation: Financial allocation amount\n            target: Target value\n\n        Returns:\n            Sufficiency ratio (1.0 = exactly sufficient, >1.0 = oversufficient)\n        "
          },
          "epistemological_foundation": {
            "paradigm": "FinancialAuditor analytical paradigm",
            "ontological_basis": "Analysis via FinancialAuditor._calculate_sufficiency",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _calculate_sufficiency implements structured analysis for D1-Q3"
            ],
            "justification": "This method contributes to D1-Q3 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "FinancialAuditor._calculate_sufficiency algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _calculate_sufficiency"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _calculate_sufficiency"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _calculate_sufficiency results for downstream analysis"
            ]
          }
        }
      }
    },
    "PDETMunicipalPlanAnalyzer": {
      "file_path": "src/methods_dispensary/financiero_viabilidad_tablas copy.py",
      "line_number": 268,
      "class_docstring": "Analizador de vanguardia para Planes de Desarrollo Municipal PDET",
      "total_usage_in_300_contracts": 840,
      "unique_questions": 220,
      "questions": [
        "Q001",
        "Q002",
        "Q003",
        "Q004",
        "Q006",
        "Q007",
        "Q008",
        "Q009",
        "Q010",
        "Q011",
        "Q012",
        "Q013",
        "Q015",
        "Q016",
        "Q018",
        "Q020",
        "Q021",
        "Q022",
        "Q023",
        "Q024",
        "Q025",
        "Q026",
        "Q031",
        "Q032",
        "Q033",
        "Q034",
        "Q036",
        "Q037",
        "Q038",
        "Q039",
        "Q040",
        "Q041",
        "Q042",
        "Q043",
        "Q045",
        "Q046",
        "Q048",
        "Q050",
        "Q051",
        "Q052",
        "Q053",
        "Q054",
        "Q055",
        "Q056",
        "Q061",
        "Q062",
        "Q063",
        "Q064",
        "Q066",
        "Q067",
        "Q068",
        "Q069",
        "Q070",
        "Q071",
        "Q072",
        "Q073",
        "Q075",
        "Q076",
        "Q078",
        "Q080",
        "Q081",
        "Q082",
        "Q083",
        "Q084",
        "Q085",
        "Q086",
        "Q091",
        "Q092",
        "Q093",
        "Q094",
        "Q096",
        "Q097",
        "Q098",
        "Q099",
        "Q100",
        "Q101",
        "Q102",
        "Q103",
        "Q105",
        "Q106",
        "Q108",
        "Q110",
        "Q111",
        "Q112",
        "Q113",
        "Q114",
        "Q115",
        "Q116",
        "Q121",
        "Q122",
        "Q123",
        "Q124",
        "Q126",
        "Q127",
        "Q128",
        "Q129",
        "Q130",
        "Q131",
        "Q132",
        "Q133",
        "Q135",
        "Q136",
        "Q138",
        "Q140",
        "Q141",
        "Q142",
        "Q143",
        "Q144",
        "Q145",
        "Q146",
        "Q151",
        "Q152",
        "Q153",
        "Q154",
        "Q156",
        "Q157",
        "Q158",
        "Q159",
        "Q160",
        "Q161",
        "Q162",
        "Q163",
        "Q165",
        "Q166",
        "Q168",
        "Q170",
        "Q171",
        "Q172",
        "Q173",
        "Q174",
        "Q175",
        "Q176",
        "Q181",
        "Q182",
        "Q183",
        "Q184",
        "Q186",
        "Q187",
        "Q188",
        "Q189",
        "Q190",
        "Q191",
        "Q192",
        "Q193",
        "Q195",
        "Q196",
        "Q198",
        "Q200",
        "Q201",
        "Q202",
        "Q203",
        "Q204",
        "Q205",
        "Q206",
        "Q211",
        "Q212",
        "Q213",
        "Q214",
        "Q216",
        "Q217",
        "Q218",
        "Q219",
        "Q220",
        "Q221",
        "Q222",
        "Q223",
        "Q225",
        "Q226",
        "Q228",
        "Q230",
        "Q231",
        "Q232",
        "Q233",
        "Q234",
        "Q235",
        "Q236",
        "Q241",
        "Q242",
        "Q243",
        "Q244",
        "Q246",
        "Q247",
        "Q248",
        "Q249",
        "Q250",
        "Q251",
        "Q252",
        "Q253",
        "Q255",
        "Q256",
        "Q258",
        "Q260",
        "Q261",
        "Q262",
        "Q263",
        "Q264",
        "Q265",
        "Q266",
        "Q271",
        "Q272",
        "Q273",
        "Q274",
        "Q276",
        "Q277",
        "Q278",
        "Q279",
        "Q280",
        "Q281",
        "Q282",
        "Q283",
        "Q285",
        "Q286",
        "Q288",
        "Q290",
        "Q291",
        "Q292",
        "Q293",
        "Q294",
        "Q295",
        "Q296"
      ],
      "base_slots": [
        "D1-Q1",
        "D1-Q2",
        "D1-Q3",
        "D1-Q4",
        "D2-Q1",
        "D2-Q2",
        "D2-Q3",
        "D2-Q4",
        "D2-Q5",
        "D3-Q1",
        "D3-Q2",
        "D3-Q3",
        "D3-Q5",
        "D4-Q1",
        "D4-Q3",
        "D4-Q5",
        "D5-Q1",
        "D5-Q2",
        "D5-Q3",
        "D5-Q4",
        "D5-Q5",
        "D6-Q1"
      ],
      "policy_areas": [
        "PA01",
        "PA02",
        "PA03",
        "PA04",
        "PA05",
        "PA06",
        "PA07",
        "PA08",
        "PA09",
        "PA10"
      ],
      "dimensions": [
        "DIM01",
        "DIM02",
        "DIM03",
        "DIM04",
        "DIM05",
        "DIM06"
      ],
      "methods": {
        "_extract_financial_amounts": {
          "canonical_abbreviation": "pdet_analysis",
          "provides": "pdet_analysis.extract_financial_amounts",
          "role": "_extract_financial_amounts_extraction",
          "usage_count_in_300": 10,
          "questions": [
            "Q001",
            "Q031",
            "Q061",
            "Q091",
            "Q121",
            "Q151",
            "Q181",
            "Q211",
            "Q241",
            "Q271"
          ],
          "base_slots": [
            "D1-Q1"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM01"
          ],
          "signature": {
            "line_number": 560,
            "is_async": false,
            "parameters": [
              {
                "name": "text",
                "type": "str"
              },
              {
                "name": "tables",
                "type": "list[ExtractedTable]"
              }
            ],
            "return_type": "list[FinancialIndicator]",
            "docstring": null
          },
          "epistemological_foundation": {
            "paradigm": "Context-specific financial analysis for PDET municipalities",
            "ontological_basis": "PDET plans have specific financial reporting requirements and structures",
            "epistemological_stance": "Domain-specific realism: Financial data extraction must account for PDET-specific terminologies and structures",
            "theoretical_framework": [
              "Institutional context sensitivity: Policy analysis tools must adapt to specific institutional frameworks (PDET)"
            ],
            "justification": "PDET municipalities may use specific financial categories (PAC, SGR) requiring specialized extraction"
          },
          "technical_approach": {
            "method_type": "domain_specific_financial_extraction",
            "algorithm": "Augment generic financial parsing with PDET-specific patterns",
            "steps": [
              {
                "step": 1,
                "description": "Apply generic amount parsing"
              },
              {
                "step": 2,
                "description": "Detect PDET-specific financial categories (SGR, regalías, PAC)"
              },
              {
                "step": 3,
                "description": "Link amounts to categories based on proximity"
              }
            ],
            "assumptions": [
              "PDET plans mention financial categories near amounts",
              "Category terminology is standardized"
            ],
            "limitations": [
              "Limited to known PDET financial categories",
              "Cannot extract amounts from complex table formats without table parsing"
            ],
            "complexity": "O(n*c) where n=amounts, c=categories"
          },
          "output_interpretation": {
            "output_structure": {
              "categorized_amounts": "List of {amount, category, source_type, confidence}"
            },
            "interpretation_guide": {
              "categorized": "Amount linked to financial category (e.g., SGR)",
              "uncategorized": "Amount found but no category detected"
            },
            "actionable_insights": [
              "Uncategorized amounts: Budget detail without transparency on funding source",
              "Heavy reliance on specific source (e.g., all from SGR): Diversification risk"
            ]
          }
        },
        "_extract_from_budget_table": {
          "canonical_abbreviation": "pdet_analysis",
          "provides": "pdet_analysis.extract_from_budget_table",
          "role": "_extract_from_budget_table_extraction",
          "usage_count_in_300": 10,
          "questions": [
            "Q001",
            "Q031",
            "Q061",
            "Q091",
            "Q121",
            "Q151",
            "Q181",
            "Q211",
            "Q241",
            "Q271"
          ],
          "base_slots": [
            "D1-Q1"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM01"
          ],
          "signature": {
            "line_number": 626,
            "is_async": false,
            "parameters": [
              {
                "name": "df",
                "type": "pd.DataFrame"
              }
            ],
            "return_type": "list[FinancialIndicator]",
            "docstring": null
          },
          "epistemological_foundation": {
            "paradigm": "Structured data extraction from tabular formats",
            "ontological_basis": "Budget data is often presented in tables with rows (programs/projects) and columns (years/amounts)",
            "epistemological_stance": "Structural realism: Tables encode structured financial information requiring specialized parsing",
            "theoretical_framework": [
              "Information architecture: Tabular formats embed relational structures (rows, columns, cells) that determine information retrieval strategies"
            ],
            "justification": "Tables contain dense, structured financial data that cannot be extracted via sentence-level text mining"
          },
          "technical_approach": {
            "method_type": "table_parsing_and_cell_extraction",
            "algorithm": "Detect table structures, parse rows/columns, extract cell values",
            "steps": [
              {
                "step": 1,
                "description": "Detect table markers (HTML <table>, Markdown tables, aligned whitespace)"
              },
              {
                "step": 2,
                "description": "Parse table structure (identify headers, rows, columns)"
              },
              {
                "step": 3,
                "description": "Extract cells containing financial amounts"
              },
              {
                "step": 4,
                "description": "Link amounts to row labels (program names) and column labels (years)"
              }
            ],
            "assumptions": [
              "Tables follow standard row-column format",
              "Headers are identifiable",
              "Amounts are in consistent column positions"
            ],
            "limitations": [
              "Fails on complex nested tables",
              "May misparse poorly formatted tables",
              "Cannot extract from image-based tables without OCR"
            ],
            "complexity": "O(r*c) where r=rows, c=columns"
          },
          "output_interpretation": {
            "output_structure": {
              "table_data": "List of {row_label, column_label, amount, table_id}"
            },
            "interpretation_guide": {
              "complete_tables": "All expected columns present: Comprehensive budget detail",
              "incomplete_tables": "Missing columns/rows: Incomplete financial information"
            },
            "actionable_insights": [
              "Multi-year budget tables: Indicates planning horizon and resource trajectory",
              "Single-year tables: Short-term planning, no long-term resource commitment"
            ]
          }
        },
        "_generate_optimal_remediations": {
          "canonical_abbreviation": "pdet_analysis",
          "provides": "pdet_analysis.generate_optimal_remediations",
          "role": "_generate_optimal_remediations_execution",
          "usage_count_in_300": 10,
          "questions": [
            "Q002",
            "Q032",
            "Q062",
            "Q092",
            "Q122",
            "Q152",
            "Q182",
            "Q212",
            "Q242",
            "Q272"
          ],
          "base_slots": [
            "D1-Q2"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM01"
          ],
          "signature": {
            "line_number": 2325,
            "is_async": false,
            "parameters": [
              {
                "name": "gaps",
                "type": "list[dict[str, Any]]"
              }
            ],
            "return_type": "list[dict[str, str]]",
            "docstring": "\n        Generate optimal remediations for identified gaps.\n\n        Args:\n            gaps: List of identified gaps\n\n        Returns:\n            List of remediation recommendations\n        "
          },
          "epistemological_foundation": {
            "paradigm": "PDETMunicipalPlanAnalyzer analytical paradigm",
            "ontological_basis": "Analysis via PDETMunicipalPlanAnalyzer._generate_optimal_remediations",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _generate_optimal_remediations implements structured analysis for D1-Q2"
            ],
            "justification": "This method contributes to D1-Q2 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "PDETMunicipalPlanAnalyzer._generate_optimal_remediations algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _generate_optimal_remediations"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _generate_optimal_remediations"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _generate_optimal_remediations results for downstream analysis"
            ]
          }
        },
        "_simulate_intervention": {
          "canonical_abbreviation": "pdet_analysis",
          "provides": "pdet_analysis.simulate_intervention",
          "role": "_simulate_intervention_execution",
          "usage_count_in_300": 30,
          "questions": [
            "Q002",
            "Q015",
            "Q021",
            "Q032",
            "Q045",
            "Q051",
            "Q062",
            "Q075",
            "Q081",
            "Q092",
            "Q105",
            "Q111",
            "Q122",
            "Q135",
            "Q141",
            "Q152",
            "Q165",
            "Q171",
            "Q182",
            "Q195",
            "Q201",
            "Q212",
            "Q225",
            "Q231",
            "Q242",
            "Q255",
            "Q261",
            "Q272",
            "Q285",
            "Q291"
          ],
          "base_slots": [
            "D1-Q2",
            "D3-Q5",
            "D5-Q1"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM01",
            "DIM03",
            "DIM05"
          ],
          "signature": {
            "line_number": 1484,
            "is_async": false,
            "parameters": [
              {
                "name": "intervention",
                "type": "dict[str, float]"
              },
              {
                "name": "dag",
                "type": "CausalDAG"
              },
              {
                "name": "causal_effects",
                "type": "list[CausalEffect]"
              },
              {
                "name": "description",
                "type": "str"
              }
            ],
            "return_type": "CounterfactualScenario",
            "docstring": "\n        Simula intervención usando do-calculus (Pearl, 2009)\n        Implementa: P(Y | do(X=x)) mediante propagación por el DAG\n        "
          },
          "epistemological_foundation": {
            "paradigm": "PDETMunicipalPlanAnalyzer analytical paradigm",
            "ontological_basis": "Analysis via PDETMunicipalPlanAnalyzer._simulate_intervention",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _simulate_intervention implements structured analysis for D1-Q2"
            ],
            "justification": "This method contributes to D1-Q2 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "PDETMunicipalPlanAnalyzer._simulate_intervention algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _simulate_intervention"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _simulate_intervention"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _simulate_intervention results for downstream analysis"
            ]
          }
        },
        "analyze_financial_feasibility": {
          "canonical_abbreviation": "pdet_analysis",
          "provides": "pdet_analysis.analyze_financial_feasibility",
          "role": "analyze_financial_feasibility_analysis",
          "usage_count_in_300": 30,
          "questions": [
            "Q003",
            "Q012",
            "Q015",
            "Q033",
            "Q042",
            "Q045",
            "Q063",
            "Q072",
            "Q075",
            "Q093",
            "Q102",
            "Q105",
            "Q123",
            "Q132",
            "Q135",
            "Q153",
            "Q162",
            "Q165",
            "Q183",
            "Q192",
            "Q195",
            "Q213",
            "Q222",
            "Q225",
            "Q243",
            "Q252",
            "Q255",
            "Q273",
            "Q282",
            "Q285"
          ],
          "base_slots": [
            "D1-Q3",
            "D3-Q2",
            "D3-Q5"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM01",
            "DIM03"
          ],
          "signature": {
            "line_number": 542,
            "is_async": false,
            "parameters": [
              {
                "name": "tables",
                "type": "list[ExtractedTable]"
              },
              {
                "name": "text",
                "type": "str"
              }
            ],
            "return_type": "dict[str, Any]",
            "docstring": null
          },
          "epistemological_foundation": {
            "paradigm": "PDETMunicipalPlanAnalyzer analytical paradigm",
            "ontological_basis": "Analysis via PDETMunicipalPlanAnalyzer.analyze_financial_feasibility",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method analyze_financial_feasibility implements structured analysis for D1-Q3"
            ],
            "justification": "This method contributes to D1-Q3 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "PDETMunicipalPlanAnalyzer.analyze_financial_feasibility algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute analyze_financial_feasibility"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from analyze_financial_feasibility"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use analyze_financial_feasibility results for downstream analysis"
            ]
          }
        },
        "_extract_budget_for_pillar": {
          "canonical_abbreviation": "pdet_analysis",
          "provides": "pdet_analysis.extract_budget_for_pillar",
          "role": "_extract_budget_for_pillar_extraction",
          "usage_count_in_300": 10,
          "questions": [
            "Q003",
            "Q033",
            "Q063",
            "Q093",
            "Q123",
            "Q153",
            "Q183",
            "Q213",
            "Q243",
            "Q273"
          ],
          "base_slots": [
            "D1-Q3"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM01"
          ],
          "signature": {
            "line_number": 1131,
            "is_async": false,
            "parameters": [
              {
                "name": "pillar",
                "type": "str"
              },
              {
                "name": "text",
                "type": "str"
              },
              {
                "name": "financial_analysis",
                "type": "dict[str, Any]"
              }
            ],
            "return_type": "Decimal | None",
            "docstring": null
          },
          "epistemological_foundation": {
            "paradigm": "PDETMunicipalPlanAnalyzer analytical paradigm",
            "ontological_basis": "Analysis via PDETMunicipalPlanAnalyzer._extract_budget_for_pillar",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _extract_budget_for_pillar implements structured analysis for D1-Q3"
            ],
            "justification": "This method contributes to D1-Q3 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "PDETMunicipalPlanAnalyzer._extract_budget_for_pillar algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _extract_budget_for_pillar"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _extract_budget_for_pillar"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _extract_budget_for_pillar results for downstream analysis"
            ]
          }
        },
        "_identify_funding_source": {
          "canonical_abbreviation": "pdet_analysis",
          "provides": "pdet_analysis.identify_funding_source",
          "role": "_identify_funding_source_execution",
          "usage_count_in_300": 10,
          "questions": [
            "Q003",
            "Q033",
            "Q063",
            "Q093",
            "Q123",
            "Q153",
            "Q183",
            "Q213",
            "Q243",
            "Q273"
          ],
          "base_slots": [
            "D1-Q3"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM01"
          ],
          "signature": {
            "line_number": 608,
            "is_async": false,
            "parameters": [
              {
                "name": "context",
                "type": "str"
              }
            ],
            "return_type": "str",
            "docstring": null
          },
          "epistemological_foundation": {
            "paradigm": "PDETMunicipalPlanAnalyzer analytical paradigm",
            "ontological_basis": "Analysis via PDETMunicipalPlanAnalyzer._identify_funding_source",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _identify_funding_source implements structured analysis for D1-Q3"
            ],
            "justification": "This method contributes to D1-Q3 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "PDETMunicipalPlanAnalyzer._identify_funding_source algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _identify_funding_source"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _identify_funding_source"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _identify_funding_source results for downstream analysis"
            ]
          }
        },
        "_classify_tables": {
          "canonical_abbreviation": "pdet_analysis",
          "provides": "pdet_analysis.classify_tables",
          "role": "_classify_tables_execution",
          "usage_count_in_300": 20,
          "questions": [
            "Q003",
            "Q006",
            "Q033",
            "Q036",
            "Q063",
            "Q066",
            "Q093",
            "Q096",
            "Q123",
            "Q126",
            "Q153",
            "Q156",
            "Q183",
            "Q186",
            "Q213",
            "Q216",
            "Q243",
            "Q246",
            "Q273",
            "Q276"
          ],
          "base_slots": [
            "D1-Q3",
            "D2-Q1"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM01",
            "DIM02"
          ],
          "signature": {
            "line_number": 515,
            "is_async": false,
            "parameters": [
              {
                "name": "tables",
                "type": "list[ExtractedTable]"
              }
            ],
            "return_type": "list[ExtractedTable]",
            "docstring": null
          },
          "epistemological_foundation": {
            "paradigm": "PDETMunicipalPlanAnalyzer analytical paradigm",
            "ontological_basis": "Analysis via PDETMunicipalPlanAnalyzer._classify_tables",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _classify_tables implements structured analysis for D1-Q3"
            ],
            "justification": "This method contributes to D1-Q3 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "PDETMunicipalPlanAnalyzer._classify_tables algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _classify_tables"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _classify_tables"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _classify_tables results for downstream analysis"
            ]
          }
        },
        "_analyze_funding_sources": {
          "canonical_abbreviation": "pdet_analysis",
          "provides": "pdet_analysis.analyze_funding_sources",
          "role": "_analyze_funding_sources_analysis",
          "usage_count_in_300": 10,
          "questions": [
            "Q003",
            "Q033",
            "Q063",
            "Q093",
            "Q123",
            "Q153",
            "Q183",
            "Q213",
            "Q243",
            "Q273"
          ],
          "base_slots": [
            "D1-Q3"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM01"
          ],
          "signature": {
            "line_number": 667,
            "is_async": false,
            "parameters": [
              {
                "name": "indicators",
                "type": "list[FinancialIndicator]"
              },
              {
                "name": "tables",
                "type": "list[ExtractedTable]"
              }
            ],
            "return_type": "dict[str, Any]",
            "docstring": null
          },
          "epistemological_foundation": {
            "paradigm": "PDETMunicipalPlanAnalyzer analytical paradigm",
            "ontological_basis": "Analysis via PDETMunicipalPlanAnalyzer._analyze_funding_sources",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _analyze_funding_sources implements structured analysis for D1-Q3"
            ],
            "justification": "This method contributes to D1-Q3 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "PDETMunicipalPlanAnalyzer._analyze_funding_sources algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _analyze_funding_sources"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _analyze_funding_sources"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _analyze_funding_sources results for downstream analysis"
            ]
          }
        },
        "_score_financial_component": {
          "canonical_abbreviation": "pdet_analysis",
          "provides": "pdet_analysis.score_financial_component",
          "role": "_score_financial_component_execution",
          "usage_count_in_300": 10,
          "questions": [
            "Q003",
            "Q033",
            "Q063",
            "Q093",
            "Q123",
            "Q153",
            "Q183",
            "Q213",
            "Q243",
            "Q273"
          ],
          "base_slots": [
            "D1-Q3"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM01"
          ],
          "signature": {
            "line_number": 1707,
            "is_async": false,
            "parameters": [
              {
                "name": "financial_analysis",
                "type": "dict[str, Any]"
              }
            ],
            "return_type": "float",
            "docstring": "Score componente financiero (0-10)"
          },
          "epistemological_foundation": {
            "paradigm": "PDETMunicipalPlanAnalyzer analytical paradigm",
            "ontological_basis": "Analysis via PDETMunicipalPlanAnalyzer._score_financial_component",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _score_financial_component implements structured analysis for D1-Q3"
            ],
            "justification": "This method contributes to D1-Q3 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "PDETMunicipalPlanAnalyzer._score_financial_component algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _score_financial_component"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _score_financial_component"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _score_financial_component results for downstream analysis"
            ]
          }
        },
        "identify_responsible_entities": {
          "canonical_abbreviation": "pdet_analysis",
          "provides": "pdet_analysis.identify_responsible_entities",
          "role": "identify_responsible_entities_execution",
          "usage_count_in_300": 20,
          "questions": [
            "Q004",
            "Q013",
            "Q034",
            "Q043",
            "Q064",
            "Q073",
            "Q094",
            "Q103",
            "Q124",
            "Q133",
            "Q154",
            "Q163",
            "Q184",
            "Q193",
            "Q214",
            "Q223",
            "Q244",
            "Q253",
            "Q274",
            "Q283"
          ],
          "base_slots": [
            "D1-Q4",
            "D3-Q3"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM01",
            "DIM03"
          ],
          "signature": {
            "line_number": 777,
            "is_async": false,
            "parameters": [
              {
                "name": "text",
                "type": "str"
              },
              {
                "name": "tables",
                "type": "list[ExtractedTable]"
              }
            ],
            "return_type": "list[ResponsibleEntity]",
            "docstring": null
          },
          "epistemological_foundation": {
            "paradigm": "PDETMunicipalPlanAnalyzer analytical paradigm",
            "ontological_basis": "Analysis via PDETMunicipalPlanAnalyzer.identify_responsible_entities",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method identify_responsible_entities implements structured analysis for D1-Q4"
            ],
            "justification": "This method contributes to D1-Q4 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "PDETMunicipalPlanAnalyzer.identify_responsible_entities algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute identify_responsible_entities"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from identify_responsible_entities"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use identify_responsible_entities results for downstream analysis"
            ]
          }
        },
        "_extract_entities_ner": {
          "canonical_abbreviation": "pdet_analysis",
          "provides": "pdet_analysis.extract_entities_ner",
          "role": "_extract_entities_ner_extraction",
          "usage_count_in_300": 20,
          "questions": [
            "Q004",
            "Q016",
            "Q034",
            "Q046",
            "Q064",
            "Q076",
            "Q094",
            "Q106",
            "Q124",
            "Q136",
            "Q154",
            "Q166",
            "Q184",
            "Q196",
            "Q214",
            "Q226",
            "Q244",
            "Q256",
            "Q274",
            "Q286"
          ],
          "base_slots": [
            "D1-Q4",
            "D4-Q1"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM01",
            "DIM04"
          ],
          "signature": {
            "line_number": 792,
            "is_async": false,
            "parameters": [
              {
                "name": "text",
                "type": "str"
              }
            ],
            "return_type": "list[ResponsibleEntity]",
            "docstring": null
          },
          "epistemological_foundation": {
            "paradigm": "PDETMunicipalPlanAnalyzer analytical paradigm",
            "ontological_basis": "Analysis via PDETMunicipalPlanAnalyzer._extract_entities_ner",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _extract_entities_ner implements structured analysis for D1-Q4"
            ],
            "justification": "This method contributes to D1-Q4 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "PDETMunicipalPlanAnalyzer._extract_entities_ner algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _extract_entities_ner"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _extract_entities_ner"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _extract_entities_ner results for downstream analysis"
            ]
          }
        },
        "_extract_entities_syntax": {
          "canonical_abbreviation": "pdet_analysis",
          "provides": "pdet_analysis.extract_entities_syntax",
          "role": "_extract_entities_syntax_extraction",
          "usage_count_in_300": 20,
          "questions": [
            "Q004",
            "Q016",
            "Q034",
            "Q046",
            "Q064",
            "Q076",
            "Q094",
            "Q106",
            "Q124",
            "Q136",
            "Q154",
            "Q166",
            "Q184",
            "Q196",
            "Q214",
            "Q226",
            "Q244",
            "Q256",
            "Q274",
            "Q286"
          ],
          "base_slots": [
            "D1-Q4",
            "D4-Q1"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM01",
            "DIM04"
          ],
          "signature": {
            "line_number": 818,
            "is_async": false,
            "parameters": [
              {
                "name": "text",
                "type": "str"
              }
            ],
            "return_type": "list[ResponsibleEntity]",
            "docstring": null
          },
          "epistemological_foundation": {
            "paradigm": "PDETMunicipalPlanAnalyzer analytical paradigm",
            "ontological_basis": "Analysis via PDETMunicipalPlanAnalyzer._extract_entities_syntax",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _extract_entities_syntax implements structured analysis for D1-Q4"
            ],
            "justification": "This method contributes to D1-Q4 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "PDETMunicipalPlanAnalyzer._extract_entities_syntax algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _extract_entities_syntax"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _extract_entities_syntax"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _extract_entities_syntax results for downstream analysis"
            ]
          }
        },
        "_classify_entity_type": {
          "canonical_abbreviation": "pdet_analysis",
          "provides": "pdet_analysis.classify_entity_type",
          "role": "_classify_entity_type_execution",
          "usage_count_in_300": 10,
          "questions": [
            "Q004",
            "Q034",
            "Q064",
            "Q094",
            "Q124",
            "Q154",
            "Q184",
            "Q214",
            "Q244",
            "Q274"
          ],
          "base_slots": [
            "D1-Q4"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM01"
          ],
          "signature": {
            "line_number": 846,
            "is_async": false,
            "parameters": [
              {
                "name": "name",
                "type": "str"
              }
            ],
            "return_type": "str",
            "docstring": null
          },
          "epistemological_foundation": {
            "paradigm": "PDETMunicipalPlanAnalyzer analytical paradigm",
            "ontological_basis": "Analysis via PDETMunicipalPlanAnalyzer._classify_entity_type",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _classify_entity_type implements structured analysis for D1-Q4"
            ],
            "justification": "This method contributes to D1-Q4 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "PDETMunicipalPlanAnalyzer._classify_entity_type algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _classify_entity_type"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _classify_entity_type"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _classify_entity_type results for downstream analysis"
            ]
          }
        },
        "_score_entity_specificity": {
          "canonical_abbreviation": "pdet_analysis",
          "provides": "pdet_analysis.score_entity_specificity",
          "role": "_score_entity_specificity_execution",
          "usage_count_in_300": 10,
          "questions": [
            "Q004",
            "Q034",
            "Q064",
            "Q094",
            "Q124",
            "Q154",
            "Q184",
            "Q214",
            "Q244",
            "Q274"
          ],
          "base_slots": [
            "D1-Q4"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM01"
          ],
          "signature": {
            "line_number": 927,
            "is_async": false,
            "parameters": [
              {
                "name": "entities",
                "type": "list[ResponsibleEntity]"
              },
              {
                "name": "full_text",
                "type": "str"
              }
            ],
            "return_type": "list[ResponsibleEntity]",
            "docstring": null
          },
          "epistemological_foundation": {
            "paradigm": "PDETMunicipalPlanAnalyzer analytical paradigm",
            "ontological_basis": "Analysis via PDETMunicipalPlanAnalyzer._score_entity_specificity",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _score_entity_specificity implements structured analysis for D1-Q4"
            ],
            "justification": "This method contributes to D1-Q4 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "PDETMunicipalPlanAnalyzer._score_entity_specificity algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _score_entity_specificity"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _score_entity_specificity"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _score_entity_specificity results for downstream analysis"
            ]
          }
        },
        "_consolidate_entities": {
          "canonical_abbreviation": "pdet_analysis",
          "provides": "pdet_analysis.consolidate_entities",
          "role": "_consolidate_entities_execution",
          "usage_count_in_300": 20,
          "questions": [
            "Q004",
            "Q013",
            "Q034",
            "Q043",
            "Q064",
            "Q073",
            "Q094",
            "Q103",
            "Q124",
            "Q133",
            "Q154",
            "Q163",
            "Q184",
            "Q193",
            "Q214",
            "Q223",
            "Q244",
            "Q253",
            "Q274",
            "Q283"
          ],
          "base_slots": [
            "D1-Q4",
            "D3-Q3"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM01",
            "DIM03"
          ],
          "signature": {
            "line_number": 892,
            "is_async": false,
            "parameters": [
              {
                "name": "entities",
                "type": "list[ResponsibleEntity]"
              }
            ],
            "return_type": "list[ResponsibleEntity]",
            "docstring": null
          },
          "epistemological_foundation": {
            "paradigm": "PDETMunicipalPlanAnalyzer analytical paradigm",
            "ontological_basis": "Analysis via PDETMunicipalPlanAnalyzer._consolidate_entities",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _consolidate_entities implements structured analysis for D1-Q4"
            ],
            "justification": "This method contributes to D1-Q4 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "PDETMunicipalPlanAnalyzer._consolidate_entities algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _consolidate_entities"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _consolidate_entities"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _consolidate_entities results for downstream analysis"
            ]
          }
        },
        "_deduplicate_tables": {
          "canonical_abbreviation": "pdet_analysis",
          "provides": "pdet_analysis.deduplicate_tables",
          "role": "_deduplicate_tables_execution",
          "usage_count_in_300": 20,
          "questions": [
            "Q006",
            "Q012",
            "Q036",
            "Q042",
            "Q066",
            "Q072",
            "Q096",
            "Q102",
            "Q126",
            "Q132",
            "Q156",
            "Q162",
            "Q186",
            "Q192",
            "Q216",
            "Q222",
            "Q246",
            "Q252",
            "Q276",
            "Q282"
          ],
          "base_slots": [
            "D2-Q1",
            "D3-Q2"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM02",
            "DIM03"
          ],
          "signature": {
            "line_number": 447,
            "is_async": false,
            "parameters": [
              {
                "name": "tables",
                "type": "list[ExtractedTable]"
              }
            ],
            "return_type": "list[ExtractedTable]",
            "docstring": null
          },
          "epistemological_foundation": {
            "paradigm": "PDETMunicipalPlanAnalyzer analytical paradigm",
            "ontological_basis": "Analysis via PDETMunicipalPlanAnalyzer._deduplicate_tables",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _deduplicate_tables implements structured analysis for D2-Q1"
            ],
            "justification": "This method contributes to D2-Q1 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "PDETMunicipalPlanAnalyzer._deduplicate_tables algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _deduplicate_tables"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _deduplicate_tables"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _deduplicate_tables results for downstream analysis"
            ]
          }
        },
        "_is_likely_header": {
          "canonical_abbreviation": "pdet_analysis",
          "provides": "pdet_analysis.is_likely_header",
          "role": "_is_likely_header_execution",
          "usage_count_in_300": 10,
          "questions": [
            "Q006",
            "Q036",
            "Q066",
            "Q096",
            "Q126",
            "Q156",
            "Q186",
            "Q216",
            "Q246",
            "Q276"
          ],
          "base_slots": [
            "D2-Q1"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM02"
          ],
          "signature": {
            "line_number": 416,
            "is_async": false,
            "parameters": [
              {
                "name": "row",
                "type": "pd.Series"
              },
              {
                "name": "**kwargs"
              }
            ],
            "return_type": "bool",
            "docstring": "\n        Determine if a DataFrame row is likely a header row based on linguistic analysis.\n\n        Args:\n            row: pandas Series representing a row from a DataFrame\n            **kwargs: Accepts additional keyword arguments for backward compatibility.\n                     These are ignored (e.g., pdf_path if mistakenly passed).\n\n        Returns:\n            Boolean indicating whether the row appears to be a header\n\n        Note:\n            This function only requires 'row' parameter. An"
          },
          "epistemological_foundation": {
            "paradigm": "PDETMunicipalPlanAnalyzer analytical paradigm",
            "ontological_basis": "Analysis via PDETMunicipalPlanAnalyzer._is_likely_header",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _is_likely_header implements structured analysis for D2-Q1"
            ],
            "justification": "This method contributes to D2-Q1 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "PDETMunicipalPlanAnalyzer._is_likely_header algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _is_likely_header"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _is_likely_header"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _is_likely_header results for downstream analysis"
            ]
          }
        },
        "_clean_dataframe": {
          "canonical_abbreviation": "pdet_analysis",
          "provides": "pdet_analysis.clean_dataframe",
          "role": "_clean_dataframe_execution",
          "usage_count_in_300": 10,
          "questions": [
            "Q006",
            "Q036",
            "Q066",
            "Q096",
            "Q126",
            "Q156",
            "Q186",
            "Q216",
            "Q246",
            "Q276"
          ],
          "base_slots": [
            "D2-Q1"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM02"
          ],
          "signature": {
            "line_number": 397,
            "is_async": false,
            "parameters": [
              {
                "name": "df",
                "type": "pd.DataFrame"
              }
            ],
            "return_type": "pd.DataFrame",
            "docstring": null
          },
          "epistemological_foundation": {
            "paradigm": "PDETMunicipalPlanAnalyzer analytical paradigm",
            "ontological_basis": "Analysis via PDETMunicipalPlanAnalyzer._clean_dataframe",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _clean_dataframe implements structured analysis for D2-Q1"
            ],
            "justification": "This method contributes to D2-Q1 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "PDETMunicipalPlanAnalyzer._clean_dataframe algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _clean_dataframe"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _clean_dataframe"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _clean_dataframe results for downstream analysis"
            ]
          }
        },
        "construct_causal_dag": {
          "canonical_abbreviation": "pdet_analysis",
          "provides": "pdet_analysis.construct_causal_dag",
          "role": "construct_causal_dag_execution",
          "usage_count_in_300": 20,
          "questions": [
            "Q007",
            "Q015",
            "Q037",
            "Q045",
            "Q067",
            "Q075",
            "Q097",
            "Q105",
            "Q127",
            "Q135",
            "Q157",
            "Q165",
            "Q187",
            "Q195",
            "Q217",
            "Q225",
            "Q247",
            "Q255",
            "Q277",
            "Q285"
          ],
          "base_slots": [
            "D2-Q2",
            "D3-Q5"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM02",
            "DIM03"
          ],
          "signature": {
            "line_number": 953,
            "is_async": false,
            "parameters": [
              {
                "name": "text",
                "type": "str"
              },
              {
                "name": "tables",
                "type": "list[ExtractedTable]"
              },
              {
                "name": "financial_analysis",
                "type": "dict[str, Any]"
              }
            ],
            "return_type": "CausalDAG",
            "docstring": null
          },
          "epistemological_foundation": {
            "paradigm": "PDETMunicipalPlanAnalyzer analytical paradigm",
            "ontological_basis": "Analysis via PDETMunicipalPlanAnalyzer.construct_causal_dag",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method construct_causal_dag implements structured analysis for D2-Q2"
            ],
            "justification": "This method contributes to D2-Q2 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "PDETMunicipalPlanAnalyzer.construct_causal_dag algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute construct_causal_dag"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from construct_causal_dag"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use construct_causal_dag results for downstream analysis"
            ]
          }
        },
        "_identify_causal_edges": {
          "canonical_abbreviation": "pdet_analysis",
          "provides": "pdet_analysis.identify_causal_edges",
          "role": "_identify_causal_edges_execution",
          "usage_count_in_300": 10,
          "questions": [
            "Q008",
            "Q038",
            "Q068",
            "Q098",
            "Q128",
            "Q158",
            "Q188",
            "Q218",
            "Q248",
            "Q278"
          ],
          "base_slots": [
            "D2-Q3"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM02"
          ],
          "signature": {
            "line_number": 1152,
            "is_async": false,
            "parameters": [
              {
                "name": "text",
                "type": "str"
              },
              {
                "name": "nodes",
                "type": "dict[str, CausalNode]"
              }
            ],
            "return_type": "list[CausalEdge]",
            "docstring": null
          },
          "epistemological_foundation": {
            "paradigm": "PDETMunicipalPlanAnalyzer analytical paradigm",
            "ontological_basis": "Analysis via PDETMunicipalPlanAnalyzer._identify_causal_edges",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _identify_causal_edges implements structured analysis for D2-Q3"
            ],
            "justification": "This method contributes to D2-Q3 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "PDETMunicipalPlanAnalyzer._identify_causal_edges algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _identify_causal_edges"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _identify_causal_edges"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _identify_causal_edges results for downstream analysis"
            ]
          }
        },
        "_refine_edge_probabilities": {
          "canonical_abbreviation": "pdet_analysis",
          "provides": "pdet_analysis.refine_edge_probabilities",
          "role": "_refine_edge_probabilities_execution",
          "usage_count_in_300": 20,
          "questions": [
            "Q008",
            "Q015",
            "Q038",
            "Q045",
            "Q068",
            "Q075",
            "Q098",
            "Q105",
            "Q128",
            "Q135",
            "Q158",
            "Q165",
            "Q188",
            "Q195",
            "Q218",
            "Q225",
            "Q248",
            "Q255",
            "Q278",
            "Q285"
          ],
          "base_slots": [
            "D2-Q3",
            "D3-Q5"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM02",
            "DIM03"
          ],
          "signature": {
            "line_number": 1240,
            "is_async": false,
            "parameters": [
              {
                "name": "edges",
                "type": "list[CausalEdge]"
              },
              {
                "name": "text",
                "type": "str"
              },
              {
                "name": "nodes",
                "type": "dict[str, CausalNode]"
              }
            ],
            "return_type": "list[CausalEdge]",
            "docstring": null
          },
          "epistemological_foundation": {
            "paradigm": "PDETMunicipalPlanAnalyzer analytical paradigm",
            "ontological_basis": "Analysis via PDETMunicipalPlanAnalyzer._refine_edge_probabilities",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _refine_edge_probabilities implements structured analysis for D2-Q3"
            ],
            "justification": "This method contributes to D2-Q3 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "PDETMunicipalPlanAnalyzer._refine_edge_probabilities algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _refine_edge_probabilities"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _refine_edge_probabilities"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _refine_edge_probabilities results for downstream analysis"
            ]
          }
        },
        "_bayesian_risk_inference": {
          "canonical_abbreviation": "pdet_analysis",
          "provides": "pdet_analysis.bayesian_risk_inference",
          "role": "_bayesian_risk_inference_execution",
          "usage_count_in_300": 10,
          "questions": [
            "Q009",
            "Q039",
            "Q069",
            "Q099",
            "Q129",
            "Q159",
            "Q189",
            "Q219",
            "Q249",
            "Q279"
          ],
          "base_slots": [
            "D2-Q4"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM02"
          ],
          "signature": {
            "line_number": 706,
            "is_async": false,
            "parameters": [
              {
                "name": "indicators",
                "type": "list[FinancialIndicator]"
              },
              {
                "name": "funding_sources",
                "type": "dict[str, Any]"
              },
              {
                "name": "sustainability",
                "type": "float"
              }
            ],
            "return_type": "dict[str, Any]",
            "docstring": null
          },
          "epistemological_foundation": {
            "paradigm": "PDETMunicipalPlanAnalyzer analytical paradigm",
            "ontological_basis": "Analysis via PDETMunicipalPlanAnalyzer._bayesian_risk_inference",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _bayesian_risk_inference implements structured analysis for D2-Q4"
            ],
            "justification": "This method contributes to D2-Q4 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "PDETMunicipalPlanAnalyzer._bayesian_risk_inference algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _bayesian_risk_inference"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _bayesian_risk_inference"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _bayesian_risk_inference results for downstream analysis"
            ]
          }
        },
        "sensitivity_analysis": {
          "canonical_abbreviation": "pdet_analysis",
          "provides": "pdet_analysis.sensitivity_analysis",
          "role": "sensitivity_analysis_execution",
          "usage_count_in_300": 10,
          "questions": [
            "Q009",
            "Q039",
            "Q069",
            "Q099",
            "Q129",
            "Q159",
            "Q189",
            "Q219",
            "Q249",
            "Q279"
          ],
          "base_slots": [
            "D2-Q4"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM02"
          ],
          "signature": {
            "line_number": 1578,
            "is_async": false,
            "parameters": [
              {
                "name": "causal_effects",
                "type": "list[CausalEffect]"
              },
              {
                "name": "dag",
                "type": "CausalDAG"
              }
            ],
            "return_type": "dict[str, Any]",
            "docstring": "\n        Análisis de sensibilidad para supuestos de identificación causal\n        Basado en: Cinelli, Forney & Pearl (2022) - \"A Crash Course in Good and Bad Controls\"\n        "
          },
          "epistemological_foundation": {
            "paradigm": "PDETMunicipalPlanAnalyzer analytical paradigm",
            "ontological_basis": "Analysis via PDETMunicipalPlanAnalyzer.sensitivity_analysis",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method sensitivity_analysis implements structured analysis for D2-Q4"
            ],
            "justification": "This method contributes to D2-Q4 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "PDETMunicipalPlanAnalyzer.sensitivity_analysis algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute sensitivity_analysis"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from sensitivity_analysis"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use sensitivity_analysis results for downstream analysis"
            ]
          }
        },
        "_interpret_risk": {
          "canonical_abbreviation": "pdet_analysis",
          "provides": "pdet_analysis.interpret_risk",
          "role": "_interpret_risk_execution",
          "usage_count_in_300": 30,
          "questions": [
            "Q009",
            "Q012",
            "Q024",
            "Q039",
            "Q042",
            "Q054",
            "Q069",
            "Q072",
            "Q084",
            "Q099",
            "Q102",
            "Q114",
            "Q129",
            "Q132",
            "Q144",
            "Q159",
            "Q162",
            "Q174",
            "Q189",
            "Q192",
            "Q204",
            "Q219",
            "Q222",
            "Q234",
            "Q249",
            "Q252",
            "Q264",
            "Q279",
            "Q282",
            "Q294"
          ],
          "base_slots": [
            "D2-Q4",
            "D3-Q2",
            "D5-Q4"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM02",
            "DIM03",
            "DIM05"
          ],
          "signature": {
            "line_number": 749,
            "is_async": false,
            "parameters": [
              {
                "name": "risk",
                "type": "float"
              }
            ],
            "return_type": "str",
            "docstring": null
          },
          "epistemological_foundation": {
            "paradigm": "PDETMunicipalPlanAnalyzer analytical paradigm",
            "ontological_basis": "Analysis via PDETMunicipalPlanAnalyzer._interpret_risk",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _interpret_risk implements structured analysis for D2-Q4"
            ],
            "justification": "This method contributes to D2-Q4 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "PDETMunicipalPlanAnalyzer._interpret_risk algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _interpret_risk"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _interpret_risk"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _interpret_risk results for downstream analysis"
            ]
          }
        },
        "_compute_robustness_value": {
          "canonical_abbreviation": "pdet_analysis",
          "provides": "pdet_analysis.compute_robustness_value",
          "role": "_compute_robustness_value_execution",
          "usage_count_in_300": 40,
          "questions": [
            "Q009",
            "Q018",
            "Q022",
            "Q025",
            "Q039",
            "Q048",
            "Q052",
            "Q055",
            "Q069",
            "Q078",
            "Q082",
            "Q085",
            "Q099",
            "Q108",
            "Q112",
            "Q115",
            "Q129",
            "Q138",
            "Q142",
            "Q145",
            "Q159",
            "Q168",
            "Q172",
            "Q175",
            "Q189",
            "Q198",
            "Q202",
            "Q205",
            "Q219",
            "Q228",
            "Q232",
            "Q235",
            "Q249",
            "Q258",
            "Q262",
            "Q265",
            "Q279",
            "Q288",
            "Q292",
            "Q295"
          ],
          "base_slots": [
            "D2-Q4",
            "D4-Q3",
            "D5-Q2",
            "D5-Q5"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM02",
            "DIM04",
            "DIM05"
          ],
          "signature": {
            "line_number": 1620,
            "is_async": false,
            "parameters": [
              {
                "name": "effect",
                "type": "CausalEffect"
              },
              {
                "name": "dag",
                "type": "CausalDAG"
              }
            ],
            "return_type": "float",
            "docstring": "\n        Robustness Value: percentil de la distribución posterior que cruza cero\n        Valores altos (>0.95) indican alta robustez\n        "
          },
          "epistemological_foundation": {
            "paradigm": "PDETMunicipalPlanAnalyzer analytical paradigm",
            "ontological_basis": "Analysis via PDETMunicipalPlanAnalyzer._compute_robustness_value",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _compute_robustness_value implements structured analysis for D2-Q4"
            ],
            "justification": "This method contributes to D2-Q4 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "PDETMunicipalPlanAnalyzer._compute_robustness_value algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _compute_robustness_value"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _compute_robustness_value"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _compute_robustness_value results for downstream analysis"
            ]
          }
        },
        "_compute_e_value": {
          "canonical_abbreviation": "pdet_analysis",
          "provides": "pdet_analysis.compute_e_value",
          "role": "_compute_e_value_execution",
          "usage_count_in_300": 30,
          "questions": [
            "Q009",
            "Q022",
            "Q025",
            "Q039",
            "Q052",
            "Q055",
            "Q069",
            "Q082",
            "Q085",
            "Q099",
            "Q112",
            "Q115",
            "Q129",
            "Q142",
            "Q145",
            "Q159",
            "Q172",
            "Q175",
            "Q189",
            "Q202",
            "Q205",
            "Q219",
            "Q232",
            "Q235",
            "Q249",
            "Q262",
            "Q265",
            "Q279",
            "Q292",
            "Q295"
          ],
          "base_slots": [
            "D2-Q4",
            "D5-Q2",
            "D5-Q5"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM02",
            "DIM05"
          ],
          "signature": {
            "line_number": 1602,
            "is_async": false,
            "parameters": [
              {
                "name": "effect",
                "type": "CausalEffect"
              }
            ],
            "return_type": "float",
            "docstring": "\n        E-value: mínima fuerza de confounding no observado para anular el efecto\n        Fórmula: E = effect_estimate + sqrt(effect_estimate * (effect_estimate - 1))\n\n        Referencia: VanderWeele & Ding (2017) - Ann Intern Med\n        "
          },
          "epistemological_foundation": {
            "paradigm": "PDETMunicipalPlanAnalyzer analytical paradigm",
            "ontological_basis": "Analysis via PDETMunicipalPlanAnalyzer._compute_e_value",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _compute_e_value implements structured analysis for D2-Q4"
            ],
            "justification": "This method contributes to D2-Q4 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "PDETMunicipalPlanAnalyzer._compute_e_value algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _compute_e_value"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _compute_e_value"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _compute_e_value results for downstream analysis"
            ]
          }
        },
        "_interpret_sensitivity": {
          "canonical_abbreviation": "pdet_analysis",
          "provides": "pdet_analysis.interpret_sensitivity",
          "role": "_interpret_sensitivity_execution",
          "usage_count_in_300": 30,
          "questions": [
            "Q009",
            "Q022",
            "Q024",
            "Q039",
            "Q052",
            "Q054",
            "Q069",
            "Q082",
            "Q084",
            "Q099",
            "Q112",
            "Q114",
            "Q129",
            "Q142",
            "Q144",
            "Q159",
            "Q172",
            "Q174",
            "Q189",
            "Q202",
            "Q204",
            "Q219",
            "Q232",
            "Q234",
            "Q249",
            "Q262",
            "Q264",
            "Q279",
            "Q292",
            "Q294"
          ],
          "base_slots": [
            "D2-Q4",
            "D5-Q2",
            "D5-Q4"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM02",
            "DIM05"
          ],
          "signature": {
            "line_number": 1638,
            "is_async": false,
            "parameters": [
              {
                "name": "e_value",
                "type": "float"
              },
              {
                "name": "robustness",
                "type": "float"
              }
            ],
            "return_type": "str",
            "docstring": "Interpretación de resultados de sensibilidad"
          },
          "epistemological_foundation": {
            "paradigm": "PDETMunicipalPlanAnalyzer analytical paradigm",
            "ontological_basis": "Analysis via PDETMunicipalPlanAnalyzer._interpret_sensitivity",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _interpret_sensitivity implements structured analysis for D2-Q4"
            ],
            "justification": "This method contributes to D2-Q4 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "PDETMunicipalPlanAnalyzer._interpret_sensitivity algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _interpret_sensitivity"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _interpret_sensitivity"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _interpret_sensitivity results for downstream analysis"
            ]
          }
        },
        "_score_causal_coherence": {
          "canonical_abbreviation": "pdet_analysis",
          "provides": "pdet_analysis.score_causal_coherence",
          "role": "_score_causal_coherence_execution",
          "usage_count_in_300": 20,
          "questions": [
            "Q010",
            "Q020",
            "Q040",
            "Q050",
            "Q070",
            "Q080",
            "Q100",
            "Q110",
            "Q130",
            "Q140",
            "Q160",
            "Q170",
            "Q190",
            "Q200",
            "Q220",
            "Q230",
            "Q250",
            "Q260",
            "Q280",
            "Q290"
          ],
          "base_slots": [
            "D2-Q5",
            "D4-Q5"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM02",
            "DIM04"
          ],
          "signature": {
            "line_number": 1834,
            "is_async": false,
            "parameters": [
              {
                "name": "dag",
                "type": "CausalDAG"
              },
              {
                "name": "effects",
                "type": "list[CausalEffect]"
              }
            ],
            "return_type": "float",
            "docstring": "Score coherencia causal del plan (0-10)"
          },
          "epistemological_foundation": {
            "paradigm": "PDETMunicipalPlanAnalyzer analytical paradigm",
            "ontological_basis": "Analysis via PDETMunicipalPlanAnalyzer._score_causal_coherence",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _score_causal_coherence implements structured analysis for D2-Q5"
            ],
            "justification": "This method contributes to D2-Q5 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "PDETMunicipalPlanAnalyzer._score_causal_coherence algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _score_causal_coherence"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _score_causal_coherence"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _score_causal_coherence results for downstream analysis"
            ]
          }
        },
        "_score_indicators": {
          "canonical_abbreviation": "pdet_analysis",
          "provides": "pdet_analysis.score_indicators",
          "role": "_score_indicators_execution",
          "usage_count_in_300": 30,
          "questions": [
            "Q011",
            "Q012",
            "Q016",
            "Q041",
            "Q042",
            "Q046",
            "Q071",
            "Q072",
            "Q076",
            "Q101",
            "Q102",
            "Q106",
            "Q131",
            "Q132",
            "Q136",
            "Q161",
            "Q162",
            "Q166",
            "Q191",
            "Q192",
            "Q196",
            "Q221",
            "Q222",
            "Q226",
            "Q251",
            "Q252",
            "Q256",
            "Q281",
            "Q282",
            "Q286"
          ],
          "base_slots": [
            "D3-Q1",
            "D3-Q2",
            "D4-Q1"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM03",
            "DIM04"
          ],
          "signature": {
            "line_number": 1729,
            "is_async": false,
            "parameters": [
              {
                "name": "tables",
                "type": "list[ExtractedTable]"
              },
              {
                "name": "text",
                "type": "str"
              }
            ],
            "return_type": "float",
            "docstring": "Score calidad de indicadores (0-10)"
          },
          "epistemological_foundation": {
            "paradigm": "PDETMunicipalPlanAnalyzer analytical paradigm",
            "ontological_basis": "Analysis via PDETMunicipalPlanAnalyzer._score_indicators",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _score_indicators implements structured analysis for D3-Q1"
            ],
            "justification": "This method contributes to D3-Q1 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "PDETMunicipalPlanAnalyzer._score_indicators algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _score_indicators"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _score_indicators"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _score_indicators results for downstream analysis"
            ]
          }
        },
        "_get_spanish_stopwords": {
          "canonical_abbreviation": "pdet_analysis",
          "provides": "pdet_analysis.get_spanish_stopwords",
          "role": "_get_spanish_stopwords_execution",
          "usage_count_in_300": 10,
          "questions": [
            "Q012",
            "Q042",
            "Q072",
            "Q102",
            "Q132",
            "Q162",
            "Q192",
            "Q222",
            "Q252",
            "Q282"
          ],
          "base_slots": [
            "D3-Q2"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM03"
          ],
          "signature": {
            "line_number": 316,
            "is_async": false,
            "parameters": [],
            "return_type": "list[str]",
            "docstring": null
          },
          "epistemological_foundation": {
            "paradigm": "PDETMunicipalPlanAnalyzer analytical paradigm",
            "ontological_basis": "Analysis via PDETMunicipalPlanAnalyzer._get_spanish_stopwords",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _get_spanish_stopwords implements structured analysis for D3-Q2"
            ],
            "justification": "This method contributes to D3-Q2 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "PDETMunicipalPlanAnalyzer._get_spanish_stopwords algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _get_spanish_stopwords"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _get_spanish_stopwords"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _get_spanish_stopwords results for downstream analysis"
            ]
          }
        },
        "_assess_financial_sustainability": {
          "canonical_abbreviation": "pdet_analysis",
          "provides": "pdet_analysis.assess_financial_sustainability",
          "role": "_assess_financial_sustainability_execution",
          "usage_count_in_300": 10,
          "questions": [
            "Q012",
            "Q042",
            "Q072",
            "Q102",
            "Q132",
            "Q162",
            "Q192",
            "Q222",
            "Q252",
            "Q282"
          ],
          "base_slots": [
            "D3-Q2"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM03"
          ],
          "signature": {
            "line_number": 689,
            "is_async": false,
            "parameters": [
              {
                "name": "indicators",
                "type": "list[FinancialIndicator]"
              },
              {
                "name": "funding_sources",
                "type": "dict[str, Any]"
              }
            ],
            "return_type": "float",
            "docstring": null
          },
          "epistemological_foundation": {
            "paradigm": "PDETMunicipalPlanAnalyzer analytical paradigm",
            "ontological_basis": "Analysis via PDETMunicipalPlanAnalyzer._assess_financial_sustainability",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _assess_financial_sustainability implements structured analysis for D3-Q2"
            ],
            "justification": "This method contributes to D3-Q2 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "PDETMunicipalPlanAnalyzer._assess_financial_sustainability algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _assess_financial_sustainability"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _assess_financial_sustainability"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _assess_financial_sustainability results for downstream analysis"
            ]
          }
        },
        "_indicator_to_dict": {
          "canonical_abbreviation": "pdet_analysis",
          "provides": "pdet_analysis.indicator_to_dict",
          "role": "_indicator_to_dict_execution",
          "usage_count_in_300": 10,
          "questions": [
            "Q012",
            "Q042",
            "Q072",
            "Q102",
            "Q132",
            "Q162",
            "Q192",
            "Q222",
            "Q252",
            "Q282"
          ],
          "base_slots": [
            "D3-Q2"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM03"
          ],
          "signature": {
            "line_number": 762,
            "is_async": false,
            "parameters": [
              {
                "name": "ind",
                "type": "FinancialIndicator"
              }
            ],
            "return_type": "dict[str, Any]",
            "docstring": null
          },
          "epistemological_foundation": {
            "paradigm": "PDETMunicipalPlanAnalyzer analytical paradigm",
            "ontological_basis": "Analysis via PDETMunicipalPlanAnalyzer._indicator_to_dict",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _indicator_to_dict implements structured analysis for D3-Q2"
            ],
            "justification": "This method contributes to D3-Q2 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "PDETMunicipalPlanAnalyzer._indicator_to_dict algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _indicator_to_dict"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _indicator_to_dict"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _indicator_to_dict results for downstream analysis"
            ]
          }
        },
        "_generate_recommendations": {
          "canonical_abbreviation": "pdet_analysis",
          "provides": "pdet_analysis.generate_recommendations",
          "role": "_generate_recommendations_execution",
          "usage_count_in_300": 20,
          "questions": [
            "Q012",
            "Q015",
            "Q042",
            "Q045",
            "Q072",
            "Q075",
            "Q102",
            "Q105",
            "Q132",
            "Q135",
            "Q162",
            "Q165",
            "Q192",
            "Q195",
            "Q222",
            "Q225",
            "Q252",
            "Q255",
            "Q282",
            "Q285"
          ],
          "base_slots": [
            "D3-Q2",
            "D3-Q5"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM03"
          ],
          "signature": {
            "line_number": 2004,
            "is_async": false,
            "parameters": [
              {
                "name": "analysis_results",
                "type": "dict[str, Any]"
              }
            ],
            "return_type": "str",
            "docstring": "Genera recomendaciones específicas basadas en el análisis"
          },
          "epistemological_foundation": {
            "paradigm": "PDETMunicipalPlanAnalyzer analytical paradigm",
            "ontological_basis": "Analysis via PDETMunicipalPlanAnalyzer._generate_recommendations",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _generate_recommendations implements structured analysis for D3-Q2"
            ],
            "justification": "This method contributes to D3-Q2 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "PDETMunicipalPlanAnalyzer._generate_recommendations algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _generate_recommendations"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _generate_recommendations"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _generate_recommendations results for downstream analysis"
            ]
          }
        },
        "_extract_from_responsibility_tables": {
          "canonical_abbreviation": "pdet_analysis",
          "provides": "pdet_analysis.extract_from_responsibility_tables",
          "role": "_extract_from_responsibility_tables_extraction",
          "usage_count_in_300": 10,
          "questions": [
            "Q013",
            "Q043",
            "Q073",
            "Q103",
            "Q133",
            "Q163",
            "Q193",
            "Q223",
            "Q253",
            "Q283"
          ],
          "base_slots": [
            "D3-Q3"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM03"
          ],
          "signature": {
            "line_number": 860,
            "is_async": false,
            "parameters": [
              {
                "name": "tables",
                "type": "list[ExtractedTable]"
              }
            ],
            "return_type": "list[ResponsibleEntity]",
            "docstring": null
          },
          "epistemological_foundation": {
            "paradigm": "PDETMunicipalPlanAnalyzer analytical paradigm",
            "ontological_basis": "Analysis via PDETMunicipalPlanAnalyzer._extract_from_responsibility_tables",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _extract_from_responsibility_tables implements structured analysis for D3-Q3"
            ],
            "justification": "This method contributes to D3-Q3 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "PDETMunicipalPlanAnalyzer._extract_from_responsibility_tables algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _extract_from_responsibility_tables"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _extract_from_responsibility_tables"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _extract_from_responsibility_tables results for downstream analysis"
            ]
          }
        },
        "_score_responsibility_clarity": {
          "canonical_abbreviation": "pdet_analysis",
          "provides": "pdet_analysis.score_responsibility_clarity",
          "role": "_score_responsibility_clarity_execution",
          "usage_count_in_300": 10,
          "questions": [
            "Q013",
            "Q043",
            "Q073",
            "Q103",
            "Q133",
            "Q163",
            "Q193",
            "Q223",
            "Q253",
            "Q283"
          ],
          "base_slots": [
            "D3-Q3"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM03"
          ],
          "signature": {
            "line_number": 1768,
            "is_async": false,
            "parameters": [
              {
                "name": "entities",
                "type": "list[ResponsibleEntity]"
              }
            ],
            "return_type": "float",
            "docstring": "Score claridad de responsables (0-10)"
          },
          "epistemological_foundation": {
            "paradigm": "PDETMunicipalPlanAnalyzer analytical paradigm",
            "ontological_basis": "Analysis via PDETMunicipalPlanAnalyzer._score_responsibility_clarity",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _score_responsibility_clarity implements structured analysis for D3-Q3"
            ],
            "justification": "This method contributes to D3-Q3 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "PDETMunicipalPlanAnalyzer._score_responsibility_clarity algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _score_responsibility_clarity"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _score_responsibility_clarity"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _score_responsibility_clarity results for downstream analysis"
            ]
          }
        },
        "_entity_to_dict": {
          "canonical_abbreviation": "pdet_analysis",
          "provides": "pdet_analysis.entity_to_dict",
          "role": "_entity_to_dict_execution",
          "usage_count_in_300": 10,
          "questions": [
            "Q013",
            "Q043",
            "Q073",
            "Q103",
            "Q133",
            "Q163",
            "Q193",
            "Q223",
            "Q253",
            "Q283"
          ],
          "base_slots": [
            "D3-Q3"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM03"
          ],
          "signature": {
            "line_number": 2237,
            "is_async": false,
            "parameters": [
              {
                "name": "entity",
                "type": "ResponsibleEntity"
              }
            ],
            "return_type": "dict[str, Any]",
            "docstring": "Convierte ResponsibleEntity a diccionario"
          },
          "epistemological_foundation": {
            "paradigm": "PDETMunicipalPlanAnalyzer analytical paradigm",
            "ontological_basis": "Analysis via PDETMunicipalPlanAnalyzer._entity_to_dict",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _entity_to_dict implements structured analysis for D3-Q3"
            ],
            "justification": "This method contributes to D3-Q3 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "PDETMunicipalPlanAnalyzer._entity_to_dict algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _entity_to_dict"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _entity_to_dict"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _entity_to_dict results for downstream analysis"
            ]
          }
        },
        "_identify_confounders": {
          "canonical_abbreviation": "pdet_analysis",
          "provides": "pdet_analysis.identify_confounders",
          "role": "_identify_confounders_execution",
          "usage_count_in_300": 10,
          "questions": [
            "Q015",
            "Q045",
            "Q075",
            "Q105",
            "Q135",
            "Q165",
            "Q195",
            "Q225",
            "Q255",
            "Q285"
          ],
          "base_slots": [
            "D3-Q5"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM03"
          ],
          "signature": {
            "line_number": 1398,
            "is_async": false,
            "parameters": [
              {
                "name": "treatment",
                "type": "str"
              },
              {
                "name": "outcome",
                "type": "str"
              },
              {
                "name": "dag",
                "type": "CausalDAG"
              }
            ],
            "return_type": "list[str]",
            "docstring": "\n        Identifica confounders usando d-separation (Pearl, 2009)\n        "
          },
          "epistemological_foundation": {
            "paradigm": "PDETMunicipalPlanAnalyzer analytical paradigm",
            "ontological_basis": "Analysis via PDETMunicipalPlanAnalyzer._identify_confounders",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _identify_confounders implements structured analysis for D3-Q5"
            ],
            "justification": "This method contributes to D3-Q5 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "PDETMunicipalPlanAnalyzer._identify_confounders algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _identify_confounders"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _identify_confounders"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _identify_confounders results for downstream analysis"
            ]
          }
        },
        "_effect_to_dict": {
          "canonical_abbreviation": "pdet_analysis",
          "provides": "pdet_analysis.effect_to_dict",
          "role": "_effect_to_dict_execution",
          "usage_count_in_300": 10,
          "questions": [
            "Q015",
            "Q045",
            "Q075",
            "Q105",
            "Q135",
            "Q165",
            "Q195",
            "Q225",
            "Q255",
            "Q285"
          ],
          "base_slots": [
            "D3-Q5"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM03"
          ],
          "signature": {
            "line_number": 2249,
            "is_async": false,
            "parameters": [
              {
                "name": "effect",
                "type": "CausalEffect"
              }
            ],
            "return_type": "dict[str, Any]",
            "docstring": "Convierte CausalEffect a diccionario"
          },
          "epistemological_foundation": {
            "paradigm": "PDETMunicipalPlanAnalyzer analytical paradigm",
            "ontological_basis": "Analysis via PDETMunicipalPlanAnalyzer._effect_to_dict",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _effect_to_dict implements structured analysis for D3-Q5"
            ],
            "justification": "This method contributes to D3-Q5 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "PDETMunicipalPlanAnalyzer._effect_to_dict algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _effect_to_dict"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _effect_to_dict"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _effect_to_dict results for downstream analysis"
            ]
          }
        },
        "_scenario_to_dict": {
          "canonical_abbreviation": "pdet_analysis",
          "provides": "pdet_analysis.scenario_to_dict",
          "role": "_scenario_to_dict_execution",
          "usage_count_in_300": 10,
          "questions": [
            "Q015",
            "Q045",
            "Q075",
            "Q105",
            "Q135",
            "Q165",
            "Q195",
            "Q225",
            "Q255",
            "Q285"
          ],
          "base_slots": [
            "D3-Q5"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM03"
          ],
          "signature": {
            "line_number": 2265,
            "is_async": false,
            "parameters": [
              {
                "name": "scenario",
                "type": "CounterfactualScenario"
              }
            ],
            "return_type": "dict[str, Any]",
            "docstring": "Convierte CounterfactualScenario a diccionario"
          },
          "epistemological_foundation": {
            "paradigm": "PDETMunicipalPlanAnalyzer analytical paradigm",
            "ontological_basis": "Analysis via PDETMunicipalPlanAnalyzer._scenario_to_dict",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _scenario_to_dict implements structured analysis for D3-Q5"
            ],
            "justification": "This method contributes to D3-Q5 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "PDETMunicipalPlanAnalyzer._scenario_to_dict algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _scenario_to_dict"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _scenario_to_dict"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _scenario_to_dict results for downstream analysis"
            ]
          }
        },
        "_identify_causal_nodes": {
          "canonical_abbreviation": "pdet_analysis",
          "provides": "pdet_analysis.identify_causal_nodes",
          "role": "_identify_causal_nodes_execution",
          "usage_count_in_300": 10,
          "questions": [
            "Q015",
            "Q045",
            "Q075",
            "Q105",
            "Q135",
            "Q165",
            "Q195",
            "Q225",
            "Q255",
            "Q285"
          ],
          "base_slots": [
            "D3-Q5"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM03"
          ],
          "signature": {
            "line_number": 996,
            "is_async": false,
            "parameters": [
              {
                "name": "text",
                "type": "str"
              },
              {
                "name": "tables",
                "type": "list[ExtractedTable]"
              },
              {
                "name": "financial_analysis",
                "type": "dict[str, Any]"
              }
            ],
            "return_type": "dict[str, CausalNode]",
            "docstring": null
          },
          "epistemological_foundation": {
            "paradigm": "PDETMunicipalPlanAnalyzer analytical paradigm",
            "ontological_basis": "Analysis via PDETMunicipalPlanAnalyzer._identify_causal_nodes",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _identify_causal_nodes implements structured analysis for D3-Q5"
            ],
            "justification": "This method contributes to D3-Q5 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "PDETMunicipalPlanAnalyzer._identify_causal_nodes algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _identify_causal_nodes"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _identify_causal_nodes"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _identify_causal_nodes results for downstream analysis"
            ]
          }
        },
        "estimate_causal_effects": {
          "canonical_abbreviation": "pdet_analysis",
          "provides": "pdet_analysis.estimate_causal_effects",
          "role": "estimate_causal_effects_execution",
          "usage_count_in_300": 10,
          "questions": [
            "Q015",
            "Q045",
            "Q075",
            "Q105",
            "Q135",
            "Q165",
            "Q195",
            "Q225",
            "Q255",
            "Q285"
          ],
          "base_slots": [
            "D3-Q5"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM03"
          ],
          "signature": {
            "line_number": 1280,
            "is_async": false,
            "parameters": [
              {
                "name": "dag",
                "type": "CausalDAG"
              },
              {
                "name": "text",
                "type": "str"
              },
              {
                "name": "financial_analysis",
                "type": "dict[str, Any]"
              }
            ],
            "return_type": "list[CausalEffect]",
            "docstring": null
          },
          "epistemological_foundation": {
            "paradigm": "PDETMunicipalPlanAnalyzer analytical paradigm",
            "ontological_basis": "Analysis via PDETMunicipalPlanAnalyzer.estimate_causal_effects",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method estimate_causal_effects implements structured analysis for D3-Q5"
            ],
            "justification": "This method contributes to D3-Q5 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "PDETMunicipalPlanAnalyzer.estimate_causal_effects algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute estimate_causal_effects"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from estimate_causal_effects"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use estimate_causal_effects results for downstream analysis"
            ]
          }
        },
        "generate_counterfactuals": {
          "canonical_abbreviation": "pdet_analysis",
          "provides": "pdet_analysis.generate_counterfactuals",
          "role": "generate_counterfactuals_execution",
          "usage_count_in_300": 20,
          "questions": [
            "Q015",
            "Q021",
            "Q045",
            "Q051",
            "Q075",
            "Q081",
            "Q105",
            "Q111",
            "Q135",
            "Q141",
            "Q165",
            "Q171",
            "Q195",
            "Q201",
            "Q225",
            "Q231",
            "Q255",
            "Q261",
            "Q285",
            "Q291"
          ],
          "base_slots": [
            "D3-Q5",
            "D5-Q1"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM03",
            "DIM05"
          ],
          "signature": {
            "line_number": 1419,
            "is_async": false,
            "parameters": [
              {
                "name": "dag",
                "type": "CausalDAG"
              },
              {
                "name": "causal_effects",
                "type": "list[CausalEffect]"
              },
              {
                "name": "financial_analysis",
                "type": "dict[str, Any]"
              }
            ],
            "return_type": "list[CounterfactualScenario]",
            "docstring": "\n        Genera escenarios contrafactuales usando el framework de Pearl (2009)\n        Level 3 - Counterfactual: \"What if we had done X instead of Y?\"\n\n        Implementación basada en:\n        - Pearl & Mackenzie (2018) - The Book of Why\n        - Sharma & Kiciman (2020) - DoWhy: An End-to-End Library for Causal Inference\n        "
          },
          "epistemological_foundation": {
            "paradigm": "PDETMunicipalPlanAnalyzer analytical paradigm",
            "ontological_basis": "Analysis via PDETMunicipalPlanAnalyzer.generate_counterfactuals",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method generate_counterfactuals implements structured analysis for D3-Q5"
            ],
            "justification": "This method contributes to D3-Q5 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "PDETMunicipalPlanAnalyzer.generate_counterfactuals algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute generate_counterfactuals"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from generate_counterfactuals"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use generate_counterfactuals results for downstream analysis"
            ]
          }
        },
        "_find_outcome_mentions": {
          "canonical_abbreviation": "pdet_analysis",
          "provides": "pdet_analysis.find_outcome_mentions",
          "role": "_find_outcome_mentions_execution",
          "usage_count_in_300": 10,
          "questions": [
            "Q016",
            "Q046",
            "Q076",
            "Q106",
            "Q136",
            "Q166",
            "Q196",
            "Q226",
            "Q256",
            "Q286"
          ],
          "base_slots": [
            "D4-Q1"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM04"
          ],
          "signature": {
            "line_number": 1066,
            "is_async": false,
            "parameters": [
              {
                "name": "text",
                "type": "str"
              },
              {
                "name": "outcome",
                "type": "str"
              }
            ],
            "return_type": "list[str]",
            "docstring": null
          },
          "epistemological_foundation": {
            "paradigm": "PDETMunicipalPlanAnalyzer analytical paradigm",
            "ontological_basis": "Analysis via PDETMunicipalPlanAnalyzer._find_outcome_mentions",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _find_outcome_mentions implements structured analysis for D4-Q1"
            ],
            "justification": "This method contributes to D4-Q1 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "PDETMunicipalPlanAnalyzer._find_outcome_mentions algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _find_outcome_mentions"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _find_outcome_mentions"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _find_outcome_mentions results for downstream analysis"
            ]
          }
        },
        "_score_temporal_consistency": {
          "canonical_abbreviation": "pdet_analysis",
          "provides": "pdet_analysis.score_temporal_consistency",
          "role": "_score_temporal_consistency_execution",
          "usage_count_in_300": 10,
          "questions": [
            "Q016",
            "Q046",
            "Q076",
            "Q106",
            "Q136",
            "Q166",
            "Q196",
            "Q226",
            "Q256",
            "Q286"
          ],
          "base_slots": [
            "D4-Q1"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM04"
          ],
          "signature": {
            "line_number": 1786,
            "is_async": false,
            "parameters": [
              {
                "name": "text",
                "type": "str"
              },
              {
                "name": "tables",
                "type": "list[ExtractedTable]"
              }
            ],
            "return_type": "float",
            "docstring": "Score consistencia temporal (0-10)"
          },
          "epistemological_foundation": {
            "paradigm": "PDETMunicipalPlanAnalyzer analytical paradigm",
            "ontological_basis": "Analysis via PDETMunicipalPlanAnalyzer._score_temporal_consistency",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _score_temporal_consistency implements structured analysis for D4-Q1"
            ],
            "justification": "This method contributes to D4-Q1 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "PDETMunicipalPlanAnalyzer._score_temporal_consistency algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _score_temporal_consistency"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _score_temporal_consistency"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _score_temporal_consistency results for downstream analysis"
            ]
          }
        },
        "_get_prior_effect": {
          "canonical_abbreviation": "pdet_analysis",
          "provides": "pdet_analysis.get_prior_effect",
          "role": "_get_prior_effect_execution",
          "usage_count_in_300": 10,
          "questions": [
            "Q018",
            "Q048",
            "Q078",
            "Q108",
            "Q138",
            "Q168",
            "Q198",
            "Q228",
            "Q258",
            "Q288"
          ],
          "base_slots": [
            "D4-Q3"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM04"
          ],
          "signature": {
            "line_number": 1377,
            "is_async": false,
            "parameters": [
              {
                "name": "treatment",
                "type": "str"
              },
              {
                "name": "outcome",
                "type": "str"
              }
            ],
            "return_type": "tuple[float, float]",
            "docstring": "\n        Priors informados basados en meta-análisis de programas PDET\n        Referencia: Cinelli et al. (2022) - Sensitivity Analysis for Causal Inference\n        "
          },
          "epistemological_foundation": {
            "paradigm": "PDETMunicipalPlanAnalyzer analytical paradigm",
            "ontological_basis": "Analysis via PDETMunicipalPlanAnalyzer._get_prior_effect",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _get_prior_effect implements structured analysis for D4-Q3"
            ],
            "justification": "This method contributes to D4-Q3 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "PDETMunicipalPlanAnalyzer._get_prior_effect algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _get_prior_effect"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _get_prior_effect"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _get_prior_effect results for downstream analysis"
            ]
          }
        },
        "_estimate_effect_bayesian": {
          "canonical_abbreviation": "pdet_analysis",
          "provides": "pdet_analysis.estimate_effect_bayesian",
          "role": "_estimate_effect_bayesian_execution",
          "usage_count_in_300": 10,
          "questions": [
            "Q018",
            "Q048",
            "Q078",
            "Q108",
            "Q138",
            "Q168",
            "Q198",
            "Q228",
            "Q258",
            "Q288"
          ],
          "base_slots": [
            "D4-Q3"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM04"
          ],
          "signature": {
            "line_number": 1306,
            "is_async": false,
            "parameters": [
              {
                "name": "treatment",
                "type": "str"
              },
              {
                "name": "outcome",
                "type": "str"
              },
              {
                "name": "dag",
                "type": "CausalDAG"
              },
              {
                "name": "financial_analysis",
                "type": "dict[str, Any]"
              }
            ],
            "return_type": "CausalEffect | None",
            "docstring": null
          },
          "epistemological_foundation": {
            "paradigm": "PDETMunicipalPlanAnalyzer analytical paradigm",
            "ontological_basis": "Analysis via PDETMunicipalPlanAnalyzer._estimate_effect_bayesian",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _estimate_effect_bayesian implements structured analysis for D4-Q3"
            ],
            "justification": "This method contributes to D4-Q3 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "PDETMunicipalPlanAnalyzer._estimate_effect_bayesian algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _estimate_effect_bayesian"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _estimate_effect_bayesian"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _estimate_effect_bayesian results for downstream analysis"
            ]
          }
        },
        "_score_pdet_alignment": {
          "canonical_abbreviation": "pdet_analysis",
          "provides": "pdet_analysis.score_pdet_alignment",
          "role": "_score_pdet_alignment_execution",
          "usage_count_in_300": 10,
          "questions": [
            "Q020",
            "Q050",
            "Q080",
            "Q110",
            "Q140",
            "Q170",
            "Q200",
            "Q230",
            "Q260",
            "Q290"
          ],
          "base_slots": [
            "D4-Q5"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM04"
          ],
          "signature": {
            "line_number": 1808,
            "is_async": false,
            "parameters": [
              {
                "name": "text",
                "type": "str"
              },
              {
                "name": "tables",
                "type": "list[ExtractedTable]"
              },
              {
                "name": "dag",
                "type": "CausalDAG"
              }
            ],
            "return_type": "float",
            "docstring": "Score alineación con pilares PDET (0-10)"
          },
          "epistemological_foundation": {
            "paradigm": "PDETMunicipalPlanAnalyzer analytical paradigm",
            "ontological_basis": "Analysis via PDETMunicipalPlanAnalyzer._score_pdet_alignment",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _score_pdet_alignment implements structured analysis for D4-Q5"
            ],
            "justification": "This method contributes to D4-Q5 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "PDETMunicipalPlanAnalyzer._score_pdet_alignment algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _score_pdet_alignment"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _score_pdet_alignment"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _score_pdet_alignment results for downstream analysis"
            ]
          }
        },
        "_generate_scenario_narrative": {
          "canonical_abbreviation": "pdet_analysis",
          "provides": "pdet_analysis.generate_scenario_narrative",
          "role": "_generate_scenario_narrative_execution",
          "usage_count_in_300": 10,
          "questions": [
            "Q021",
            "Q051",
            "Q081",
            "Q111",
            "Q141",
            "Q171",
            "Q201",
            "Q231",
            "Q261",
            "Q291"
          ],
          "base_slots": [
            "D5-Q1"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM05"
          ],
          "signature": {
            "line_number": 1548,
            "is_async": false,
            "parameters": [
              {
                "name": "description",
                "type": "str"
              },
              {
                "name": "intervention",
                "type": "dict[str, float]"
              },
              {
                "name": "predicted_outcomes",
                "type": "dict[str, tuple[float, float, float]]"
              },
              {
                "name": "probabilities",
                "type": "dict[str, float]"
              }
            ],
            "return_type": "str",
            "docstring": "Genera narrativa interpretable del escenario contrafactual"
          },
          "epistemological_foundation": {
            "paradigm": "PDETMunicipalPlanAnalyzer analytical paradigm",
            "ontological_basis": "Analysis via PDETMunicipalPlanAnalyzer._generate_scenario_narrative",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _generate_scenario_narrative implements structured analysis for D5-Q1"
            ],
            "justification": "This method contributes to D5-Q1 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "PDETMunicipalPlanAnalyzer._generate_scenario_narrative algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _generate_scenario_narrative"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _generate_scenario_narrative"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _generate_scenario_narrative results for downstream analysis"
            ]
          }
        },
        "_find_mediator_mentions": {
          "canonical_abbreviation": "pdet_analysis",
          "provides": "pdet_analysis.find_mediator_mentions",
          "role": "_find_mediator_mentions_execution",
          "usage_count_in_300": 20,
          "questions": [
            "Q021",
            "Q023",
            "Q051",
            "Q053",
            "Q081",
            "Q083",
            "Q111",
            "Q113",
            "Q141",
            "Q143",
            "Q171",
            "Q173",
            "Q201",
            "Q203",
            "Q231",
            "Q233",
            "Q261",
            "Q263",
            "Q291",
            "Q293"
          ],
          "base_slots": [
            "D5-Q1",
            "D5-Q3"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM05"
          ],
          "signature": {
            "line_number": 1099,
            "is_async": false,
            "parameters": [
              {
                "name": "text",
                "type": "str"
              },
              {
                "name": "mediator",
                "type": "str"
              }
            ],
            "return_type": "list[str]",
            "docstring": null
          },
          "epistemological_foundation": {
            "paradigm": "PDETMunicipalPlanAnalyzer analytical paradigm",
            "ontological_basis": "Analysis via PDETMunicipalPlanAnalyzer._find_mediator_mentions",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _find_mediator_mentions implements structured analysis for D5-Q1"
            ],
            "justification": "This method contributes to D5-Q1 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "PDETMunicipalPlanAnalyzer._find_mediator_mentions algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _find_mediator_mentions"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _find_mediator_mentions"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _find_mediator_mentions results for downstream analysis"
            ]
          }
        },
        "calculate_quality_score": {
          "canonical_abbreviation": "pdet_analysis",
          "provides": "pdet_analysis.calculate_quality_score",
          "role": "calculate_quality_score_calculation",
          "usage_count_in_300": 10,
          "questions": [
            "Q022",
            "Q052",
            "Q082",
            "Q112",
            "Q142",
            "Q172",
            "Q202",
            "Q232",
            "Q262",
            "Q292"
          ],
          "base_slots": [
            "D5-Q2"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM05"
          ],
          "signature": {
            "line_number": 1655,
            "is_async": false,
            "parameters": [
              {
                "name": "text",
                "type": "str"
              },
              {
                "name": "tables",
                "type": "list[ExtractedTable]"
              },
              {
                "name": "financial_analysis",
                "type": "dict[str, Any]"
              },
              {
                "name": "responsible_entities",
                "type": "list[ResponsibleEntity]"
              },
              {
                "name": "causal_dag",
                "type": "CausalDAG"
              },
              {
                "name": "causal_effects",
                "type": "list[CausalEffect]"
              }
            ],
            "return_type": "QualityScore",
            "docstring": "\n        Puntaje bayesiano integral de calidad del PDM\n        Integra todas las dimensiones de análisis con pesos calibrados\n        "
          },
          "epistemological_foundation": {
            "paradigm": "PDETMunicipalPlanAnalyzer analytical paradigm",
            "ontological_basis": "Analysis via PDETMunicipalPlanAnalyzer.calculate_quality_score",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method calculate_quality_score implements structured analysis for D5-Q2"
            ],
            "justification": "This method contributes to D5-Q2 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "PDETMunicipalPlanAnalyzer.calculate_quality_score algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute calculate_quality_score"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from calculate_quality_score"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use calculate_quality_score results for downstream analysis"
            ]
          }
        },
        "_estimate_score_confidence": {
          "canonical_abbreviation": "pdet_analysis",
          "provides": "pdet_analysis.estimate_score_confidence",
          "role": "_estimate_score_confidence_execution",
          "usage_count_in_300": 10,
          "questions": [
            "Q022",
            "Q052",
            "Q082",
            "Q112",
            "Q142",
            "Q172",
            "Q202",
            "Q232",
            "Q262",
            "Q292"
          ],
          "base_slots": [
            "D5-Q2"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM05"
          ],
          "signature": {
            "line_number": 1859,
            "is_async": false,
            "parameters": [
              {
                "name": "scores",
                "type": "np.ndarray"
              },
              {
                "name": "weights",
                "type": "np.ndarray"
              }
            ],
            "return_type": "tuple[float, float]",
            "docstring": "Estima intervalo de confianza para el score usando bootstrap"
          },
          "epistemological_foundation": {
            "paradigm": "PDETMunicipalPlanAnalyzer analytical paradigm",
            "ontological_basis": "Analysis via PDETMunicipalPlanAnalyzer._estimate_score_confidence",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _estimate_score_confidence implements structured analysis for D5-Q2"
            ],
            "justification": "This method contributes to D5-Q2 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "PDETMunicipalPlanAnalyzer._estimate_score_confidence algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _estimate_score_confidence"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _estimate_score_confidence"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _estimate_score_confidence results for downstream analysis"
            ]
          }
        },
        "_interpret_overall_quality": {
          "canonical_abbreviation": "pdet_analysis",
          "provides": "pdet_analysis.interpret_overall_quality",
          "role": "_interpret_overall_quality_execution",
          "usage_count_in_300": 10,
          "questions": [
            "Q022",
            "Q052",
            "Q082",
            "Q112",
            "Q142",
            "Q172",
            "Q202",
            "Q232",
            "Q262",
            "Q292"
          ],
          "base_slots": [
            "D5-Q2"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM05"
          ],
          "signature": {
            "line_number": 1982,
            "is_async": false,
            "parameters": [
              {
                "name": "score",
                "type": "float"
              }
            ],
            "return_type": "str",
            "docstring": "Interpretación del score global"
          },
          "epistemological_foundation": {
            "paradigm": "PDETMunicipalPlanAnalyzer analytical paradigm",
            "ontological_basis": "Analysis via PDETMunicipalPlanAnalyzer._interpret_overall_quality",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _interpret_overall_quality implements structured analysis for D5-Q2"
            ],
            "justification": "This method contributes to D5-Q2 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "PDETMunicipalPlanAnalyzer._interpret_overall_quality algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _interpret_overall_quality"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _interpret_overall_quality"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _interpret_overall_quality results for downstream analysis"
            ]
          }
        },
        "_quality_to_dict": {
          "canonical_abbreviation": "pdet_analysis",
          "provides": "pdet_analysis.quality_to_dict",
          "role": "_quality_to_dict_execution",
          "usage_count_in_300": 10,
          "questions": [
            "Q022",
            "Q052",
            "Q082",
            "Q112",
            "Q142",
            "Q172",
            "Q202",
            "Q232",
            "Q262",
            "Q292"
          ],
          "base_slots": [
            "D5-Q2"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM05"
          ],
          "signature": {
            "line_number": 2275,
            "is_async": false,
            "parameters": [
              {
                "name": "quality",
                "type": "QualityScore"
              }
            ],
            "return_type": "dict[str, Any]",
            "docstring": "Convierte QualityScore a diccionario"
          },
          "epistemological_foundation": {
            "paradigm": "PDETMunicipalPlanAnalyzer analytical paradigm",
            "ontological_basis": "Analysis via PDETMunicipalPlanAnalyzer._quality_to_dict",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _quality_to_dict implements structured analysis for D5-Q2"
            ],
            "justification": "This method contributes to D5-Q2 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "PDETMunicipalPlanAnalyzer._quality_to_dict algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _quality_to_dict"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _quality_to_dict"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _quality_to_dict results for downstream analysis"
            ]
          }
        },
        "generate_executive_report": {
          "canonical_abbreviation": "pdet_analysis",
          "provides": "pdet_analysis.generate_executive_report",
          "role": "generate_executive_report_execution",
          "usage_count_in_300": 10,
          "questions": [
            "Q022",
            "Q052",
            "Q082",
            "Q112",
            "Q142",
            "Q172",
            "Q202",
            "Q232",
            "Q262",
            "Q292"
          ],
          "base_slots": [
            "D5-Q2"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM05"
          ],
          "signature": {
            "line_number": 1899,
            "is_async": false,
            "parameters": [
              {
                "name": "analysis_results",
                "type": "dict[str, Any]"
              }
            ],
            "return_type": "str",
            "docstring": "Genera reporte ejecutivo en Markdown"
          },
          "epistemological_foundation": {
            "paradigm": "PDETMunicipalPlanAnalyzer analytical paradigm",
            "ontological_basis": "Analysis via PDETMunicipalPlanAnalyzer.generate_executive_report",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method generate_executive_report implements structured analysis for D5-Q2"
            ],
            "justification": "This method contributes to D5-Q2 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "PDETMunicipalPlanAnalyzer.generate_executive_report algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute generate_executive_report"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from generate_executive_report"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use generate_executive_report results for downstream analysis"
            ]
          }
        },
        "_break_cycles": {
          "canonical_abbreviation": "pdet_analysis",
          "provides": "pdet_analysis.break_cycles",
          "role": "_break_cycles_execution",
          "usage_count_in_300": 10,
          "questions": [
            "Q024",
            "Q054",
            "Q084",
            "Q114",
            "Q144",
            "Q174",
            "Q204",
            "Q234",
            "Q264",
            "Q294"
          ],
          "base_slots": [
            "D5-Q4"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM05"
          ],
          "signature": {
            "line_number": 1264,
            "is_async": false,
            "parameters": [
              {
                "name": "G",
                "type": "nx.DiGraph"
              }
            ],
            "return_type": "nx.DiGraph",
            "docstring": null
          },
          "epistemological_foundation": {
            "paradigm": "PDETMunicipalPlanAnalyzer analytical paradigm",
            "ontological_basis": "Analysis via PDETMunicipalPlanAnalyzer._break_cycles",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _break_cycles implements structured analysis for D5-Q4"
            ],
            "justification": "This method contributes to D5-Q4 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "PDETMunicipalPlanAnalyzer._break_cycles algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _break_cycles"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _break_cycles"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _break_cycles results for downstream analysis"
            ]
          }
        },
        "export_causal_network": {
          "canonical_abbreviation": "pdet_analysis",
          "provides": "pdet_analysis.export_causal_network",
          "role": "export_causal_network_execution",
          "usage_count_in_300": 10,
          "questions": [
            "Q026",
            "Q056",
            "Q086",
            "Q116",
            "Q146",
            "Q176",
            "Q206",
            "Q236",
            "Q266",
            "Q296"
          ],
          "base_slots": [
            "D6-Q1"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM06"
          ],
          "signature": {
            "line_number": 1881,
            "is_async": false,
            "parameters": [
              {
                "name": "dag",
                "type": "CausalDAG"
              },
              {
                "name": "output_path",
                "type": "str"
              }
            ],
            "return_type": "None",
            "docstring": "Exporta el DAG causal en formato GraphML para Gephi/Cytoscape"
          },
          "epistemological_foundation": {
            "paradigm": "PDETMunicipalPlanAnalyzer analytical paradigm",
            "ontological_basis": "Analysis via PDETMunicipalPlanAnalyzer.export_causal_network",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method export_causal_network implements structured analysis for D6-Q1"
            ],
            "justification": "This method contributes to D6-Q1 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "PDETMunicipalPlanAnalyzer.export_causal_network algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute export_causal_network"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from export_causal_network"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use export_causal_network results for downstream analysis"
            ]
          }
        }
      }
    },
    "PolicyContradictionDetector": {
      "file_path": "src/methods_dispensary/contradiction_deteccion.py",
      "line_number": 328,
      "class_docstring": "\n    Sistema avanzado de detección de contradicciones para PDMs colombianos.\n    Implementa el estado del arte en NLP y razonamiento lógico.\n    ",
      "total_usage_in_300_contracts": 120,
      "unique_questions": 40,
      "questions": [
        "Q001",
        "Q002",
        "Q010",
        "Q019",
        "Q031",
        "Q032",
        "Q040",
        "Q049",
        "Q061",
        "Q062",
        "Q070",
        "Q079",
        "Q091",
        "Q092",
        "Q100",
        "Q109",
        "Q121",
        "Q122",
        "Q130",
        "Q139",
        "Q151",
        "Q152",
        "Q160",
        "Q169",
        "Q181",
        "Q182",
        "Q190",
        "Q199",
        "Q211",
        "Q212",
        "Q220",
        "Q229",
        "Q241",
        "Q242",
        "Q250",
        "Q259",
        "Q271",
        "Q272",
        "Q280",
        "Q289"
      ],
      "base_slots": [
        "D1-Q1",
        "D1-Q2",
        "D2-Q5",
        "D4-Q4"
      ],
      "policy_areas": [
        "PA01",
        "PA02",
        "PA03",
        "PA04",
        "PA05",
        "PA06",
        "PA07",
        "PA08",
        "PA09",
        "PA10"
      ],
      "dimensions": [
        "DIM01",
        "DIM02",
        "DIM04"
      ],
      "methods": {
        "_extract_quantitative_claims": {
          "canonical_abbreviation": "contradiction_detection",
          "provides": "contradiction_detection.extract_quantitative_claims",
          "role": "_extract_quantitative_claims_extraction",
          "usage_count_in_300": 10,
          "questions": [
            "Q001",
            "Q031",
            "Q061",
            "Q091",
            "Q121",
            "Q151",
            "Q181",
            "Q211",
            "Q241",
            "Q271"
          ],
          "base_slots": [
            "D1-Q1"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM01"
          ],
          "signature": {
            "line_number": 1187,
            "is_async": false,
            "parameters": [
              {
                "name": "text",
                "type": "str"
              }
            ],
            "return_type": "list[dict[str, Any]]",
            "docstring": "Extrae afirmaciones cuantitativas estructuradas"
          },
          "epistemological_foundation": {
            "paradigm": "Claim extraction for contradiction detection",
            "ontological_basis": "Policies make quantitative claims (e.g., 'VBG rate is 15%') that can be contradictory or inconsistent",
            "epistemological_stance": "Logical consistency checking: Policy documents should be internally consistent in their quantitative claims",
            "theoretical_framework": [
              "Coherence theory of justification: Contradictory claims undermine policy credibility"
            ],
            "justification": "Extracting quantitative claims enables detecting contradictions that signal data quality issues or errors"
          },
          "technical_approach": {
            "method_type": "quantitative_proposition_extraction",
            "algorithm": "Extract statements with quantifiers, normalize for comparison",
            "steps": [
              {
                "step": 1,
                "description": "Identify quantitative statements (X es Y%, la tasa de X es Y)"
              },
              {
                "step": 2,
                "description": "Parse subject (X) and value (Y)"
              },
              {
                "step": 3,
                "description": "Store as structured claim {subject, predicate, value}"
              }
            ],
            "assumptions": [
              "Quantitative claims have standard subject-predicate-value structure",
              "Same subjects are referred to consistently (no synonyms)"
            ],
            "limitations": [
              "Cannot resolve synonyms (tasa de VBG vs. violencia de género)",
              "May extract claims that are not meant to be factual (hypothetical scenarios)"
            ],
            "complexity": "O(n) where n=quantitative statements"
          },
          "output_interpretation": {
            "output_structure": {
              "claims": "List of {subject, value, unit, sentence_id, confidence}"
            },
            "interpretation_guide": {
              "duplicate_subjects": "Multiple claims about same subject: Check for contradictions",
              "unique_subjects": "Each claim covers distinct topic"
            },
            "actionable_insights": [
              "Contradictory claims: Data quality issue or conceptual confusion",
              "Consistent claims: Internal coherence of baseline data"
            ]
          }
        },
        "_parse_number": {
          "canonical_abbreviation": "contradiction_detection",
          "provides": "contradiction_detection.parse_number",
          "role": "_parse_number_parsing",
          "usage_count_in_300": 10,
          "questions": [
            "Q001",
            "Q031",
            "Q061",
            "Q091",
            "Q121",
            "Q151",
            "Q181",
            "Q211",
            "Q241",
            "Q271"
          ],
          "base_slots": [
            "D1-Q1"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM01"
          ],
          "signature": {
            "line_number": 1218,
            "is_async": false,
            "parameters": [
              {
                "name": "text",
                "type": "str"
              }
            ],
            "return_type": "float",
            "docstring": "Parsea número desde texto"
          },
          "epistemological_foundation": {
            "paradigm": "Numerical data normalization",
            "ontological_basis": "Numbers are objective entities that must be correctly parsed for valid comparisons",
            "epistemological_stance": "Objectivism about numbers: Numeric values have objective meanings independent of representation",
            "theoretical_framework": [
              "Formal semantics of numbers: Numerical representations map to abstract mathematical objects"
            ],
            "justification": "Detecting contradictions requires comparing numbers; parsing ensures accurate comparison"
          },
          "technical_approach": {
            "method_type": "robust_numeric_parsing",
            "algorithm": "Handle multiple numeric formats, convert to standard float",
            "steps": [
              {
                "step": 1,
                "description": "Detect numeric patterns (12.5%, 1.234, 1,234.56)"
              },
              {
                "step": 2,
                "description": "Normalize separators (. vs , for decimals/thousands)"
              },
              {
                "step": 3,
                "description": "Convert to float, preserve precision"
              }
            ],
            "assumptions": [
              "Colombian format conventions (. for thousands, , for decimals or vice versa depending on context)"
            ],
            "limitations": [
              "Ambiguity in separator usage (1.234 could be 1234 or 1.234)",
              "May fail on non-standard formats"
            ],
            "complexity": "O(n) where n=numeric strings"
          },
          "output_interpretation": {
            "output_structure": {
              "parsed_value": "Float representation of number",
              "parsing_confidence": "Confidence in parsing correctness (0-1)"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.95: Unambiguous format",
              "low_confidence": "<0.8: Ambiguous, may be misparsed"
            },
            "actionable_insights": [
              "Low parsing confidence: Document uses non-standard number formatting"
            ]
          }
        },
        "_statistical_significance_test": {
          "canonical_abbreviation": "contradiction_detection",
          "provides": "contradiction_detection.statistical_significance_test",
          "role": "_statistical_significance_test_execution",
          "usage_count_in_300": 10,
          "questions": [
            "Q001",
            "Q031",
            "Q061",
            "Q091",
            "Q121",
            "Q151",
            "Q181",
            "Q211",
            "Q241",
            "Q271"
          ],
          "base_slots": [
            "D1-Q1"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM01"
          ],
          "signature": {
            "line_number": 1423,
            "is_async": false,
            "parameters": [
              {
                "name": "claim_a",
                "type": "dict"
              },
              {
                "name": "claim_b",
                "type": "dict"
              }
            ],
            "return_type": "float",
            "docstring": "Realiza test de significancia estadística"
          },
          "epistemological_foundation": {
            "paradigm": "Frequentist hypothesis testing",
            "ontological_basis": "Observed differences between claimed values may be due to sampling error or genuine contradictions",
            "epistemological_stance": "Statistical inference: Use hypothesis testing to distinguish signal from noise",
            "theoretical_framework": [
              "Frequentist statistics: p-values quantify evidence against null hypothesis (no difference)",
              "Error control: Significance testing controls Type I error (false positive contradictions)"
            ],
            "justification": "Not all numeric discrepancies are meaningful; statistical testing prevents false alarms from minor variations"
          },
          "technical_approach": {
            "method_type": "hypothesis_testing_for_contradictions",
            "algorithm": "Apply t-test or chi-square test to compare claimed values",
            "steps": [
              {
                "step": 1,
                "description": "For each pair of claims about same subject, compute difference"
              },
              {
                "step": 2,
                "description": "Estimate measurement error (if metadata available)"
              },
              {
                "step": 3,
                "description": "Perform significance test (H0: difference = 0)"
              },
              {
                "step": 4,
                "description": "Flag contradiction if p < 0.05"
              }
            ],
            "assumptions": [
              "Measurement errors are normally distributed",
              "Alpha = 0.05 is appropriate threshold"
            ],
            "limitations": [
              "Requires error estimates (often unavailable in policy documents)",
              "Significance ≠ practical importance"
            ],
            "complexity": "O(n²) for pairwise comparisons of n claims"
          },
          "output_interpretation": {
            "output_structure": {
              "test_results": "List of {claim1_id, claim2_id, difference, p_value, significant}"
            },
            "interpretation_guide": {
              "p < 0.05": "Statistically significant contradiction",
              "p ≥ 0.05": "Difference not statistically significant"
            },
            "actionable_insights": [
              "Significant contradictions: Data quality problem requiring investigation",
              "Non-significant differences: Likely rounding or measurement error"
            ]
          }
        },
        "_detect_numerical_inconsistencies": {
          "canonical_abbreviation": "contradiction_detection",
          "provides": "contradiction_detection.detect_numerical_inconsistencies",
          "role": "_detect_numerical_inconsistencies_detection",
          "usage_count_in_300": 10,
          "questions": [
            "Q002",
            "Q032",
            "Q062",
            "Q092",
            "Q122",
            "Q152",
            "Q182",
            "Q212",
            "Q242",
            "Q272"
          ],
          "base_slots": [
            "D1-Q2"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM01"
          ],
          "signature": {
            "line_number": 612,
            "is_async": false,
            "parameters": [
              {
                "name": "statements",
                "type": "list[PolicyStatement]"
              }
            ],
            "return_type": "list[ContradictionEvidence]",
            "docstring": "Detecta inconsistencias numéricas con análisis estadístico"
          },
          "epistemological_foundation": {
            "paradigm": "PolicyContradictionDetector analytical paradigm",
            "ontological_basis": "Analysis via PolicyContradictionDetector._detect_numerical_inconsistencies",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _detect_numerical_inconsistencies implements structured analysis for D1-Q2"
            ],
            "justification": "This method contributes to D1-Q2 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "PolicyContradictionDetector._detect_numerical_inconsistencies algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _detect_numerical_inconsistencies"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _detect_numerical_inconsistencies"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _detect_numerical_inconsistencies results for downstream analysis"
            ]
          }
        },
        "_calculate_numerical_divergence": {
          "canonical_abbreviation": "contradiction_detection",
          "provides": "contradiction_detection.calculate_numerical_divergence",
          "role": "_calculate_numerical_divergence_calculation",
          "usage_count_in_300": 10,
          "questions": [
            "Q002",
            "Q032",
            "Q062",
            "Q092",
            "Q122",
            "Q152",
            "Q182",
            "Q212",
            "Q242",
            "Q272"
          ],
          "base_slots": [
            "D1-Q2"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM01"
          ],
          "signature": {
            "line_number": 1403,
            "is_async": false,
            "parameters": [
              {
                "name": "claim_a",
                "type": "dict"
              },
              {
                "name": "claim_b",
                "type": "dict"
              }
            ],
            "return_type": "float | None",
            "docstring": "Calcula divergencia entre valores numéricos"
          },
          "epistemological_foundation": {
            "paradigm": "PolicyContradictionDetector analytical paradigm",
            "ontological_basis": "Analysis via PolicyContradictionDetector._calculate_numerical_divergence",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _calculate_numerical_divergence implements structured analysis for D1-Q2"
            ],
            "justification": "This method contributes to D1-Q2 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "PolicyContradictionDetector._calculate_numerical_divergence algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _calculate_numerical_divergence"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _calculate_numerical_divergence"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _calculate_numerical_divergence results for downstream analysis"
            ]
          }
        },
        "_detect_logical_incompatibilities": {
          "canonical_abbreviation": "contradiction_detection",
          "provides": "contradiction_detection.detect_logical_incompatibilities",
          "role": "_detect_logical_incompatibilities_detection",
          "usage_count_in_300": 10,
          "questions": [
            "Q010",
            "Q040",
            "Q070",
            "Q100",
            "Q130",
            "Q160",
            "Q190",
            "Q220",
            "Q250",
            "Q280"
          ],
          "base_slots": [
            "D2-Q5"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM02"
          ],
          "signature": {
            "line_number": 708,
            "is_async": false,
            "parameters": [
              {
                "name": "statements",
                "type": "list[PolicyStatement]"
              }
            ],
            "return_type": "list[ContradictionEvidence]",
            "docstring": "Detecta incompatibilidades lógicas usando razonamiento en grafo"
          },
          "epistemological_foundation": {
            "paradigm": "PolicyContradictionDetector analytical paradigm",
            "ontological_basis": "Analysis via PolicyContradictionDetector._detect_logical_incompatibilities",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _detect_logical_incompatibilities implements structured analysis for D2-Q5"
            ],
            "justification": "This method contributes to D2-Q5 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "PolicyContradictionDetector._detect_logical_incompatibilities algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _detect_logical_incompatibilities"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _detect_logical_incompatibilities"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _detect_logical_incompatibilities results for downstream analysis"
            ]
          }
        },
        "_calculate_coherence_metrics": {
          "canonical_abbreviation": "contradiction_detection",
          "provides": "contradiction_detection.calculate_coherence_metrics",
          "role": "_calculate_coherence_metrics_calculation",
          "usage_count_in_300": 10,
          "questions": [
            "Q010",
            "Q040",
            "Q070",
            "Q100",
            "Q130",
            "Q160",
            "Q190",
            "Q220",
            "Q250",
            "Q280"
          ],
          "base_slots": [
            "D2-Q5"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM02"
          ],
          "signature": {
            "line_number": 816,
            "is_async": false,
            "parameters": [
              {
                "name": "contradictions",
                "type": "list[ContradictionEvidence]"
              },
              {
                "name": "statements",
                "type": "list[PolicyStatement]"
              },
              {
                "name": "text",
                "type": "str"
              }
            ],
            "return_type": "dict[str, float]",
            "docstring": "Calcula métricas avanzadas de coherencia del documento"
          },
          "epistemological_foundation": {
            "paradigm": "PolicyContradictionDetector analytical paradigm",
            "ontological_basis": "Analysis via PolicyContradictionDetector._calculate_coherence_metrics",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _calculate_coherence_metrics implements structured analysis for D2-Q5"
            ],
            "justification": "This method contributes to D2-Q5 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "PolicyContradictionDetector._calculate_coherence_metrics algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _calculate_coherence_metrics"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _calculate_coherence_metrics"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _calculate_coherence_metrics results for downstream analysis"
            ]
          }
        },
        "_calculate_objective_alignment": {
          "canonical_abbreviation": "contradiction_detection",
          "provides": "contradiction_detection.calculate_objective_alignment",
          "role": "_calculate_objective_alignment_calculation",
          "usage_count_in_300": 20,
          "questions": [
            "Q010",
            "Q019",
            "Q040",
            "Q049",
            "Q070",
            "Q079",
            "Q100",
            "Q109",
            "Q130",
            "Q139",
            "Q160",
            "Q169",
            "Q190",
            "Q199",
            "Q220",
            "Q229",
            "Q250",
            "Q259",
            "Q280",
            "Q289"
          ],
          "base_slots": [
            "D2-Q5",
            "D4-Q4"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM02",
            "DIM04"
          ],
          "signature": {
            "line_number": 901,
            "is_async": false,
            "parameters": [
              {
                "name": "statements",
                "type": "list[PolicyStatement]"
              }
            ],
            "return_type": "float",
            "docstring": "Calcula alineación entre objetivos declarados"
          },
          "epistemological_foundation": {
            "paradigm": "PolicyContradictionDetector analytical paradigm",
            "ontological_basis": "Analysis via PolicyContradictionDetector._calculate_objective_alignment",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _calculate_objective_alignment implements structured analysis for D2-Q5"
            ],
            "justification": "This method contributes to D2-Q5 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "PolicyContradictionDetector._calculate_objective_alignment algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _calculate_objective_alignment"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _calculate_objective_alignment"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _calculate_objective_alignment results for downstream analysis"
            ]
          }
        },
        "_calculate_graph_fragmentation": {
          "canonical_abbreviation": "contradiction_detection",
          "provides": "contradiction_detection.calculate_graph_fragmentation",
          "role": "_calculate_graph_fragmentation_calculation",
          "usage_count_in_300": 10,
          "questions": [
            "Q010",
            "Q040",
            "Q070",
            "Q100",
            "Q130",
            "Q160",
            "Q190",
            "Q220",
            "Q250",
            "Q280"
          ],
          "base_slots": [
            "D2-Q5"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM02"
          ],
          "signature": {
            "line_number": 928,
            "is_async": false,
            "parameters": [],
            "return_type": "float",
            "docstring": "Calcula fragmentación del grafo de conocimiento"
          },
          "epistemological_foundation": {
            "paradigm": "PolicyContradictionDetector analytical paradigm",
            "ontological_basis": "Analysis via PolicyContradictionDetector._calculate_graph_fragmentation",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _calculate_graph_fragmentation implements structured analysis for D2-Q5"
            ],
            "justification": "This method contributes to D2-Q5 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "PolicyContradictionDetector._calculate_graph_fragmentation algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _calculate_graph_fragmentation"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _calculate_graph_fragmentation"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _calculate_graph_fragmentation results for downstream analysis"
            ]
          }
        },
        "_identify_affected_sections": {
          "canonical_abbreviation": "contradiction_detection",
          "provides": "contradiction_detection.identify_affected_sections",
          "role": "_identify_affected_sections_execution",
          "usage_count_in_300": 10,
          "questions": [
            "Q019",
            "Q049",
            "Q079",
            "Q109",
            "Q139",
            "Q169",
            "Q199",
            "Q229",
            "Q259",
            "Q289"
          ],
          "base_slots": [
            "D4-Q4"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM04"
          ],
          "signature": {
            "line_number": 1105,
            "is_async": false,
            "parameters": [
              {
                "name": "conflicts",
                "type": "list[ContradictionEvidence]"
              }
            ],
            "return_type": "list[str]",
            "docstring": "Identifica secciones del plan afectadas por contradicciones"
          },
          "epistemological_foundation": {
            "paradigm": "PolicyContradictionDetector analytical paradigm",
            "ontological_basis": "Analysis via PolicyContradictionDetector._identify_affected_sections",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _identify_affected_sections implements structured analysis for D4-Q4"
            ],
            "justification": "This method contributes to D4-Q4 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "PolicyContradictionDetector._identify_affected_sections algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _identify_affected_sections"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _identify_affected_sections"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _identify_affected_sections results for downstream analysis"
            ]
          }
        },
        "_generate_resolution_recommendations": {
          "canonical_abbreviation": "contradiction_detection",
          "provides": "contradiction_detection.generate_resolution_recommendations",
          "role": "_generate_resolution_recommendations_execution",
          "usage_count_in_300": 10,
          "questions": [
            "Q019",
            "Q049",
            "Q079",
            "Q109",
            "Q139",
            "Q169",
            "Q199",
            "Q229",
            "Q259",
            "Q289"
          ],
          "base_slots": [
            "D4-Q4"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM04"
          ],
          "signature": {
            "line_number": 1031,
            "is_async": false,
            "parameters": [
              {
                "name": "contradictions",
                "type": "list[ContradictionEvidence]"
              }
            ],
            "return_type": "list[dict[str, Any]]",
            "docstring": "Genera recomendaciones específicas para resolver contradicciones"
          },
          "epistemological_foundation": {
            "paradigm": "PolicyContradictionDetector analytical paradigm",
            "ontological_basis": "Analysis via PolicyContradictionDetector._generate_resolution_recommendations",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _generate_resolution_recommendations implements structured analysis for D4-Q4"
            ],
            "justification": "This method contributes to D4-Q4 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "PolicyContradictionDetector._generate_resolution_recommendations algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _generate_resolution_recommendations"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _generate_resolution_recommendations"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _generate_resolution_recommendations results for downstream analysis"
            ]
          }
        }
      }
    },
    "BayesianNumericalAnalyzer": {
      "file_path": "src/methods_dispensary/embedding_policy.py",
      "line_number": 492,
      "class_docstring": "\n    Bayesian framework for uncertainty-aware numerical policy analysis.\n\n    Implements:\n    - Beta-Binomial conjugate prior for proportions\n    - Normal-Normal conjugate prior for continuous metrics\n    - Bayesian hypothesis testing for policy comparisons\n    - Credible interval estimation\n    - Evidence strength quantification (Bayes factors)\n    ",
      "total_usage_in_300_contracts": 20,
      "unique_questions": 10,
      "questions": [
        "Q001",
        "Q031",
        "Q061",
        "Q091",
        "Q121",
        "Q151",
        "Q181",
        "Q211",
        "Q241",
        "Q271"
      ],
      "base_slots": [
        "D1-Q1"
      ],
      "policy_areas": [
        "PA01",
        "PA02",
        "PA03",
        "PA04",
        "PA05",
        "PA06",
        "PA07",
        "PA08",
        "PA09",
        "PA10"
      ],
      "dimensions": [
        "DIM01"
      ],
      "methods": {
        "evaluate_policy_metric": {
          "canonical_abbreviation": "bayesian_analysis",
          "provides": "bayesian_analysis.evaluate_policy_metric",
          "role": "evaluate_policy_metric_evaluation",
          "usage_count_in_300": 10,
          "questions": [
            "Q001",
            "Q031",
            "Q061",
            "Q091",
            "Q121",
            "Q151",
            "Q181",
            "Q211",
            "Q241",
            "Q271"
          ],
          "base_slots": [
            "D1-Q1"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM01"
          ],
          "signature": {
            "line_number": 515,
            "is_async": false,
            "parameters": [
              {
                "name": "observed_values",
                "type": "list[float]"
              },
              {
                "name": "n_posterior_samples",
                "type": "int",
                "default": "10000"
              },
              {
                "name": "**kwargs",
                "type": "Any"
              }
            ],
            "return_type": "BayesianEvaluation",
            "docstring": "\n        Bayesian evaluation of policy metric with uncertainty quantification.\n\n        Returns posterior distribution, credible intervals, and evidence strength.\n\n        Args:\n            observed_values: List of observed metric values\n            n_posterior_samples: Number of posterior samples to generate\n            **kwargs: Additional optional parameters for compatibility\n\n        Returns:\n            BayesianEvaluation with posterior samples and credible intervals\n        "
          },
          "epistemological_foundation": {
            "paradigm": "Bayesian statistical inference",
            "ontological_basis": "Policy metrics (e.g., VBG rates) are uncertain quantities with probability distributions, not point estimates",
            "epistemological_stance": "Bayesian epistemology: Knowledge is represented as probability distributions updated via Bayes' theorem",
            "theoretical_framework": [
              "Bayesian inference: Prior beliefs + data → posterior beliefs",
              "Uncertainty quantification: Bayesian methods explicitly model uncertainty"
            ],
            "justification": "Policy baselines often come from small samples or imperfect data; Bayesian methods quantify uncertainty properly"
          },
          "technical_approach": {
            "method_type": "bayesian_posterior_computation",
            "algorithm": "Combine prior distribution with observed data to compute posterior",
            "steps": [
              {
                "step": 1,
                "description": "Specify prior distribution for metric (e.g., Beta for rates)"
              },
              {
                "step": 2,
                "description": "Incorporate observed data (e.g., count of VBG cases, population)"
              },
              {
                "step": 3,
                "description": "Compute posterior distribution via conjugate updates or MCMC"
              },
              {
                "step": 4,
                "description": "Extract posterior mean, credible intervals"
              }
            ],
            "assumptions": [
              "Prior is appropriate for metric domain",
              "Data generation process matches likelihood model"
            ],
            "limitations": [
              "Prior choice affects results (subjective)",
              "Computational cost for non-conjugate models"
            ],
            "complexity": "O(MCMC iterations) for non-conjugate, O(1) for conjugate"
          },
          "output_interpretation": {
            "output_structure": {
              "posterior_mean": "Point estimate of metric",
              "credible_interval": "95% Bayesian credible interval",
              "posterior_distribution": "Full posterior (if needed for downstream analysis)"
            },
            "interpretation_guide": {
              "narrow_CI": "High precision, low uncertainty",
              "wide_CI": "High uncertainty, more data needed"
            },
            "actionable_insights": [
              "Wide credible intervals: Baseline estimates unreliable, strengthen data collection",
              "Posterior far from prior: Data strongly updates beliefs"
            ]
          }
        },
        "compare_policies": {
          "canonical_abbreviation": "bayesian_analysis",
          "provides": "bayesian_analysis.compare_policies",
          "role": "compare_policies_comparison",
          "usage_count_in_300": 10,
          "questions": [
            "Q001",
            "Q031",
            "Q061",
            "Q091",
            "Q121",
            "Q151",
            "Q181",
            "Q211",
            "Q241",
            "Q271"
          ],
          "base_slots": [
            "D1-Q1"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM01"
          ],
          "signature": {
            "line_number": 724,
            "is_async": false,
            "parameters": [
              {
                "name": "policy_a_values",
                "type": "list[float]"
              },
              {
                "name": "policy_b_values",
                "type": "list[float]"
              }
            ],
            "return_type": "dict[str, Any]",
            "docstring": "\n        Bayesian comparison of two policy metrics.\n\n        Returns probability that A > B and Bayes factor.\n        "
          },
          "epistemological_foundation": {
            "paradigm": "Bayesian comparative analysis",
            "ontological_basis": "Comparing municipalities or policy cycles requires accounting for uncertainty in all estimates",
            "epistemological_stance": "Probabilistic comparison: Compare posterior distributions, not point estimates",
            "theoretical_framework": [
              "Bayesian model comparison: Quantify probability that policy A > policy B",
              "Decision theory: Bayesian methods support rational decision-making under uncertainty"
            ],
            "justification": "Ranking municipalities by baseline indicators requires proper uncertainty quantification to avoid spurious rankings"
          },
          "technical_approach": {
            "method_type": "posterior_distribution_comparison",
            "algorithm": "Compute P(metric_A > metric_B | data) from posterior samples",
            "steps": [
              {
                "step": 1,
                "description": "Obtain posterior distributions for both policies"
              },
              {
                "step": 2,
                "description": "Draw samples from both posteriors"
              },
              {
                "step": 3,
                "description": "Compute proportion of samples where policy A > policy B"
              },
              {
                "step": 4,
                "description": "Report probability and effect size"
              }
            ],
            "assumptions": [
              "Posteriors are correctly specified",
              "Sufficient samples for stable probability estimate"
            ],
            "limitations": [
              "Requires posterior samples (computational cost)",
              "Interpretation requires Bayesian literacy"
            ],
            "complexity": "O(S) where S=number of posterior samples"
          },
          "output_interpretation": {
            "output_structure": {
              "prob_A_better": "P(metric_A > metric_B | data)",
              "effect_size": "Mean difference in metric",
              "overlap": "Degree of posterior overlap"
            },
            "interpretation_guide": {
              "P > 0.95": "Strong evidence A is better",
              "0.75 < P < 0.95": "Moderate evidence",
              "P ≈ 0.5": "No clear difference"
            },
            "actionable_insights": [
              "High overlap despite different point estimates: Differences not meaningful given uncertainty",
              "Clear separation: Confident ranking"
            ]
          }
        }
      }
    },
    "SemanticProcessor": {
      "file_path": "src/methods_dispensary/semantic_chunking_policy.py",
      "line_number": 129,
      "class_docstring": "\n    State-of-the-art semantic processing with:\n    - BGE-M3 embeddings (2024 SOTA)\n    - Policy-aware chunking (respects PDM structure)\n    - Efficient batching with FP16\n    ",
      "total_usage_in_300_contracts": 50,
      "unique_questions": 20,
      "questions": [
        "Q001",
        "Q030",
        "Q031",
        "Q060",
        "Q061",
        "Q090",
        "Q091",
        "Q120",
        "Q121",
        "Q150",
        "Q151",
        "Q180",
        "Q181",
        "Q210",
        "Q211",
        "Q240",
        "Q241",
        "Q270",
        "Q271",
        "Q300"
      ],
      "base_slots": [
        "D1-Q1",
        "D6-Q5"
      ],
      "policy_areas": [
        "PA01",
        "PA02",
        "PA03",
        "PA04",
        "PA05",
        "PA06",
        "PA07",
        "PA08",
        "PA09",
        "PA10"
      ],
      "dimensions": [
        "DIM01",
        "DIM06"
      ],
      "methods": {
        "chunk_text": {
          "canonical_abbreviation": "semantic_processing",
          "provides": "semantic_processing.chunk_text",
          "role": "chunk_text_chunking",
          "usage_count_in_300": 20,
          "questions": [
            "Q001",
            "Q030",
            "Q031",
            "Q060",
            "Q061",
            "Q090",
            "Q091",
            "Q120",
            "Q121",
            "Q150",
            "Q151",
            "Q180",
            "Q181",
            "Q210",
            "Q211",
            "Q240",
            "Q241",
            "Q270",
            "Q271",
            "Q300"
          ],
          "base_slots": [
            "D1-Q1",
            "D6-Q5"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM01",
            "DIM06"
          ],
          "signature": {
            "line_number": 172,
            "is_async": false,
            "parameters": [
              {
                "name": "text",
                "type": "str"
              },
              {
                "name": "preserve_structure",
                "type": "bool",
                "default": "True"
              }
            ],
            "return_type": "list[dict[str, Any]]",
            "docstring": "\n        Policy-aware semantic chunking:\n        - Respects section boundaries (numbered lists, headers)\n        - Maintains table integrity\n        - Preserves reference links between text segments\n        "
          },
          "epistemological_foundation": {
            "paradigm": "Semantic preprocessing for neural methods",
            "ontological_basis": "Long documents must be divided into chunks that fit neural model context windows while preserving semantic coherence",
            "epistemological_stance": "Pragmatism: Chunking strategies are justified by their utility for downstream tasks (embedding, retrieval)",
            "theoretical_framework": [
              "Information retrieval: Chunk granularity affects retrieval precision and recall",
              "Neural model constraints: Transformer models have fixed context windows requiring chunking"
            ],
            "justification": "Semantic search and embedding require chunked text; good chunking preserves semantic units (paragraphs, sections)"
          },
          "technical_approach": {
            "method_type": "semantic_aware_text_chunking",
            "algorithm": "Chunk by paragraph/section with overlap to preserve context",
            "steps": [
              {
                "step": 1,
                "description": "Identify natural boundaries (paragraphs, sections)"
              },
              {
                "step": 2,
                "description": "Create chunks respecting boundaries, max N tokens"
              },
              {
                "step": 3,
                "description": "Add overlap (e.g., 50 tokens) between chunks for continuity"
              }
            ],
            "assumptions": [
              "Document has clear structural boundaries",
              "Overlap improves retrieval (empirical assumption)"
            ],
            "limitations": [
              "Fixed chunk size may split semantic units",
              "Overlap increases storage and computation"
            ],
            "complexity": "O(n) where n=document length"
          },
          "output_interpretation": {
            "output_structure": {
              "chunks": "List of {chunk_text, start_pos, end_pos, chunk_id}"
            },
            "interpretation_guide": {
              "many_small_chunks": "Document is dense or highly structured",
              "few_large_chunks": "Document is sparse or unstructured"
            },
            "actionable_insights": [
              "Chunk boundaries split semantic units: Consider different chunking strategy",
              "Chunks align with document structure: Good chunking"
            ]
          }
        },
        "embed_single": {
          "canonical_abbreviation": "semantic_processing",
          "provides": "semantic_processing.embed_single",
          "role": "embed_single_embedding",
          "usage_count_in_300": 10,
          "questions": [
            "Q001",
            "Q031",
            "Q061",
            "Q091",
            "Q121",
            "Q151",
            "Q181",
            "Q211",
            "Q241",
            "Q271"
          ],
          "base_slots": [
            "D1-Q1"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM01"
          ],
          "signature": {
            "line_number": 290,
            "is_async": false,
            "parameters": [
              {
                "name": "text",
                "type": "str"
              }
            ],
            "return_type": "NDArray[np.floating[Any]]",
            "docstring": "Single text embedding"
          },
          "epistemological_foundation": {
            "paradigm": "Distributional semantics via neural embeddings",
            "ontological_basis": "Text meaning can be represented as high-dimensional vectors in semantic space",
            "epistemological_stance": "Distributional hypothesis: Words/phrases with similar meanings occur in similar contexts, captured by embeddings",
            "theoretical_framework": [
              "Vector semantics: Meaning is position in vector space",
              "Neural language models: Transformers learn contextual representations"
            ],
            "justification": "Embeddings enable semantic similarity search and clustering beyond keyword matching"
          },
          "technical_approach": {
            "method_type": "transformer_based_text_embedding",
            "algorithm": "Pass text through pretrained transformer, extract embedding vector",
            "steps": [
              {
                "step": 1,
                "description": "Tokenize text for transformer model"
              },
              {
                "step": 2,
                "description": "Pass through model (e.g., BERT, MPNet)"
              },
              {
                "step": 3,
                "description": "Extract embedding (e.g., [CLS] token or mean pooling)"
              },
              {
                "step": 4,
                "description": "Normalize to unit vector"
              }
            ],
            "assumptions": [
              "Pretrained model generalizes to policy domain",
              "Embedding dimensionality (e.g., 768) captures relevant semantics"
            ],
            "limitations": [
              "Domain shift: Model trained on general text may miss domain-specific nuances",
              "Black box: Embeddings are not directly interpretable"
            ],
            "complexity": "O(L) where L=sequence length for transformer forward pass"
          },
          "output_interpretation": {
            "output_structure": {
              "embedding": "Dense vector (e.g., 768-dim float array)",
              "model_id": "Identifier of embedding model used"
            },
            "interpretation_guide": {
              "cosine_similarity": "Use cosine similarity to compare embeddings",
              "clustering": "High-dimensional clustering reveals thematic groups"
            },
            "actionable_insights": [
              "Low similarity between baseline and goals: Diagnostic-planning disconnect",
              "High similarity: Coherent evidence-based planning"
            ]
          }
        },
        "_detect_pdm_structure": {
          "canonical_abbreviation": "semantic_processing",
          "provides": "semantic_processing.detect_pdm_structure",
          "role": "_detect_pdm_structure_detection",
          "usage_count_in_300": 10,
          "questions": [
            "Q030",
            "Q060",
            "Q090",
            "Q120",
            "Q150",
            "Q180",
            "Q210",
            "Q240",
            "Q270",
            "Q300"
          ],
          "base_slots": [
            "D6-Q5"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM06"
          ],
          "signature": {
            "line_number": 215,
            "is_async": false,
            "parameters": [
              {
                "name": "text",
                "type": "str"
              }
            ],
            "return_type": "list[dict[str, Any]]",
            "docstring": "Detect PDM sections using Colombian policy document patterns"
          },
          "epistemological_foundation": {
            "paradigm": "SemanticProcessor analytical paradigm",
            "ontological_basis": "Analysis via SemanticProcessor._detect_pdm_structure",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _detect_pdm_structure implements structured analysis for D6-Q5"
            ],
            "justification": "This method contributes to D6-Q5 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "SemanticProcessor._detect_pdm_structure algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _detect_pdm_structure"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _detect_pdm_structure"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _detect_pdm_structure results for downstream analysis"
            ]
          }
        },
        "_detect_table": {
          "canonical_abbreviation": "semantic_processing",
          "provides": "semantic_processing.detect_table",
          "role": "_detect_table_detection",
          "usage_count_in_300": 10,
          "questions": [
            "Q030",
            "Q060",
            "Q090",
            "Q120",
            "Q150",
            "Q180",
            "Q210",
            "Q240",
            "Q270",
            "Q300"
          ],
          "base_slots": [
            "D6-Q5"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM06"
          ],
          "signature": {
            "line_number": 243,
            "is_async": false,
            "parameters": [
              {
                "name": "text",
                "type": "str"
              }
            ],
            "return_type": "bool",
            "docstring": "Detect if chunk contains tabular data"
          },
          "epistemological_foundation": {
            "paradigm": "SemanticProcessor analytical paradigm",
            "ontological_basis": "Analysis via SemanticProcessor._detect_table",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _detect_table implements structured analysis for D6-Q5"
            ],
            "justification": "This method contributes to D6-Q5 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "SemanticProcessor._detect_table algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _detect_table"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _detect_table"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _detect_table results for downstream analysis"
            ]
          }
        }
      }
    },
    "OperationalizationAuditor": {
      "file_path": "src/methods_dispensary/derek_beach.py",
      "line_number": 2285,
      "class_docstring": "Audit operationalization quality",
      "total_usage_in_300_contracts": 90,
      "unique_questions": 70,
      "questions": [
        "Q002",
        "Q004",
        "Q009",
        "Q010",
        "Q011",
        "Q019",
        "Q024",
        "Q032",
        "Q034",
        "Q039",
        "Q040",
        "Q041",
        "Q049",
        "Q054",
        "Q062",
        "Q064",
        "Q069",
        "Q070",
        "Q071",
        "Q079",
        "Q084",
        "Q092",
        "Q094",
        "Q099",
        "Q100",
        "Q101",
        "Q109",
        "Q114",
        "Q122",
        "Q124",
        "Q129",
        "Q130",
        "Q131",
        "Q139",
        "Q144",
        "Q152",
        "Q154",
        "Q159",
        "Q160",
        "Q161",
        "Q169",
        "Q174",
        "Q182",
        "Q184",
        "Q189",
        "Q190",
        "Q191",
        "Q199",
        "Q204",
        "Q212",
        "Q214",
        "Q219",
        "Q220",
        "Q221",
        "Q229",
        "Q234",
        "Q242",
        "Q244",
        "Q249",
        "Q250",
        "Q251",
        "Q259",
        "Q264",
        "Q272",
        "Q274",
        "Q279",
        "Q280",
        "Q281",
        "Q289",
        "Q294"
      ],
      "base_slots": [
        "D1-Q2",
        "D1-Q4",
        "D2-Q4",
        "D2-Q5",
        "D3-Q1",
        "D4-Q4",
        "D5-Q4"
      ],
      "policy_areas": [
        "PA01",
        "PA02",
        "PA03",
        "PA04",
        "PA05",
        "PA06",
        "PA07",
        "PA08",
        "PA09",
        "PA10"
      ],
      "dimensions": [
        "DIM01",
        "DIM02",
        "DIM03",
        "DIM04",
        "DIM05"
      ],
      "methods": {
        "_audit_direct_evidence": {
          "canonical_abbreviation": "operationalizationauditor",
          "provides": "operationalizationauditor.audit_direct_evidence",
          "role": "_audit_direct_evidence_execution",
          "usage_count_in_300": 10,
          "questions": [
            "Q002",
            "Q032",
            "Q062",
            "Q092",
            "Q122",
            "Q152",
            "Q182",
            "Q212",
            "Q242",
            "Q272"
          ],
          "base_slots": [
            "D1-Q2"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM01"
          ],
          "signature": {
            "line_number": 2558,
            "is_async": false,
            "parameters": [
              {
                "name": "nodes",
                "type": "dict[str, MetaNode]"
              },
              {
                "name": "scm_dag",
                "type": "nx.DiGraph"
              },
              {
                "name": "historical_data",
                "type": "dict[str, Any]"
              }
            ],
            "return_type": "dict[str, dict[str, Any]]",
            "docstring": "Layer 1: Audit direct evidence of required components\n\n        Enhanced with highly specific Bayesian priors for rare evidence items.\n        Example: D2-Q4 risk matrix, D5-Q5 unwanted effects are rare in poor PDMs.\n        "
          },
          "epistemological_foundation": {
            "paradigm": "OperationalizationAuditor analytical paradigm",
            "ontological_basis": "Analysis via OperationalizationAuditor._audit_direct_evidence",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _audit_direct_evidence implements structured analysis for D1-Q2"
            ],
            "justification": "This method contributes to D1-Q2 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "OperationalizationAuditor._audit_direct_evidence algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _audit_direct_evidence"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _audit_direct_evidence"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _audit_direct_evidence results for downstream analysis"
            ]
          }
        },
        "_audit_systemic_risk": {
          "canonical_abbreviation": "operationalizationauditor",
          "provides": "operationalizationauditor.audit_systemic_risk",
          "role": "_audit_systemic_risk_execution",
          "usage_count_in_300": 30,
          "questions": [
            "Q002",
            "Q009",
            "Q024",
            "Q032",
            "Q039",
            "Q054",
            "Q062",
            "Q069",
            "Q084",
            "Q092",
            "Q099",
            "Q114",
            "Q122",
            "Q129",
            "Q144",
            "Q152",
            "Q159",
            "Q174",
            "Q182",
            "Q189",
            "Q204",
            "Q212",
            "Q219",
            "Q234",
            "Q242",
            "Q249",
            "Q264",
            "Q272",
            "Q279",
            "Q294"
          ],
          "base_slots": [
            "D1-Q2",
            "D2-Q4",
            "D5-Q4"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM01",
            "DIM02",
            "DIM05"
          ],
          "signature": {
            "line_number": 2715,
            "is_async": false,
            "parameters": [
              {
                "name": "nodes",
                "type": "dict[str, MetaNode]"
              },
              {
                "name": "graph",
                "type": "nx.DiGraph"
              },
              {
                "name": "direct_evidence",
                "type": "dict[str, dict[str, Any]]"
              },
              {
                "name": "causal_implications",
                "type": "dict[str, dict[str, Any]]"
              },
              {
                "name": "pdet_alignment",
                "type": "float | None",
                "default": "None"
              }
            ],
            "return_type": "dict[str, Any]",
            "docstring": "\n        AUDIT POINT 2.3: Policy Alignment Dual Constraint\n        Layer 3: Calculate systemic risk from accumulated omissions\n\n        Harmonic Front 3 - Enhancement 1: Alignment and Systemic Risk Linkage\n        Incorporates Policy Alignment scores (PND, ODS, RRI) as variable in systemic risk.\n\n        For D5-Q4 (Riesgos Sistémicos) and D4-Q5 (Alineación):\n        - If pdet_alignment ≤ 0.60), applies 1.2× multiplier to risk_score\n        - Excelente on D5-Q4 requires risk_score < 0.10)\n\n      "
          },
          "epistemological_foundation": {
            "paradigm": "OperationalizationAuditor analytical paradigm",
            "ontological_basis": "Analysis via OperationalizationAuditor._audit_systemic_risk",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _audit_systemic_risk implements structured analysis for D1-Q2"
            ],
            "justification": "This method contributes to D1-Q2 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "OperationalizationAuditor._audit_systemic_risk algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _audit_systemic_risk"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _audit_systemic_risk"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _audit_systemic_risk results for downstream analysis"
            ]
          }
        },
        "audit_evidence_traceability": {
          "canonical_abbreviation": "operationalizationauditor",
          "provides": "operationalizationauditor.audit_evidence_traceability",
          "role": "audit_evidence_traceability_execution",
          "usage_count_in_300": 20,
          "questions": [
            "Q004",
            "Q011",
            "Q034",
            "Q041",
            "Q064",
            "Q071",
            "Q094",
            "Q101",
            "Q124",
            "Q131",
            "Q154",
            "Q161",
            "Q184",
            "Q191",
            "Q214",
            "Q221",
            "Q244",
            "Q251",
            "Q274",
            "Q281"
          ],
          "base_slots": [
            "D1-Q4",
            "D3-Q1"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM01",
            "DIM03"
          ],
          "signature": {
            "line_number": 2296,
            "is_async": false,
            "parameters": [
              {
                "name": "nodes",
                "type": "dict[str, MetaNode]"
              }
            ],
            "return_type": "dict[str, AuditResult]",
            "docstring": "Audit evidence traceability for all nodes\n\n        Enhanced with D3-Q1 Ficha Técnica validation:\n        - Cross-checks baseline/target against extracted quantitative_claims\n        - Verifies DNP INDICATOR_STRUCTURE compliance for producto nodes\n        - Scores 'Excelente' only if ≥80% of productos pass full audit\n        "
          },
          "epistemological_foundation": {
            "paradigm": "OperationalizationAuditor analytical paradigm",
            "ontological_basis": "Analysis via OperationalizationAuditor.audit_evidence_traceability",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method audit_evidence_traceability implements structured analysis for D1-Q4"
            ],
            "justification": "This method contributes to D1-Q4 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "OperationalizationAuditor.audit_evidence_traceability algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute audit_evidence_traceability"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from audit_evidence_traceability"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use audit_evidence_traceability results for downstream analysis"
            ]
          }
        },
        "audit_sequence_logic": {
          "canonical_abbreviation": "operationalizationauditor",
          "provides": "operationalizationauditor.audit_sequence_logic",
          "role": "audit_sequence_logic_execution",
          "usage_count_in_300": 10,
          "questions": [
            "Q010",
            "Q040",
            "Q070",
            "Q100",
            "Q130",
            "Q160",
            "Q190",
            "Q220",
            "Q250",
            "Q280"
          ],
          "base_slots": [
            "D2-Q5"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM02"
          ],
          "signature": {
            "line_number": 2427,
            "is_async": false,
            "parameters": [
              {
                "name": "graph",
                "type": "nx.DiGraph"
              }
            ],
            "return_type": "list[str]",
            "docstring": "Audit logical sequence of activities"
          },
          "epistemological_foundation": {
            "paradigm": "OperationalizationAuditor analytical paradigm",
            "ontological_basis": "Analysis via OperationalizationAuditor.audit_sequence_logic",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method audit_sequence_logic implements structured analysis for D2-Q5"
            ],
            "justification": "This method contributes to D2-Q5 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "OperationalizationAuditor.audit_sequence_logic algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute audit_sequence_logic"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from audit_sequence_logic"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use audit_sequence_logic results for downstream analysis"
            ]
          }
        },
        "_generate_optimal_remediations": {
          "canonical_abbreviation": "operationalizationauditor",
          "provides": "operationalizationauditor.generate_optimal_remediations",
          "role": "_generate_optimal_remediations_execution",
          "usage_count_in_300": 10,
          "questions": [
            "Q019",
            "Q049",
            "Q079",
            "Q109",
            "Q139",
            "Q169",
            "Q199",
            "Q229",
            "Q259",
            "Q289"
          ],
          "base_slots": [
            "D4-Q4"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM04"
          ],
          "signature": {
            "line_number": 2834,
            "is_async": false,
            "parameters": [
              {
                "name": "direct_evidence",
                "type": "dict[str, dict[str, Any]]"
              },
              {
                "name": "causal_implications",
                "type": "dict[str, dict[str, Any]]"
              },
              {
                "name": "systemic_risk",
                "type": "dict[str, Any]"
              }
            ],
            "return_type": "list[dict[str, Any]]",
            "docstring": "Generate prioritized remediation recommendations"
          },
          "epistemological_foundation": {
            "paradigm": "OperationalizationAuditor analytical paradigm",
            "ontological_basis": "Analysis via OperationalizationAuditor._generate_optimal_remediations",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _generate_optimal_remediations implements structured analysis for D4-Q4"
            ],
            "justification": "This method contributes to D4-Q4 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "OperationalizationAuditor._generate_optimal_remediations algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _generate_optimal_remediations"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _generate_optimal_remediations"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _generate_optimal_remediations results for downstream analysis"
            ]
          }
        },
        "_get_remediation_text": {
          "canonical_abbreviation": "operationalizationauditor",
          "provides": "operationalizationauditor.get_remediation_text",
          "role": "_get_remediation_text_execution",
          "usage_count_in_300": 10,
          "questions": [
            "Q019",
            "Q049",
            "Q079",
            "Q109",
            "Q139",
            "Q169",
            "Q199",
            "Q229",
            "Q259",
            "Q289"
          ],
          "base_slots": [
            "D4-Q4"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM04"
          ],
          "signature": {
            "line_number": 2883,
            "is_async": false,
            "parameters": [
              {
                "name": "omission",
                "type": "str"
              },
              {
                "name": "node_id",
                "type": "str"
              }
            ],
            "return_type": "str",
            "docstring": "Get specific remediation text for an omission"
          },
          "epistemological_foundation": {
            "paradigm": "OperationalizationAuditor analytical paradigm",
            "ontological_basis": "Analysis via OperationalizationAuditor._get_remediation_text",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _get_remediation_text implements structured analysis for D4-Q4"
            ],
            "justification": "This method contributes to D4-Q4 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "OperationalizationAuditor._get_remediation_text algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _get_remediation_text"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _get_remediation_text"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _get_remediation_text results for downstream analysis"
            ]
          }
        }
      }
    },
    "BayesianMechanismInference": {
      "file_path": "src/methods_dispensary/derek_beach.py",
      "line_number": 2935,
      "class_docstring": "\n    AGUJA II: El Modelo Generativo de Mecanismos\n    Hierarchical Bayesian model for causal mechanism inference\n\n    F1.2 ARCHITECTURAL REFACTORING:\n    This class now integrates with refactored Bayesian engine components:\n    - BayesianPriorBuilder: Construye priors adaptativos (AGUJA I)\n    - BayesianSamplingEngine: Ejecuta MCMC sampling (AGUJA II)\n    - NecessitySufficiencyTester: Ejecuta Hoop Tests (AGUJA III)\n\n    The refactored components provide:\n    - Crystal-clear separation of concern",
      "total_usage_in_300_contracts": 210,
      "unique_questions": 110,
      "questions": [
        "Q002",
        "Q007",
        "Q010",
        "Q012",
        "Q014",
        "Q015",
        "Q018",
        "Q022",
        "Q023",
        "Q025",
        "Q027",
        "Q032",
        "Q037",
        "Q040",
        "Q042",
        "Q044",
        "Q045",
        "Q048",
        "Q052",
        "Q053",
        "Q055",
        "Q057",
        "Q062",
        "Q067",
        "Q070",
        "Q072",
        "Q074",
        "Q075",
        "Q078",
        "Q082",
        "Q083",
        "Q085",
        "Q087",
        "Q092",
        "Q097",
        "Q100",
        "Q102",
        "Q104",
        "Q105",
        "Q108",
        "Q112",
        "Q113",
        "Q115",
        "Q117",
        "Q122",
        "Q127",
        "Q130",
        "Q132",
        "Q134",
        "Q135",
        "Q138",
        "Q142",
        "Q143",
        "Q145",
        "Q147",
        "Q152",
        "Q157",
        "Q160",
        "Q162",
        "Q164",
        "Q165",
        "Q168",
        "Q172",
        "Q173",
        "Q175",
        "Q177",
        "Q182",
        "Q187",
        "Q190",
        "Q192",
        "Q194",
        "Q195",
        "Q198",
        "Q202",
        "Q203",
        "Q205",
        "Q207",
        "Q212",
        "Q217",
        "Q220",
        "Q222",
        "Q224",
        "Q225",
        "Q228",
        "Q232",
        "Q233",
        "Q235",
        "Q237",
        "Q242",
        "Q247",
        "Q250",
        "Q252",
        "Q254",
        "Q255",
        "Q258",
        "Q262",
        "Q263",
        "Q265",
        "Q267",
        "Q272",
        "Q277",
        "Q280",
        "Q282",
        "Q284",
        "Q285",
        "Q288",
        "Q292",
        "Q293",
        "Q295",
        "Q297"
      ],
      "base_slots": [
        "D1-Q2",
        "D2-Q2",
        "D2-Q5",
        "D3-Q2",
        "D3-Q4",
        "D3-Q5",
        "D4-Q3",
        "D5-Q2",
        "D5-Q3",
        "D5-Q5",
        "D6-Q2"
      ],
      "policy_areas": [
        "PA01",
        "PA02",
        "PA03",
        "PA04",
        "PA05",
        "PA06",
        "PA07",
        "PA08",
        "PA09",
        "PA10"
      ],
      "dimensions": [
        "DIM01",
        "DIM02",
        "DIM03",
        "DIM04",
        "DIM05",
        "DIM06"
      ],
      "methods": {
        "_detect_gaps": {
          "canonical_abbreviation": "bayesianmechanisminference",
          "provides": "bayesianmechanisminference.detect_gaps",
          "role": "_detect_gaps_detection",
          "usage_count_in_300": 10,
          "questions": [
            "Q002",
            "Q032",
            "Q062",
            "Q092",
            "Q122",
            "Q152",
            "Q182",
            "Q212",
            "Q242",
            "Q272"
          ],
          "base_slots": [
            "D1-Q2"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM01"
          ],
          "signature": {
            "line_number": 3433,
            "is_async": false,
            "parameters": [
              {
                "name": "node",
                "type": "MetaNode"
              },
              {
                "name": "observations",
                "type": "dict[str, Any]"
              },
              {
                "name": "uncertainty",
                "type": "dict[str, float]"
              }
            ],
            "return_type": "list[dict[str, str]]",
            "docstring": "Detect documentation gaps based on uncertainty"
          },
          "epistemological_foundation": {
            "paradigm": "BayesianMechanismInference analytical paradigm",
            "ontological_basis": "Analysis via BayesianMechanismInference._detect_gaps",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _detect_gaps implements structured analysis for D1-Q2"
            ],
            "justification": "This method contributes to D1-Q2 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "BayesianMechanismInference._detect_gaps algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _detect_gaps"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _detect_gaps"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _detect_gaps results for downstream analysis"
            ]
          }
        },
        "infer_mechanisms": {
          "canonical_abbreviation": "bayesianmechanisminference",
          "provides": "bayesianmechanisminference.infer_mechanisms",
          "role": "infer_mechanisms_execution",
          "usage_count_in_300": 20,
          "questions": [
            "Q007",
            "Q015",
            "Q037",
            "Q045",
            "Q067",
            "Q075",
            "Q097",
            "Q105",
            "Q127",
            "Q135",
            "Q157",
            "Q165",
            "Q187",
            "Q195",
            "Q217",
            "Q225",
            "Q247",
            "Q255",
            "Q277",
            "Q285"
          ],
          "base_slots": [
            "D2-Q2",
            "D3-Q5"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM02",
            "DIM03"
          ],
          "signature": {
            "line_number": 3028,
            "is_async": false,
            "parameters": [
              {
                "name": "nodes",
                "type": "dict[str, MetaNode]"
              },
              {
                "name": "text",
                "type": "str"
              }
            ],
            "return_type": "dict[str, dict[str, Any]]",
            "docstring": "\n        Infer latent causal mechanisms using hierarchical Bayesian modeling\n\n        HARMONIC FRONT 4 ENHANCEMENT:\n        - Tracks mean mechanism_type uncertainty for quality criteria\n        - Reports uncertainty reduction metrics\n        "
          },
          "epistemological_foundation": {
            "paradigm": "BayesianMechanismInference analytical paradigm",
            "ontological_basis": "Analysis via BayesianMechanismInference.infer_mechanisms",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method infer_mechanisms implements structured analysis for D2-Q2"
            ],
            "justification": "This method contributes to D2-Q2 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "BayesianMechanismInference.infer_mechanisms algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute infer_mechanisms"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from infer_mechanisms"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use infer_mechanisms results for downstream analysis"
            ]
          }
        },
        "_infer_single_mechanism": {
          "canonical_abbreviation": "bayesianmechanisminference",
          "provides": "bayesianmechanisminference.infer_single_mechanism",
          "role": "_infer_single_mechanism_execution",
          "usage_count_in_300": 10,
          "questions": [
            "Q007",
            "Q037",
            "Q067",
            "Q097",
            "Q127",
            "Q157",
            "Q187",
            "Q217",
            "Q247",
            "Q277"
          ],
          "base_slots": [
            "D2-Q2"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM02"
          ],
          "signature": {
            "line_number": 3068,
            "is_async": false,
            "parameters": [
              {
                "name": "node",
                "type": "MetaNode"
              },
              {
                "name": "text",
                "type": "str"
              },
              {
                "name": "all_nodes",
                "type": "dict[str, MetaNode]"
              }
            ],
            "return_type": "dict[str, Any]",
            "docstring": "Infer mechanism for a single product node"
          },
          "epistemological_foundation": {
            "paradigm": "BayesianMechanismInference analytical paradigm",
            "ontological_basis": "Analysis via BayesianMechanismInference._infer_single_mechanism",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _infer_single_mechanism implements structured analysis for D2-Q2"
            ],
            "justification": "This method contributes to D2-Q2 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "BayesianMechanismInference._infer_single_mechanism algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _infer_single_mechanism"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _infer_single_mechanism"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _infer_single_mechanism results for downstream analysis"
            ]
          }
        },
        "_infer_mechanism_type": {
          "canonical_abbreviation": "bayesianmechanisminference",
          "provides": "bayesianmechanisminference.infer_mechanism_type",
          "role": "_infer_mechanism_type_execution",
          "usage_count_in_300": 10,
          "questions": [
            "Q007",
            "Q037",
            "Q067",
            "Q097",
            "Q127",
            "Q157",
            "Q187",
            "Q217",
            "Q247",
            "Q277"
          ],
          "base_slots": [
            "D2-Q2"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM02"
          ],
          "signature": {
            "line_number": 3154,
            "is_async": false,
            "parameters": [
              {
                "name": "observations",
                "type": "dict[str, Any]"
              }
            ],
            "return_type": "dict[str, float]",
            "docstring": "Infer mechanism type using Bayesian updating"
          },
          "epistemological_foundation": {
            "paradigm": "BayesianMechanismInference analytical paradigm",
            "ontological_basis": "Analysis via BayesianMechanismInference._infer_mechanism_type",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _infer_mechanism_type implements structured analysis for D2-Q2"
            ],
            "justification": "This method contributes to D2-Q2 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "BayesianMechanismInference._infer_mechanism_type algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _infer_mechanism_type"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _infer_mechanism_type"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _infer_mechanism_type results for downstream analysis"
            ]
          }
        },
        "_test_sufficiency": {
          "canonical_abbreviation": "bayesianmechanisminference",
          "provides": "bayesianmechanisminference.test_sufficiency",
          "role": "_test_sufficiency_execution",
          "usage_count_in_300": 30,
          "questions": [
            "Q007",
            "Q012",
            "Q027",
            "Q037",
            "Q042",
            "Q057",
            "Q067",
            "Q072",
            "Q087",
            "Q097",
            "Q102",
            "Q117",
            "Q127",
            "Q132",
            "Q147",
            "Q157",
            "Q162",
            "Q177",
            "Q187",
            "Q192",
            "Q207",
            "Q217",
            "Q222",
            "Q237",
            "Q247",
            "Q252",
            "Q267",
            "Q277",
            "Q282",
            "Q297"
          ],
          "base_slots": [
            "D2-Q2",
            "D3-Q2",
            "D6-Q2"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM02",
            "DIM03",
            "DIM06"
          ],
          "signature": {
            "line_number": 3264,
            "is_async": false,
            "parameters": [
              {
                "name": "node",
                "type": "MetaNode"
              },
              {
                "name": "observations",
                "type": "dict[str, Any]"
              }
            ],
            "return_type": "dict[str, Any]",
            "docstring": "Test if mechanism is sufficient to produce the outcome"
          },
          "epistemological_foundation": {
            "paradigm": "BayesianMechanismInference analytical paradigm",
            "ontological_basis": "Analysis via BayesianMechanismInference._test_sufficiency",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _test_sufficiency implements structured analysis for D2-Q2"
            ],
            "justification": "This method contributes to D2-Q2 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "BayesianMechanismInference._test_sufficiency algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _test_sufficiency"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _test_sufficiency"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _test_sufficiency results for downstream analysis"
            ]
          }
        },
        "_test_necessity": {
          "canonical_abbreviation": "bayesianmechanisminference",
          "provides": "bayesianmechanisminference.test_necessity",
          "role": "_test_necessity_execution",
          "usage_count_in_300": 40,
          "questions": [
            "Q007",
            "Q012",
            "Q014",
            "Q027",
            "Q037",
            "Q042",
            "Q044",
            "Q057",
            "Q067",
            "Q072",
            "Q074",
            "Q087",
            "Q097",
            "Q102",
            "Q104",
            "Q117",
            "Q127",
            "Q132",
            "Q134",
            "Q147",
            "Q157",
            "Q162",
            "Q164",
            "Q177",
            "Q187",
            "Q192",
            "Q194",
            "Q207",
            "Q217",
            "Q222",
            "Q224",
            "Q237",
            "Q247",
            "Q252",
            "Q254",
            "Q267",
            "Q277",
            "Q282",
            "Q284",
            "Q297"
          ],
          "base_slots": [
            "D2-Q2",
            "D3-Q2",
            "D3-Q4",
            "D6-Q2"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM02",
            "DIM03",
            "DIM06"
          ],
          "signature": {
            "line_number": 3293,
            "is_async": false,
            "parameters": [
              {
                "name": "node",
                "type": "MetaNode"
              },
              {
                "name": "observations",
                "type": "dict[str, Any]"
              }
            ],
            "return_type": "dict[str, Any]",
            "docstring": "\n        AUDIT POINT 2.2: Mechanism Necessity Hoop Test\n\n        Test if mechanism is necessary by checking documented components:\n        - Entity (responsable)\n        - Activity (verb lemma sequence)\n        - Budget (presupuesto asignado)\n\n        Implements Beach 2017 Hoop Tests for necessity verification.\n        Per Falleti & Lynch 2009, Bayesian-deterministic hybrid boosts mechanism depth.\n\n        Returns:\n            Dict with 'is_necessary', 'missing_components', and remediation text\n"
          },
          "epistemological_foundation": {
            "paradigm": "BayesianMechanismInference analytical paradigm",
            "ontological_basis": "Analysis via BayesianMechanismInference._test_necessity",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _test_necessity implements structured analysis for D2-Q2"
            ],
            "justification": "This method contributes to D2-Q2 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "BayesianMechanismInference._test_necessity algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _test_necessity"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _test_necessity"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _test_necessity results for downstream analysis"
            ]
          }
        },
        "_calculate_coherence_factor": {
          "canonical_abbreviation": "bayesianmechanisminference",
          "provides": "bayesianmechanisminference.calculate_coherence_factor",
          "role": "_calculate_coherence_factor_calculation",
          "usage_count_in_300": 30,
          "questions": [
            "Q010",
            "Q025",
            "Q027",
            "Q040",
            "Q055",
            "Q057",
            "Q070",
            "Q085",
            "Q087",
            "Q100",
            "Q115",
            "Q117",
            "Q130",
            "Q145",
            "Q147",
            "Q160",
            "Q175",
            "Q177",
            "Q190",
            "Q205",
            "Q207",
            "Q220",
            "Q235",
            "Q237",
            "Q250",
            "Q265",
            "Q267",
            "Q280",
            "Q295",
            "Q297"
          ],
          "base_slots": [
            "D2-Q5",
            "D5-Q5",
            "D6-Q2"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM02",
            "DIM05",
            "DIM06"
          ],
          "signature": {
            "line_number": 3222,
            "is_async": false,
            "parameters": [
              {
                "name": "node",
                "type": "MetaNode"
              },
              {
                "name": "observations",
                "type": "dict[str, Any]"
              },
              {
                "name": "all_nodes",
                "type": "dict[str, MetaNode]"
              }
            ],
            "return_type": "float",
            "docstring": "Calculate mechanism coherence score"
          },
          "epistemological_foundation": {
            "paradigm": "BayesianMechanismInference analytical paradigm",
            "ontological_basis": "Analysis via BayesianMechanismInference._calculate_coherence_factor",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _calculate_coherence_factor implements structured analysis for D2-Q5"
            ],
            "justification": "This method contributes to D2-Q5 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "BayesianMechanismInference._calculate_coherence_factor algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _calculate_coherence_factor"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _calculate_coherence_factor"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _calculate_coherence_factor results for downstream analysis"
            ]
          }
        },
        "_log_refactored_components": {
          "canonical_abbreviation": "bayesianmechanisminference",
          "provides": "bayesianmechanisminference.log_refactored_components",
          "role": "_log_refactored_components_execution",
          "usage_count_in_300": 10,
          "questions": [
            "Q012",
            "Q042",
            "Q072",
            "Q102",
            "Q132",
            "Q162",
            "Q192",
            "Q222",
            "Q252",
            "Q282"
          ],
          "base_slots": [
            "D3-Q2"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM03"
          ],
          "signature": {
            "line_number": 3016,
            "is_async": false,
            "parameters": [],
            "return_type": "None",
            "docstring": "Log status of refactored Bayesian components (F1.2)"
          },
          "epistemological_foundation": {
            "paradigm": "BayesianMechanismInference analytical paradigm",
            "ontological_basis": "Analysis via BayesianMechanismInference._log_refactored_components",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _log_refactored_components implements structured analysis for D3-Q2"
            ],
            "justification": "This method contributes to D3-Q2 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "BayesianMechanismInference._log_refactored_components algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _log_refactored_components"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _log_refactored_components"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _log_refactored_components results for downstream analysis"
            ]
          }
        },
        "_infer_activity_sequence": {
          "canonical_abbreviation": "bayesianmechanisminference",
          "provides": "bayesianmechanisminference.infer_activity_sequence",
          "role": "_infer_activity_sequence_execution",
          "usage_count_in_300": 10,
          "questions": [
            "Q015",
            "Q045",
            "Q075",
            "Q105",
            "Q135",
            "Q165",
            "Q195",
            "Q225",
            "Q255",
            "Q285"
          ],
          "base_slots": [
            "D3-Q5"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM03"
          ],
          "signature": {
            "line_number": 3193,
            "is_async": false,
            "parameters": [
              {
                "name": "observations",
                "type": "dict[str, Any]"
              },
              {
                "name": "mechanism_type_posterior",
                "type": "dict[str, float]"
              }
            ],
            "return_type": "dict[str, Any]",
            "docstring": "Infer activity sequence parameters"
          },
          "epistemological_foundation": {
            "paradigm": "BayesianMechanismInference analytical paradigm",
            "ontological_basis": "Analysis via BayesianMechanismInference._infer_activity_sequence",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _infer_activity_sequence implements structured analysis for D3-Q5"
            ],
            "justification": "This method contributes to D3-Q5 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "BayesianMechanismInference._infer_activity_sequence algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _infer_activity_sequence"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _infer_activity_sequence"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _infer_activity_sequence results for downstream analysis"
            ]
          }
        },
        "_generate_necessity_remediation": {
          "canonical_abbreviation": "bayesianmechanisminference",
          "provides": "bayesianmechanisminference.generate_necessity_remediation",
          "role": "_generate_necessity_remediation_execution",
          "usage_count_in_300": 10,
          "questions": [
            "Q015",
            "Q045",
            "Q075",
            "Q105",
            "Q135",
            "Q165",
            "Q195",
            "Q225",
            "Q255",
            "Q285"
          ],
          "base_slots": [
            "D3-Q5"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM03"
          ],
          "signature": {
            "line_number": 3378,
            "is_async": false,
            "parameters": [
              {
                "name": "node_id",
                "type": "str"
              },
              {
                "name": "missing_components",
                "type": "list[str]"
              }
            ],
            "return_type": "str",
            "docstring": "Generate remediation text for failed necessity test"
          },
          "epistemological_foundation": {
            "paradigm": "BayesianMechanismInference analytical paradigm",
            "ontological_basis": "Analysis via BayesianMechanismInference._generate_necessity_remediation",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _generate_necessity_remediation implements structured analysis for D3-Q5"
            ],
            "justification": "This method contributes to D3-Q5 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "BayesianMechanismInference._generate_necessity_remediation algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _generate_necessity_remediation"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _generate_necessity_remediation"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _generate_necessity_remediation results for downstream analysis"
            ]
          }
        },
        "_aggregate_bayesian_confidence": {
          "canonical_abbreviation": "bayesianmechanisminference",
          "provides": "bayesianmechanisminference.aggregate_bayesian_confidence",
          "role": "_aggregate_bayesian_confidence_execution",
          "usage_count_in_300": 20,
          "questions": [
            "Q018",
            "Q022",
            "Q048",
            "Q052",
            "Q078",
            "Q082",
            "Q108",
            "Q112",
            "Q138",
            "Q142",
            "Q168",
            "Q172",
            "Q198",
            "Q202",
            "Q228",
            "Q232",
            "Q258",
            "Q262",
            "Q288",
            "Q292"
          ],
          "base_slots": [
            "D4-Q3",
            "D5-Q2"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM04",
            "DIM05"
          ],
          "signature": {
            "line_number": 3477,
            "is_async": false,
            "parameters": [
              {
                "name": "confidences",
                "type": "list[float]"
              }
            ],
            "return_type": "float",
            "docstring": "\n        Aggregate multiple Bayesian confidence values.\n\n        Args:\n            confidences: List of confidence values to aggregate\n\n        Returns:\n            Aggregated confidence value\n        "
          },
          "epistemological_foundation": {
            "paradigm": "BayesianMechanismInference analytical paradigm",
            "ontological_basis": "Analysis via BayesianMechanismInference._aggregate_bayesian_confidence",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _aggregate_bayesian_confidence implements structured analysis for D4-Q3"
            ],
            "justification": "This method contributes to D4-Q3 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "BayesianMechanismInference._aggregate_bayesian_confidence algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _aggregate_bayesian_confidence"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _aggregate_bayesian_confidence"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _aggregate_bayesian_confidence results for downstream analysis"
            ]
          }
        },
        "_quantify_uncertainty": {
          "canonical_abbreviation": "bayesianmechanisminference",
          "provides": "bayesianmechanisminference.quantify_uncertainty",
          "role": "_quantify_uncertainty_execution",
          "usage_count_in_300": 10,
          "questions": [
            "Q023",
            "Q053",
            "Q083",
            "Q113",
            "Q143",
            "Q173",
            "Q203",
            "Q233",
            "Q263",
            "Q293"
          ],
          "base_slots": [
            "D5-Q3"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM05"
          ],
          "signature": {
            "line_number": 3398,
            "is_async": false,
            "parameters": [
              {
                "name": "mechanism_type_posterior",
                "type": "dict[str, float]"
              },
              {
                "name": "sequence_posterior",
                "type": "dict[str, Any]"
              },
              {
                "name": "coherence_score",
                "type": "float"
              }
            ],
            "return_type": "dict[str, float]",
            "docstring": "Quantify epistemic uncertainty"
          },
          "epistemological_foundation": {
            "paradigm": "BayesianMechanismInference analytical paradigm",
            "ontological_basis": "Analysis via BayesianMechanismInference._quantify_uncertainty",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _quantify_uncertainty implements structured analysis for D5-Q3"
            ],
            "justification": "This method contributes to D5-Q3 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "BayesianMechanismInference._quantify_uncertainty algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _quantify_uncertainty"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _quantify_uncertainty"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _quantify_uncertainty results for downstream analysis"
            ]
          }
        }
      }
    },
    "BayesianCounterfactualAuditor": {
      "file_path": "src/methods_dispensary/derek_beach.py",
      "line_number": 5734,
      "class_docstring": "\n    AGUJA III - Auditor Contrafactual con SCM y do-calculus\n\n    PROMPT III-1: Construcción de SCM y queries gemelas\n    Construye SCM={DAG, f_i} y responde omission_impact, sufficiency_test, necessity_test.\n\n    PROMPT III-2: Riesgo sistémico y priorización\n    Agrega riesgos, propaga incertidumbre, calcula priority.\n\n    PROMPT III-3: Refutación, negativos y cordura do(.)\n    Ejecuta controles negativos, pruebas placebo, sanity checks.\n\n    QUALITY CRITERIA:\n    - Consistencia de signos factu",
      "total_usage_in_300_contracts": 170,
      "unique_questions": 110,
      "questions": [
        "Q002",
        "Q003",
        "Q008",
        "Q009",
        "Q015",
        "Q017",
        "Q019",
        "Q021",
        "Q022",
        "Q024",
        "Q027",
        "Q032",
        "Q033",
        "Q038",
        "Q039",
        "Q045",
        "Q047",
        "Q049",
        "Q051",
        "Q052",
        "Q054",
        "Q057",
        "Q062",
        "Q063",
        "Q068",
        "Q069",
        "Q075",
        "Q077",
        "Q079",
        "Q081",
        "Q082",
        "Q084",
        "Q087",
        "Q092",
        "Q093",
        "Q098",
        "Q099",
        "Q105",
        "Q107",
        "Q109",
        "Q111",
        "Q112",
        "Q114",
        "Q117",
        "Q122",
        "Q123",
        "Q128",
        "Q129",
        "Q135",
        "Q137",
        "Q139",
        "Q141",
        "Q142",
        "Q144",
        "Q147",
        "Q152",
        "Q153",
        "Q158",
        "Q159",
        "Q165",
        "Q167",
        "Q169",
        "Q171",
        "Q172",
        "Q174",
        "Q177",
        "Q182",
        "Q183",
        "Q188",
        "Q189",
        "Q195",
        "Q197",
        "Q199",
        "Q201",
        "Q202",
        "Q204",
        "Q207",
        "Q212",
        "Q213",
        "Q218",
        "Q219",
        "Q225",
        "Q227",
        "Q229",
        "Q231",
        "Q232",
        "Q234",
        "Q237",
        "Q242",
        "Q243",
        "Q248",
        "Q249",
        "Q255",
        "Q257",
        "Q259",
        "Q261",
        "Q262",
        "Q264",
        "Q267",
        "Q272",
        "Q273",
        "Q278",
        "Q279",
        "Q285",
        "Q287",
        "Q289",
        "Q291",
        "Q292",
        "Q294",
        "Q297"
      ],
      "base_slots": [
        "D1-Q2",
        "D1-Q3",
        "D2-Q3",
        "D2-Q4",
        "D3-Q5",
        "D4-Q2",
        "D4-Q4",
        "D5-Q1",
        "D5-Q2",
        "D5-Q4",
        "D6-Q2"
      ],
      "policy_areas": [
        "PA01",
        "PA02",
        "PA03",
        "PA04",
        "PA05",
        "PA06",
        "PA07",
        "PA08",
        "PA09",
        "PA10"
      ],
      "dimensions": [
        "DIM01",
        "DIM02",
        "DIM03",
        "DIM04",
        "DIM05",
        "DIM06"
      ],
      "methods": {
        "counterfactual_query": {
          "canonical_abbreviation": "bayesiancounterfactualauditor",
          "provides": "bayesiancounterfactualauditor.counterfactual_query",
          "role": "counterfactual_query_execution",
          "usage_count_in_300": 10,
          "questions": [
            "Q002",
            "Q032",
            "Q062",
            "Q092",
            "Q122",
            "Q152",
            "Q182",
            "Q212",
            "Q242",
            "Q272"
          ],
          "base_slots": [
            "D1-Q2"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM01"
          ],
          "signature": {
            "line_number": 5825,
            "is_async": false,
            "parameters": [
              {
                "name": "intervention",
                "type": "dict[str, float]"
              },
              {
                "name": "target",
                "type": "str"
              },
              {
                "name": "evidence",
                "type": "dict[str, float] | None",
                "default": "None"
              }
            ],
            "return_type": "dict[str, Any]",
            "docstring": "\n        PROMPT III-1: Queries gemelas (omission, sufficiency, necessity)\n\n        Evalúa:\n        - Factual: P(Y | evidence)\n        - Counterfactual: P(Y | do(X=x), evidence)\n        - Causal effect, sufficiency, necessity\n\n        Args:\n            intervention: {nodo: valor} para do(.) operation\n            target: Nodo objetivo Y\n            evidence: Evidencia observada (opcional)\n\n        Returns:\n            Dict con p_factual, p_counterfactual, causal_effect, is_sufficient, is_necessary"
          },
          "epistemological_foundation": {
            "paradigm": "BayesianCounterfactualAuditor analytical paradigm",
            "ontological_basis": "Analysis via BayesianCounterfactualAuditor.counterfactual_query",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method counterfactual_query implements structured analysis for D1-Q2"
            ],
            "justification": "This method contributes to D1-Q2 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "BayesianCounterfactualAuditor.counterfactual_query algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute counterfactual_query"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from counterfactual_query"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use counterfactual_query results for downstream analysis"
            ]
          }
        },
        "_test_effect_stability": {
          "canonical_abbreviation": "bayesiancounterfactualauditor",
          "provides": "bayesiancounterfactualauditor.test_effect_stability",
          "role": "_test_effect_stability_execution",
          "usage_count_in_300": 30,
          "questions": [
            "Q002",
            "Q024",
            "Q027",
            "Q032",
            "Q054",
            "Q057",
            "Q062",
            "Q084",
            "Q087",
            "Q092",
            "Q114",
            "Q117",
            "Q122",
            "Q144",
            "Q147",
            "Q152",
            "Q174",
            "Q177",
            "Q182",
            "Q204",
            "Q207",
            "Q212",
            "Q234",
            "Q237",
            "Q242",
            "Q264",
            "Q267",
            "Q272",
            "Q294",
            "Q297"
          ],
          "base_slots": [
            "D1-Q2",
            "D5-Q4",
            "D6-Q2"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM01",
            "DIM05",
            "DIM06"
          ],
          "signature": {
            "line_number": 5977,
            "is_async": false,
            "parameters": [
              {
                "name": "intervention",
                "type": "dict[str, float]"
              },
              {
                "name": "target",
                "type": "str"
              },
              {
                "name": "evidence",
                "type": "dict[str, float] | None"
              },
              {
                "name": "n_perturbations",
                "type": "int",
                "default": "5"
              }
            ],
            "return_type": "float",
            "docstring": "Testa estabilidad al variar priors/ecuaciones ±10%"
          },
          "epistemological_foundation": {
            "paradigm": "BayesianCounterfactualAuditor analytical paradigm",
            "ontological_basis": "Analysis via BayesianCounterfactualAuditor._test_effect_stability",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _test_effect_stability implements structured analysis for D1-Q2"
            ],
            "justification": "This method contributes to D1-Q2 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "BayesianCounterfactualAuditor._test_effect_stability algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _test_effect_stability"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _test_effect_stability"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _test_effect_stability results for downstream analysis"
            ]
          }
        },
        "aggregate_risk_and_prioritize": {
          "canonical_abbreviation": "bayesiancounterfactualauditor",
          "provides": "bayesiancounterfactualauditor.aggregate_risk_and_prioritize",
          "role": "aggregate_risk_and_prioritize_execution",
          "usage_count_in_300": 40,
          "questions": [
            "Q003",
            "Q009",
            "Q019",
            "Q022",
            "Q033",
            "Q039",
            "Q049",
            "Q052",
            "Q063",
            "Q069",
            "Q079",
            "Q082",
            "Q093",
            "Q099",
            "Q109",
            "Q112",
            "Q123",
            "Q129",
            "Q139",
            "Q142",
            "Q153",
            "Q159",
            "Q169",
            "Q172",
            "Q183",
            "Q189",
            "Q199",
            "Q202",
            "Q213",
            "Q219",
            "Q229",
            "Q232",
            "Q243",
            "Q249",
            "Q259",
            "Q262",
            "Q273",
            "Q279",
            "Q289",
            "Q292"
          ],
          "base_slots": [
            "D1-Q3",
            "D2-Q4",
            "D4-Q4",
            "D5-Q2"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM01",
            "DIM02",
            "DIM04",
            "DIM05"
          ],
          "signature": {
            "line_number": 6014,
            "is_async": false,
            "parameters": [
              {
                "name": "omission_score",
                "type": "float"
              },
              {
                "name": "insufficiency_score",
                "type": "float"
              },
              {
                "name": "unnecessity_score",
                "type": "float"
              },
              {
                "name": "causal_effect",
                "type": "float"
              },
              {
                "name": "feasibility",
                "type": "float",
                "default": "0.8"
              },
              {
                "name": "cost",
                "type": "float",
                "default": "1.0"
              }
            ],
            "return_type": "dict[str, Any]",
            "docstring": "\n        PROMPT III-2: Riesgo sistémico y priorización con incertidumbre\n\n        Fórmulas:\n        - risk = 0.50·omission + 0.35·insufficiency + 0.15·unnecessity\n        - priority = |effect|·feasibility/(cost+ε)·(1−uncertainty)\n\n        Args:\n            omission_score: Riesgo de omisión de mecanismo [0,1]\n            insufficiency_score: Insuficiencia del mecanismo [0,1]\n            unnecessity_score: Mecanismo innecesario [0,1]\n            causal_effect: Efecto causal estimado\n            fe"
          },
          "epistemological_foundation": {
            "paradigm": "BayesianCounterfactualAuditor analytical paradigm",
            "ontological_basis": "Analysis via BayesianCounterfactualAuditor.aggregate_risk_and_prioritize",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method aggregate_risk_and_prioritize implements structured analysis for D1-Q3"
            ],
            "justification": "This method contributes to D1-Q3 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "BayesianCounterfactualAuditor.aggregate_risk_and_prioritize algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute aggregate_risk_and_prioritize"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from aggregate_risk_and_prioritize"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use aggregate_risk_and_prioritize results for downstream analysis"
            ]
          }
        },
        "construct_scm": {
          "canonical_abbreviation": "bayesiancounterfactualauditor",
          "provides": "bayesiancounterfactualauditor.construct_scm",
          "role": "construct_scm_execution",
          "usage_count_in_300": 30,
          "questions": [
            "Q008",
            "Q017",
            "Q021",
            "Q038",
            "Q047",
            "Q051",
            "Q068",
            "Q077",
            "Q081",
            "Q098",
            "Q107",
            "Q111",
            "Q128",
            "Q137",
            "Q141",
            "Q158",
            "Q167",
            "Q171",
            "Q188",
            "Q197",
            "Q201",
            "Q218",
            "Q227",
            "Q231",
            "Q248",
            "Q257",
            "Q261",
            "Q278",
            "Q287",
            "Q291"
          ],
          "base_slots": [
            "D2-Q3",
            "D4-Q2",
            "D5-Q1"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM02",
            "DIM04",
            "DIM05"
          ],
          "signature": {
            "line_number": 5758,
            "is_async": false,
            "parameters": [
              {
                "name": "dag",
                "type": "nx.DiGraph"
              },
              {
                "name": "structural_equations",
                "type": "dict[str, callable] | None",
                "default": "None"
              }
            ],
            "return_type": "dict[str, Any]",
            "docstring": "\n        PROMPT III-1: Construcción de SCM\n\n        Construye SCM = {DAG, f_i} desde grafo y ecuaciones estructurales.\n\n        Args:\n            dag: NetworkX DiGraph (debe ser acíclico)\n            structural_equations: Dict {node: function} para f_i\n\n        Returns:\n            SCM con DAG validado y funciones estructurales\n\n        Raises:\n            ValueError: Si DAG no es acíclico\n        "
          },
          "epistemological_foundation": {
            "paradigm": "BayesianCounterfactualAuditor analytical paradigm",
            "ontological_basis": "Analysis via BayesianCounterfactualAuditor.construct_scm",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method construct_scm implements structured analysis for D2-Q3"
            ],
            "justification": "This method contributes to D2-Q3 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "BayesianCounterfactualAuditor.construct_scm algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute construct_scm"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from construct_scm"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use construct_scm results for downstream analysis"
            ]
          }
        },
        "_create_default_equations": {
          "canonical_abbreviation": "bayesiancounterfactualauditor",
          "provides": "bayesiancounterfactualauditor.create_default_equations",
          "role": "_create_default_equations_execution",
          "usage_count_in_300": 10,
          "questions": [
            "Q008",
            "Q038",
            "Q068",
            "Q098",
            "Q128",
            "Q158",
            "Q188",
            "Q218",
            "Q248",
            "Q278"
          ],
          "base_slots": [
            "D2-Q3"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM02"
          ],
          "signature": {
            "line_number": 5803,
            "is_async": false,
            "parameters": [
              {
                "name": "dag",
                "type": "nx.DiGraph"
              }
            ],
            "return_type": "dict[str, callable]",
            "docstring": "Crea ecuaciones estructurales lineales por defecto"
          },
          "epistemological_foundation": {
            "paradigm": "BayesianCounterfactualAuditor analytical paradigm",
            "ontological_basis": "Analysis via BayesianCounterfactualAuditor._create_default_equations",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _create_default_equations implements structured analysis for D2-Q3"
            ],
            "justification": "This method contributes to D2-Q3 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "BayesianCounterfactualAuditor._create_default_equations algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _create_default_equations"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _create_default_equations"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _create_default_equations results for downstream analysis"
            ]
          }
        },
        "refutation_and_sanity_checks": {
          "canonical_abbreviation": "bayesiancounterfactualauditor",
          "provides": "bayesiancounterfactualauditor.refutation_and_sanity_checks",
          "role": "refutation_and_sanity_checks_execution",
          "usage_count_in_300": 30,
          "questions": [
            "Q009",
            "Q015",
            "Q024",
            "Q039",
            "Q045",
            "Q054",
            "Q069",
            "Q075",
            "Q084",
            "Q099",
            "Q105",
            "Q114",
            "Q129",
            "Q135",
            "Q144",
            "Q159",
            "Q165",
            "Q174",
            "Q189",
            "Q195",
            "Q204",
            "Q219",
            "Q225",
            "Q234",
            "Q249",
            "Q255",
            "Q264",
            "Q279",
            "Q285",
            "Q294"
          ],
          "base_slots": [
            "D2-Q4",
            "D3-Q5",
            "D5-Q4"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM02",
            "DIM03",
            "DIM05"
          ],
          "signature": {
            "line_number": 6125,
            "is_async": false,
            "parameters": [
              {
                "name": "dag",
                "type": "nx.DiGraph"
              },
              {
                "name": "target",
                "type": "str"
              },
              {
                "name": "treatment",
                "type": "str"
              },
              {
                "name": "confounders",
                "type": "list[str] | None",
                "default": "None"
              }
            ],
            "return_type": "dict[str, Any]",
            "docstring": "\n        PROMPT III-3: Refutación, negativos y cordura do(.)\n\n        Ejecuta:\n        1. Controles negativos: nodos irrelevantes → |efecto| ≤ 0.05)\n        2. Pruebas placebo: permuta edges no causales\n        3. Sanity checks: añadir cofactores no reduce P(Y|do(X=1))\n\n        Args:\n            dag: Grafo causal\n            target: Nodo objetivo Y\n            treatment: Nodo de tratamiento X\n            confounders: Lista de cofactores\n\n        Returns:\n            Dict con negative_controls, p"
          },
          "epistemological_foundation": {
            "paradigm": "BayesianCounterfactualAuditor analytical paradigm",
            "ontological_basis": "Analysis via BayesianCounterfactualAuditor.refutation_and_sanity_checks",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method refutation_and_sanity_checks implements structured analysis for D2-Q4"
            ],
            "justification": "This method contributes to D2-Q4 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "BayesianCounterfactualAuditor.refutation_and_sanity_checks algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute refutation_and_sanity_checks"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from refutation_and_sanity_checks"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use refutation_and_sanity_checks results for downstream analysis"
            ]
          }
        },
        "_evaluate_factual": {
          "canonical_abbreviation": "bayesiancounterfactualauditor",
          "provides": "bayesiancounterfactualauditor.evaluate_factual",
          "role": "_evaluate_factual_evaluation",
          "usage_count_in_300": 10,
          "questions": [
            "Q015",
            "Q045",
            "Q075",
            "Q105",
            "Q135",
            "Q165",
            "Q195",
            "Q225",
            "Q255",
            "Q285"
          ],
          "base_slots": [
            "D3-Q5"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM03"
          ],
          "signature": {
            "line_number": 5898,
            "is_async": false,
            "parameters": [
              {
                "name": "target",
                "type": "str"
              },
              {
                "name": "evidence",
                "type": "dict[str, float]"
              }
            ],
            "return_type": "float",
            "docstring": "Evalúa P(target | evidence) propagando hacia adelante en DAG"
          },
          "epistemological_foundation": {
            "paradigm": "BayesianCounterfactualAuditor analytical paradigm",
            "ontological_basis": "Analysis via BayesianCounterfactualAuditor._evaluate_factual",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _evaluate_factual implements structured analysis for D3-Q5"
            ],
            "justification": "This method contributes to D3-Q5 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "BayesianCounterfactualAuditor._evaluate_factual algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _evaluate_factual"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _evaluate_factual"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _evaluate_factual results for downstream analysis"
            ]
          }
        },
        "_evaluate_counterfactual": {
          "canonical_abbreviation": "bayesiancounterfactualauditor",
          "provides": "bayesiancounterfactualauditor.evaluate_counterfactual",
          "role": "_evaluate_counterfactual_evaluation",
          "usage_count_in_300": 10,
          "questions": [
            "Q015",
            "Q045",
            "Q075",
            "Q105",
            "Q135",
            "Q165",
            "Q195",
            "Q225",
            "Q255",
            "Q285"
          ],
          "base_slots": [
            "D3-Q5"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM03"
          ],
          "signature": {
            "line_number": 5940,
            "is_async": false,
            "parameters": [
              {
                "name": "target",
                "type": "str"
              },
              {
                "name": "intervention",
                "type": "dict[str, float]"
              },
              {
                "name": "evidence",
                "type": "dict[str, float]"
              }
            ],
            "return_type": "float",
            "docstring": "Evalúa P(target | do(intervention), evidence) con DAG mutilado"
          },
          "epistemological_foundation": {
            "paradigm": "BayesianCounterfactualAuditor analytical paradigm",
            "ontological_basis": "Analysis via BayesianCounterfactualAuditor._evaluate_counterfactual",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _evaluate_counterfactual implements structured analysis for D3-Q5"
            ],
            "justification": "This method contributes to D3-Q5 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "BayesianCounterfactualAuditor._evaluate_counterfactual algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _evaluate_counterfactual"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _evaluate_counterfactual"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _evaluate_counterfactual results for downstream analysis"
            ]
          }
        }
      }
    },
    "BayesianConfidenceCalculator": {
      "file_path": "src/methods_dispensary/contradiction_deteccion.py",
      "line_number": 102,
      "class_docstring": "\n    Bayesian confidence calculator with domain-informed priors.\n\n    Uses Beta distribution priors calibrated from empirical analysis of\n    Colombian municipal development plans (PDMs).\n    ",
      "total_usage_in_300_contracts": 10,
      "unique_questions": 10,
      "questions": [
        "Q002",
        "Q032",
        "Q062",
        "Q092",
        "Q122",
        "Q152",
        "Q182",
        "Q212",
        "Q242",
        "Q272"
      ],
      "base_slots": [
        "D1-Q2"
      ],
      "policy_areas": [
        "PA01",
        "PA02",
        "PA03",
        "PA04",
        "PA05",
        "PA06",
        "PA07",
        "PA08",
        "PA09",
        "PA10"
      ],
      "dimensions": [
        "DIM01"
      ],
      "methods": {
        "calculate_posterior": {
          "canonical_abbreviation": "bayesianconfidencecalculator",
          "provides": "bayesianconfidencecalculator.calculate_posterior",
          "role": "calculate_posterior_calculation",
          "usage_count_in_300": 10,
          "questions": [
            "Q002",
            "Q032",
            "Q062",
            "Q092",
            "Q122",
            "Q152",
            "Q182",
            "Q212",
            "Q242",
            "Q272"
          ],
          "base_slots": [
            "D1-Q2"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM01"
          ],
          "signature": {
            "line_number": 115,
            "is_async": false,
            "parameters": [
              {
                "name": "evidence_strength",
                "type": "float"
              },
              {
                "name": "observations",
                "type": "int"
              },
              {
                "name": "domain_weight",
                "type": "float",
                "default": "1.0"
              }
            ],
            "return_type": "float",
            "docstring": "\n        Calculate posterior probability using Bayesian inference.\n\n        Updates the Beta distribution prior with observed evidence to compute\n        the posterior mean, which represents the confidence level in the finding.\n\n        Args:\n            evidence_strength: Strength of the evidence (0.0-1.0 scale, unitless ratio)\n            observations: Number of observations supporting the evidence (count)\n            domain_weight: Policy domain-specific weight (multiplier, default: 1.0)\n\n   "
          },
          "epistemological_foundation": {
            "paradigm": "BayesianConfidenceCalculator analytical paradigm",
            "ontological_basis": "Analysis via BayesianConfidenceCalculator.calculate_posterior",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method calculate_posterior implements structured analysis for D1-Q2"
            ],
            "justification": "This method contributes to D1-Q2 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "BayesianConfidenceCalculator.calculate_posterior algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute calculate_posterior"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from calculate_posterior"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use calculate_posterior results for downstream analysis"
            ]
          }
        }
      }
    },
    "PerformanceAnalyzer": {
      "file_path": "src/methods_dispensary/analyzer_one.py",
      "line_number": 1429,
      "class_docstring": "Analyze value chain performance with operational loss functions.",
      "total_usage_in_300_contracts": 60,
      "unique_questions": 40,
      "questions": [
        "Q002",
        "Q014",
        "Q016",
        "Q028",
        "Q032",
        "Q044",
        "Q046",
        "Q058",
        "Q062",
        "Q074",
        "Q076",
        "Q088",
        "Q092",
        "Q104",
        "Q106",
        "Q118",
        "Q122",
        "Q134",
        "Q136",
        "Q148",
        "Q152",
        "Q164",
        "Q166",
        "Q178",
        "Q182",
        "Q194",
        "Q196",
        "Q208",
        "Q212",
        "Q224",
        "Q226",
        "Q238",
        "Q242",
        "Q254",
        "Q256",
        "Q268",
        "Q272",
        "Q284",
        "Q286",
        "Q298"
      ],
      "base_slots": [
        "D1-Q2",
        "D3-Q4",
        "D4-Q1",
        "D6-Q3"
      ],
      "policy_areas": [
        "PA01",
        "PA02",
        "PA03",
        "PA04",
        "PA05",
        "PA06",
        "PA07",
        "PA08",
        "PA09",
        "PA10"
      ],
      "dimensions": [
        "DIM01",
        "DIM03",
        "DIM04",
        "DIM06"
      ],
      "methods": {
        "analyze_performance": {
          "canonical_abbreviation": "performanceanalyzer",
          "provides": "performanceanalyzer.analyze_performance",
          "role": "analyze_performance_analysis",
          "usage_count_in_300": 40,
          "questions": [
            "Q002",
            "Q014",
            "Q016",
            "Q028",
            "Q032",
            "Q044",
            "Q046",
            "Q058",
            "Q062",
            "Q074",
            "Q076",
            "Q088",
            "Q092",
            "Q104",
            "Q106",
            "Q118",
            "Q122",
            "Q134",
            "Q136",
            "Q148",
            "Q152",
            "Q164",
            "Q166",
            "Q178",
            "Q182",
            "Q194",
            "Q196",
            "Q208",
            "Q212",
            "Q224",
            "Q226",
            "Q238",
            "Q242",
            "Q254",
            "Q256",
            "Q268",
            "Q272",
            "Q284",
            "Q286",
            "Q298"
          ],
          "base_slots": [
            "D1-Q2",
            "D3-Q4",
            "D4-Q1",
            "D6-Q3"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM01",
            "DIM03",
            "DIM04",
            "DIM06"
          ],
          "signature": {
            "line_number": 1440,
            "is_async": false,
            "parameters": [
              {
                "name": "semantic_cube",
                "type": "dict[str, Any]"
              }
            ],
            "return_type": "dict[str, Any]",
            "docstring": "Analyze performance indicators across value chain links."
          },
          "epistemological_foundation": {
            "paradigm": "PerformanceAnalyzer analytical paradigm",
            "ontological_basis": "Analysis via PerformanceAnalyzer.analyze_performance",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method analyze_performance implements structured analysis for D1-Q2"
            ],
            "justification": "This method contributes to D1-Q2 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "PerformanceAnalyzer.analyze_performance algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute analyze_performance"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from analyze_performance"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use analyze_performance results for downstream analysis"
            ]
          }
        },
        "_calculate_loss_functions": {
          "canonical_abbreviation": "performanceanalyzer",
          "provides": "performanceanalyzer.calculate_loss_functions",
          "role": "_calculate_loss_functions_calculation",
          "usage_count_in_300": 10,
          "questions": [
            "Q014",
            "Q044",
            "Q074",
            "Q104",
            "Q134",
            "Q164",
            "Q194",
            "Q224",
            "Q254",
            "Q284"
          ],
          "base_slots": [
            "D3-Q4"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM03"
          ],
          "signature": {
            "line_number": 1546,
            "is_async": false,
            "parameters": [
              {
                "name": "metrics",
                "type": "dict[str, Any]"
              },
              {
                "name": "link_config",
                "type": "ValueChainLink"
              }
            ],
            "return_type": "dict[str, Any]",
            "docstring": "Calculate operational loss functions."
          },
          "epistemological_foundation": {
            "paradigm": "PerformanceAnalyzer analytical paradigm",
            "ontological_basis": "Analysis via PerformanceAnalyzer._calculate_loss_functions",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _calculate_loss_functions implements structured analysis for D3-Q4"
            ],
            "justification": "This method contributes to D3-Q4 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "PerformanceAnalyzer._calculate_loss_functions algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _calculate_loss_functions"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _calculate_loss_functions"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _calculate_loss_functions results for downstream analysis"
            ]
          }
        },
        "_generate_recommendations": {
          "canonical_abbreviation": "performanceanalyzer",
          "provides": "performanceanalyzer.generate_recommendations",
          "role": "_generate_recommendations_execution",
          "usage_count_in_300": 10,
          "questions": [
            "Q016",
            "Q046",
            "Q076",
            "Q106",
            "Q136",
            "Q166",
            "Q196",
            "Q226",
            "Q256",
            "Q286"
          ],
          "base_slots": [
            "D4-Q1"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM04"
          ],
          "signature": {
            "line_number": 1581,
            "is_async": false,
            "parameters": [
              {
                "name": "performance_analysis",
                "type": "dict[str, Any]"
              }
            ],
            "return_type": "list[dict[str, Any]]",
            "docstring": "Generate optimization recommendations."
          },
          "epistemological_foundation": {
            "paradigm": "PerformanceAnalyzer analytical paradigm",
            "ontological_basis": "Analysis via PerformanceAnalyzer._generate_recommendations",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _generate_recommendations implements structured analysis for D4-Q1"
            ],
            "justification": "This method contributes to D4-Q1 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "PerformanceAnalyzer._generate_recommendations algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _generate_recommendations"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _generate_recommendations"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _generate_recommendations results for downstream analysis"
            ]
          }
        }
      }
    },
    "MechanismPartExtractor": {
      "file_path": "src/methods_dispensary/derek_beach.py",
      "line_number": 1751,
      "class_docstring": "Extract Entity-Activity pairs for mechanism parts",
      "total_usage_in_300_contracts": 40,
      "unique_questions": 10,
      "questions": [
        "Q004",
        "Q034",
        "Q064",
        "Q094",
        "Q124",
        "Q154",
        "Q184",
        "Q214",
        "Q244",
        "Q274"
      ],
      "base_slots": [
        "D1-Q4"
      ],
      "policy_areas": [
        "PA01",
        "PA02",
        "PA03",
        "PA04",
        "PA05",
        "PA06",
        "PA07",
        "PA08",
        "PA09",
        "PA10"
      ],
      "dimensions": [
        "DIM01"
      ],
      "methods": {
        "extract_entity_activity": {
          "canonical_abbreviation": "mechanismpartextractor",
          "provides": "mechanismpartextractor.extract_entity_activity",
          "role": "extract_entity_activity_extraction",
          "usage_count_in_300": 10,
          "questions": [
            "Q004",
            "Q034",
            "Q064",
            "Q094",
            "Q124",
            "Q154",
            "Q184",
            "Q214",
            "Q244",
            "Q274"
          ],
          "base_slots": [
            "D1-Q4"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM01"
          ],
          "signature": {
            "line_number": 1761,
            "is_async": false,
            "parameters": [
              {
                "name": "text",
                "type": "str"
              }
            ],
            "return_type": "EntityActivity | None",
            "docstring": "Extract Entity-Activity tuple from text"
          },
          "epistemological_foundation": {
            "paradigm": "MechanismPartExtractor analytical paradigm",
            "ontological_basis": "Analysis via MechanismPartExtractor.extract_entity_activity",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method extract_entity_activity implements structured analysis for D1-Q4"
            ],
            "justification": "This method contributes to D1-Q4 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "MechanismPartExtractor.extract_entity_activity algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute extract_entity_activity"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from extract_entity_activity"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use extract_entity_activity results for downstream analysis"
            ]
          }
        },
        "_normalize_entity": {
          "canonical_abbreviation": "mechanismpartextractor",
          "provides": "mechanismpartextractor.normalize_entity",
          "role": "_normalize_entity_execution",
          "usage_count_in_300": 10,
          "questions": [
            "Q004",
            "Q034",
            "Q064",
            "Q094",
            "Q124",
            "Q154",
            "Q184",
            "Q214",
            "Q244",
            "Q274"
          ],
          "base_slots": [
            "D1-Q4"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM01"
          ],
          "signature": {
            "line_number": 1800,
            "is_async": false,
            "parameters": [
              {
                "name": "entity",
                "type": "str"
              }
            ],
            "return_type": "str",
            "docstring": "Normalize entity name using aliases"
          },
          "epistemological_foundation": {
            "paradigm": "MechanismPartExtractor analytical paradigm",
            "ontological_basis": "Analysis via MechanismPartExtractor._normalize_entity",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _normalize_entity implements structured analysis for D1-Q4"
            ],
            "justification": "This method contributes to D1-Q4 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "MechanismPartExtractor._normalize_entity algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _normalize_entity"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _normalize_entity"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _normalize_entity results for downstream analysis"
            ]
          }
        },
        "_validate_entity_activity": {
          "canonical_abbreviation": "mechanismpartextractor",
          "provides": "mechanismpartextractor.validate_entity_activity",
          "role": "_validate_entity_activity_validation",
          "usage_count_in_300": 10,
          "questions": [
            "Q004",
            "Q034",
            "Q064",
            "Q094",
            "Q124",
            "Q154",
            "Q184",
            "Q214",
            "Q244",
            "Q274"
          ],
          "base_slots": [
            "D1-Q4"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM01"
          ],
          "signature": {
            "line_number": 1886,
            "is_async": false,
            "parameters": [
              {
                "name": "entity",
                "type": "str"
              },
              {
                "name": "activity",
                "type": "str"
              }
            ],
            "return_type": "bool",
            "docstring": "\n        Validate that an entity-activity pair makes sense.\n\n        Args:\n            entity: Entity text\n            activity: Activity text\n\n        Returns:\n            True if valid pair\n        "
          },
          "epistemological_foundation": {
            "paradigm": "MechanismPartExtractor analytical paradigm",
            "ontological_basis": "Analysis via MechanismPartExtractor._validate_entity_activity",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _validate_entity_activity implements structured analysis for D1-Q4"
            ],
            "justification": "This method contributes to D1-Q4 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "MechanismPartExtractor._validate_entity_activity algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _validate_entity_activity"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _validate_entity_activity"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _validate_entity_activity results for downstream analysis"
            ]
          }
        },
        "_calculate_ea_confidence": {
          "canonical_abbreviation": "mechanismpartextractor",
          "provides": "mechanismpartextractor.calculate_ea_confidence",
          "role": "_calculate_ea_confidence_calculation",
          "usage_count_in_300": 10,
          "questions": [
            "Q004",
            "Q034",
            "Q064",
            "Q094",
            "Q124",
            "Q154",
            "Q184",
            "Q214",
            "Q244",
            "Q274"
          ],
          "base_slots": [
            "D1-Q4"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM01"
          ],
          "signature": {
            "line_number": 1806,
            "is_async": false,
            "parameters": [
              {
                "name": "entity",
                "type": "str"
              },
              {
                "name": "activity",
                "type": "str"
              },
              {
                "name": "context",
                "type": "str",
                "default": "''"
              }
            ],
            "return_type": "float",
            "docstring": "\n        Calculate confidence for an entity-activity pair.\n\n        Args:\n            entity: Entity text\n            activity: Activity text\n            context: Surrounding context\n\n        Returns:\n            Confidence score between 0 and 1\n        "
          },
          "epistemological_foundation": {
            "paradigm": "MechanismPartExtractor analytical paradigm",
            "ontological_basis": "Analysis via MechanismPartExtractor._calculate_ea_confidence",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _calculate_ea_confidence implements structured analysis for D1-Q4"
            ],
            "justification": "This method contributes to D1-Q4 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "MechanismPartExtractor._calculate_ea_confidence algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _calculate_ea_confidence"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _calculate_ea_confidence"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _calculate_ea_confidence results for downstream analysis"
            ]
          }
        }
      }
    },
    "TemporalLogicVerifier": {
      "file_path": "src/methods_dispensary/contradiction_deteccion.py",
      "line_number": 150,
      "class_docstring": "\n    Temporal consistency verification using Linear Temporal Logic (LTL).\n\n    Analyzes policy statements for temporal contradictions, deadline violations,\n    and ordering conflicts using temporal logic patterns.\n    ",
      "total_usage_in_300_contracts": 50,
      "unique_questions": 20,
      "questions": [
        "Q005",
        "Q016",
        "Q035",
        "Q046",
        "Q065",
        "Q076",
        "Q095",
        "Q106",
        "Q125",
        "Q136",
        "Q155",
        "Q166",
        "Q185",
        "Q196",
        "Q215",
        "Q226",
        "Q245",
        "Q256",
        "Q275",
        "Q286"
      ],
      "base_slots": [
        "D1-Q5",
        "D4-Q1"
      ],
      "policy_areas": [
        "PA01",
        "PA02",
        "PA03",
        "PA04",
        "PA05",
        "PA06",
        "PA07",
        "PA08",
        "PA09",
        "PA10"
      ],
      "dimensions": [
        "DIM01",
        "DIM04"
      ],
      "methods": {
        "_check_deadline_constraints": {
          "canonical_abbreviation": "temporallogicverifier",
          "provides": "temporallogicverifier.check_deadline_constraints",
          "role": "_check_deadline_constraints_execution",
          "usage_count_in_300": 10,
          "questions": [
            "Q005",
            "Q035",
            "Q065",
            "Q095",
            "Q125",
            "Q155",
            "Q185",
            "Q215",
            "Q245",
            "Q275"
          ],
          "base_slots": [
            "D1-Q5"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM01"
          ],
          "signature": {
            "line_number": 297,
            "is_async": false,
            "parameters": [
              {
                "name": "timeline",
                "type": "list[dict]"
              }
            ],
            "return_type": "list[dict]",
            "docstring": "Verifica violaciones de restricciones de plazo"
          },
          "epistemological_foundation": {
            "paradigm": "TemporalLogicVerifier analytical paradigm",
            "ontological_basis": "Analysis via TemporalLogicVerifier._check_deadline_constraints",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _check_deadline_constraints implements structured analysis for D1-Q5"
            ],
            "justification": "This method contributes to D1-Q5 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "TemporalLogicVerifier._check_deadline_constraints algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _check_deadline_constraints"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _check_deadline_constraints"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _check_deadline_constraints results for downstream analysis"
            ]
          }
        },
        "verify_temporal_consistency": {
          "canonical_abbreviation": "temporallogicverifier",
          "provides": "temporallogicverifier.verify_temporal_consistency",
          "role": "verify_temporal_consistency_execution",
          "usage_count_in_300": 10,
          "questions": [
            "Q005",
            "Q035",
            "Q065",
            "Q095",
            "Q125",
            "Q155",
            "Q185",
            "Q215",
            "Q245",
            "Q275"
          ],
          "base_slots": [
            "D1-Q5"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM01"
          ],
          "signature": {
            "line_number": 166,
            "is_async": false,
            "parameters": [
              {
                "name": "statements",
                "type": "list[PolicyStatement]"
              }
            ],
            "return_type": "tuple[bool, list[dict[str, Any]]]",
            "docstring": "\n        Verify temporal consistency between policy statements.\n\n        Analyzes temporal ordering and deadline constraints to identify\n        contradictions or violations in the policy timeline.\n\n        Args:\n            statements: List of policy statements to analyze\n\n        Returns:\n            tuple[bool, list[dict]]: A tuple containing:\n                - is_consistent: True if no conflicts found\n                - conflicts_found: List of detected temporal conflicts\n        "
          },
          "epistemological_foundation": {
            "paradigm": "TemporalLogicVerifier analytical paradigm",
            "ontological_basis": "Analysis via TemporalLogicVerifier.verify_temporal_consistency",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method verify_temporal_consistency implements structured analysis for D1-Q5"
            ],
            "justification": "This method contributes to D1-Q5 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "TemporalLogicVerifier.verify_temporal_consistency algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute verify_temporal_consistency"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from verify_temporal_consistency"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use verify_temporal_consistency results for downstream analysis"
            ]
          }
        },
        "_classify_temporal_type": {
          "canonical_abbreviation": "temporallogicverifier",
          "provides": "temporallogicverifier.classify_temporal_type",
          "role": "_classify_temporal_type_execution",
          "usage_count_in_300": 10,
          "questions": [
            "Q016",
            "Q046",
            "Q076",
            "Q106",
            "Q136",
            "Q166",
            "Q196",
            "Q226",
            "Q256",
            "Q286"
          ],
          "base_slots": [
            "D4-Q1"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM04"
          ],
          "signature": {
            "line_number": 321,
            "is_async": false,
            "parameters": [
              {
                "name": "marker",
                "type": "str"
              }
            ],
            "return_type": "str",
            "docstring": "Clasifica el tipo de marcador temporal"
          },
          "epistemological_foundation": {
            "paradigm": "TemporalLogicVerifier analytical paradigm",
            "ontological_basis": "Analysis via TemporalLogicVerifier._classify_temporal_type",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _classify_temporal_type implements structured analysis for D4-Q1"
            ],
            "justification": "This method contributes to D4-Q1 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "TemporalLogicVerifier._classify_temporal_type algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _classify_temporal_type"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _classify_temporal_type"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _classify_temporal_type results for downstream analysis"
            ]
          }
        },
        "_parse_temporal_marker": {
          "canonical_abbreviation": "temporallogicverifier",
          "provides": "temporallogicverifier.parse_temporal_marker",
          "role": "_parse_temporal_marker_parsing",
          "usage_count_in_300": 10,
          "questions": [
            "Q016",
            "Q046",
            "Q076",
            "Q106",
            "Q136",
            "Q166",
            "Q196",
            "Q226",
            "Q256",
            "Q286"
          ],
          "base_slots": [
            "D4-Q1"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM04"
          ],
          "signature": {
            "line_number": 229,
            "is_async": false,
            "parameters": [
              {
                "name": "marker",
                "type": "str"
              }
            ],
            "return_type": "int | None",
            "docstring": "\n        Parse temporal marker to numeric timestamp.\n\n        Implements Colombian policy document temporal format parsing.\n\n        Args:\n            marker: Temporal marker string (e.g., \"2024\", \"Q2\", \"segundo trimestre\")\n\n        Returns:\n            int | None: Numeric timestamp, or None if parsing fails\n        "
          },
          "epistemological_foundation": {
            "paradigm": "TemporalLogicVerifier analytical paradigm",
            "ontological_basis": "Analysis via TemporalLogicVerifier._parse_temporal_marker",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _parse_temporal_marker implements structured analysis for D4-Q1"
            ],
            "justification": "This method contributes to D4-Q1 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "TemporalLogicVerifier._parse_temporal_marker algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _parse_temporal_marker"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _parse_temporal_marker"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _parse_temporal_marker results for downstream analysis"
            ]
          }
        },
        "_extract_resources": {
          "canonical_abbreviation": "temporallogicverifier",
          "provides": "temporallogicverifier.extract_resources",
          "role": "_extract_resources_extraction",
          "usage_count_in_300": 10,
          "questions": [
            "Q016",
            "Q046",
            "Q076",
            "Q106",
            "Q136",
            "Q166",
            "Q196",
            "Q226",
            "Q256",
            "Q286"
          ],
          "base_slots": [
            "D4-Q1"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM04"
          ],
          "signature": {
            "line_number": 281,
            "is_async": false,
            "parameters": [
              {
                "name": "text",
                "type": "str"
              }
            ],
            "return_type": "list[str]",
            "docstring": "Extrae recursos mencionados en el texto"
          },
          "epistemological_foundation": {
            "paradigm": "TemporalLogicVerifier analytical paradigm",
            "ontological_basis": "Analysis via TemporalLogicVerifier._extract_resources",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _extract_resources implements structured analysis for D4-Q1"
            ],
            "justification": "This method contributes to D4-Q1 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "TemporalLogicVerifier._extract_resources algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _extract_resources"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _extract_resources"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _extract_resources results for downstream analysis"
            ]
          }
        }
      }
    },
    "CausalInferenceSetup": {
      "file_path": "src/methods_dispensary/derek_beach.py",
      "line_number": 3594,
      "class_docstring": "Prepare model for causal inference",
      "total_usage_in_300_contracts": 40,
      "unique_questions": 30,
      "questions": [
        "Q005",
        "Q011",
        "Q030",
        "Q035",
        "Q041",
        "Q060",
        "Q065",
        "Q071",
        "Q090",
        "Q095",
        "Q101",
        "Q120",
        "Q125",
        "Q131",
        "Q150",
        "Q155",
        "Q161",
        "Q180",
        "Q185",
        "Q191",
        "Q210",
        "Q215",
        "Q221",
        "Q240",
        "Q245",
        "Q251",
        "Q270",
        "Q275",
        "Q281",
        "Q300"
      ],
      "base_slots": [
        "D1-Q5",
        "D3-Q1",
        "D6-Q5"
      ],
      "policy_areas": [
        "PA01",
        "PA02",
        "PA03",
        "PA04",
        "PA05",
        "PA06",
        "PA07",
        "PA08",
        "PA09",
        "PA10"
      ],
      "dimensions": [
        "DIM01",
        "DIM03",
        "DIM06"
      ],
      "methods": {
        "identify_failure_points": {
          "canonical_abbreviation": "causalinferencesetup",
          "provides": "causalinferencesetup.identify_failure_points",
          "role": "identify_failure_points_execution",
          "usage_count_in_300": 20,
          "questions": [
            "Q005",
            "Q030",
            "Q035",
            "Q060",
            "Q065",
            "Q090",
            "Q095",
            "Q120",
            "Q125",
            "Q150",
            "Q155",
            "Q180",
            "Q185",
            "Q210",
            "Q215",
            "Q240",
            "Q245",
            "Q270",
            "Q275",
            "Q300"
          ],
          "base_slots": [
            "D1-Q5",
            "D6-Q5"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM01",
            "DIM06"
          ],
          "signature": {
            "line_number": 3677,
            "is_async": false,
            "parameters": [
              {
                "name": "graph",
                "type": "nx.DiGraph"
              },
              {
                "name": "text",
                "type": "str"
              }
            ],
            "return_type": "set[str]",
            "docstring": "Identify single points of failure in causal chain\n\n        Harmonic Front 3 - Enhancement 2: Contextual Failure Point Detection\n        Expands risk_pattern to explicitly include localized contextual factors from rubrics:\n        - restricciones territoriales\n        - patrones culturales machistas\n        - limitación normativa\n\n        For D6-Q5 (Enfoque Diferencial/Restricciones): Excelente requires ≥3 distinct\n        contextual factors correctly mapped to nodes, satisfying enfoque_diferenci"
          },
          "epistemological_foundation": {
            "paradigm": "CausalInferenceSetup analytical paradigm",
            "ontological_basis": "Analysis via CausalInferenceSetup.identify_failure_points",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method identify_failure_points implements structured analysis for D1-Q5"
            ],
            "justification": "This method contributes to D1-Q5 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "CausalInferenceSetup.identify_failure_points algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute identify_failure_points"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from identify_failure_points"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use identify_failure_points results for downstream analysis"
            ]
          }
        },
        "assign_probative_value": {
          "canonical_abbreviation": "causalinferencesetup",
          "provides": "causalinferencesetup.assign_probative_value",
          "role": "assign_probative_value_execution",
          "usage_count_in_300": 10,
          "questions": [
            "Q011",
            "Q041",
            "Q071",
            "Q101",
            "Q131",
            "Q161",
            "Q191",
            "Q221",
            "Q251",
            "Q281"
          ],
          "base_slots": [
            "D3-Q1"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM03"
          ],
          "signature": {
            "line_number": 3617,
            "is_async": false,
            "parameters": [
              {
                "name": "nodes",
                "type": "dict[str, MetaNode]"
              }
            ],
            "return_type": "None",
            "docstring": "Assign probative test types to nodes"
          },
          "epistemological_foundation": {
            "paradigm": "CausalInferenceSetup analytical paradigm",
            "ontological_basis": "Analysis via CausalInferenceSetup.assign_probative_value",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method assign_probative_value implements structured analysis for D3-Q1"
            ],
            "justification": "This method contributes to D3-Q1 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "CausalInferenceSetup.assign_probative_value algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute assign_probative_value"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from assign_probative_value"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use assign_probative_value results for downstream analysis"
            ]
          }
        },
        "_get_dynamics_pattern": {
          "canonical_abbreviation": "causalinferencesetup",
          "provides": "causalinferencesetup.get_dynamics_pattern",
          "role": "_get_dynamics_pattern_execution",
          "usage_count_in_300": 10,
          "questions": [
            "Q030",
            "Q060",
            "Q090",
            "Q120",
            "Q150",
            "Q180",
            "Q210",
            "Q240",
            "Q270",
            "Q300"
          ],
          "base_slots": [
            "D6-Q5"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM06"
          ],
          "signature": {
            "line_number": 3776,
            "is_async": false,
            "parameters": [
              {
                "name": "dynamics_type",
                "type": "str"
              }
            ],
            "return_type": "str",
            "docstring": "\n        Get the pattern associated with a dynamics type.\n\n        Args:\n            dynamics_type: Type of dynamics (suma, decreciente, constante, indefinido)\n\n        Returns:\n            Pattern string for the dynamics type\n        "
          },
          "epistemological_foundation": {
            "paradigm": "CausalInferenceSetup analytical paradigm",
            "ontological_basis": "Analysis via CausalInferenceSetup._get_dynamics_pattern",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _get_dynamics_pattern implements structured analysis for D6-Q5"
            ],
            "justification": "This method contributes to D6-Q5 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "CausalInferenceSetup._get_dynamics_pattern algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _get_dynamics_pattern"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _get_dynamics_pattern"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _get_dynamics_pattern results for downstream analysis"
            ]
          }
        }
      }
    },
    "PDFProcessor": {
      "file_path": "src/methods_dispensary/derek_beach.py",
      "line_number": 848,
      "class_docstring": "Advanced PDF processing and extraction",
      "total_usage_in_300_contracts": 10,
      "unique_questions": 10,
      "questions": [
        "Q006",
        "Q036",
        "Q066",
        "Q096",
        "Q126",
        "Q156",
        "Q186",
        "Q216",
        "Q246",
        "Q276"
      ],
      "base_slots": [
        "D2-Q1"
      ],
      "policy_areas": [
        "PA01",
        "PA02",
        "PA03",
        "PA04",
        "PA05",
        "PA06",
        "PA07",
        "PA08",
        "PA09",
        "PA10"
      ],
      "dimensions": [
        "DIM02"
      ],
      "methods": {
        "extract_tables": {
          "canonical_abbreviation": "pdfprocessor",
          "provides": "pdfprocessor.extract_tables",
          "role": "extract_tables_extraction",
          "usage_count_in_300": 10,
          "questions": [
            "Q006",
            "Q036",
            "Q066",
            "Q096",
            "Q126",
            "Q156",
            "Q186",
            "Q216",
            "Q246",
            "Q276"
          ],
          "base_slots": [
            "D2-Q1"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM02"
          ],
          "signature": {
            "line_number": 916,
            "is_async": false,
            "parameters": [],
            "return_type": "list[pd.DataFrame]",
            "docstring": "Extract tables from PDF"
          },
          "epistemological_foundation": {
            "paradigm": "PDFProcessor analytical paradigm",
            "ontological_basis": "Analysis via PDFProcessor.extract_tables",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method extract_tables implements structured analysis for D2-Q1"
            ],
            "justification": "This method contributes to D2-Q1 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "PDFProcessor.extract_tables algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute extract_tables"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from extract_tables"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use extract_tables results for downstream analysis"
            ]
          }
        }
      }
    },
    "ReportingEngine": {
      "file_path": "src/methods_dispensary/derek_beach.py",
      "line_number": 3794,
      "class_docstring": "Generate visualizations and reports",
      "total_usage_in_300_contracts": 50,
      "unique_questions": 40,
      "questions": [
        "Q006",
        "Q013",
        "Q022",
        "Q026",
        "Q036",
        "Q043",
        "Q052",
        "Q056",
        "Q066",
        "Q073",
        "Q082",
        "Q086",
        "Q096",
        "Q103",
        "Q112",
        "Q116",
        "Q126",
        "Q133",
        "Q142",
        "Q146",
        "Q156",
        "Q163",
        "Q172",
        "Q176",
        "Q186",
        "Q193",
        "Q202",
        "Q206",
        "Q216",
        "Q223",
        "Q232",
        "Q236",
        "Q246",
        "Q253",
        "Q262",
        "Q266",
        "Q276",
        "Q283",
        "Q292",
        "Q296"
      ],
      "base_slots": [
        "D2-Q1",
        "D3-Q3",
        "D5-Q2",
        "D6-Q1"
      ],
      "policy_areas": [
        "PA01",
        "PA02",
        "PA03",
        "PA04",
        "PA05",
        "PA06",
        "PA07",
        "PA08",
        "PA09",
        "PA10"
      ],
      "dimensions": [
        "DIM02",
        "DIM03",
        "DIM05",
        "DIM06"
      ],
      "methods": {
        "generate_accountability_matrix": {
          "canonical_abbreviation": "reportingengine",
          "provides": "reportingengine.generate_accountability_matrix",
          "role": "generate_accountability_matrix_execution",
          "usage_count_in_300": 20,
          "questions": [
            "Q006",
            "Q013",
            "Q036",
            "Q043",
            "Q066",
            "Q073",
            "Q096",
            "Q103",
            "Q126",
            "Q133",
            "Q156",
            "Q163",
            "Q186",
            "Q193",
            "Q216",
            "Q223",
            "Q246",
            "Q253",
            "Q276",
            "Q283"
          ],
          "base_slots": [
            "D2-Q1",
            "D3-Q3"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM02",
            "DIM03"
          ],
          "signature": {
            "line_number": 3892,
            "is_async": false,
            "parameters": [
              {
                "name": "graph",
                "type": "nx.DiGraph"
              },
              {
                "name": "policy_code",
                "type": "str"
              }
            ],
            "return_type": "Path",
            "docstring": "Generate accountability matrix in Markdown"
          },
          "epistemological_foundation": {
            "paradigm": "ReportingEngine analytical paradigm",
            "ontological_basis": "Analysis via ReportingEngine.generate_accountability_matrix",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method generate_accountability_matrix implements structured analysis for D2-Q1"
            ],
            "justification": "This method contributes to D2-Q1 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "ReportingEngine.generate_accountability_matrix algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute generate_accountability_matrix"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from generate_accountability_matrix"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use generate_accountability_matrix results for downstream analysis"
            ]
          }
        },
        "_calculate_quality_score": {
          "canonical_abbreviation": "reportingengine",
          "provides": "reportingengine.calculate_quality_score",
          "role": "_calculate_quality_score_calculation",
          "usage_count_in_300": 10,
          "questions": [
            "Q022",
            "Q052",
            "Q082",
            "Q112",
            "Q142",
            "Q172",
            "Q202",
            "Q232",
            "Q262",
            "Q292"
          ],
          "base_slots": [
            "D5-Q2"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM05"
          ],
          "signature": {
            "line_number": 4043,
            "is_async": false,
            "parameters": [
              {
                "name": "traceability",
                "type": "float"
              },
              {
                "name": "financial",
                "type": "float"
              },
              {
                "name": "logic",
                "type": "float"
              },
              {
                "name": "ea",
                "type": "float"
              }
            ],
            "return_type": "float",
            "docstring": "Calculate overall quality score (0-100)"
          },
          "epistemological_foundation": {
            "paradigm": "ReportingEngine analytical paradigm",
            "ontological_basis": "Analysis via ReportingEngine._calculate_quality_score",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _calculate_quality_score implements structured analysis for D5-Q2"
            ],
            "justification": "This method contributes to D5-Q2 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "ReportingEngine._calculate_quality_score algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _calculate_quality_score"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _calculate_quality_score"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _calculate_quality_score results for downstream analysis"
            ]
          }
        },
        "generate_causal_diagram": {
          "canonical_abbreviation": "reportingengine",
          "provides": "reportingengine.generate_causal_diagram",
          "role": "generate_causal_diagram_execution",
          "usage_count_in_300": 10,
          "questions": [
            "Q026",
            "Q056",
            "Q086",
            "Q116",
            "Q146",
            "Q176",
            "Q206",
            "Q236",
            "Q266",
            "Q296"
          ],
          "base_slots": [
            "D6-Q1"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM06"
          ],
          "signature": {
            "line_number": 3804,
            "is_async": false,
            "parameters": [
              {
                "name": "graph",
                "type": "nx.DiGraph"
              },
              {
                "name": "policy_code",
                "type": "str"
              }
            ],
            "return_type": "Path",
            "docstring": "Generate causal diagram visualization"
          },
          "epistemological_foundation": {
            "paradigm": "ReportingEngine analytical paradigm",
            "ontological_basis": "Analysis via ReportingEngine.generate_causal_diagram",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method generate_causal_diagram implements structured analysis for D6-Q1"
            ],
            "justification": "This method contributes to D6-Q1 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "ReportingEngine.generate_causal_diagram algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute generate_causal_diagram"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from generate_causal_diagram"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use generate_causal_diagram results for downstream analysis"
            ]
          }
        },
        "generate_causal_model_json": {
          "canonical_abbreviation": "reportingengine",
          "provides": "reportingengine.generate_causal_model_json",
          "role": "generate_causal_model_json_execution",
          "usage_count_in_300": 10,
          "questions": [
            "Q026",
            "Q056",
            "Q086",
            "Q116",
            "Q146",
            "Q176",
            "Q206",
            "Q236",
            "Q266",
            "Q296"
          ],
          "base_slots": [
            "D6-Q1"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM06"
          ],
          "signature": {
            "line_number": 4054,
            "is_async": false,
            "parameters": [
              {
                "name": "graph",
                "type": "nx.DiGraph"
              },
              {
                "name": "nodes",
                "type": "dict[str, MetaNode]"
              },
              {
                "name": "policy_code",
                "type": "str"
              }
            ],
            "return_type": "Path",
            "docstring": "Generate structured JSON export of causal model"
          },
          "epistemological_foundation": {
            "paradigm": "ReportingEngine analytical paradigm",
            "ontological_basis": "Analysis via ReportingEngine.generate_causal_model_json",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method generate_causal_model_json implements structured analysis for D6-Q1"
            ],
            "justification": "This method contributes to D6-Q1 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "ReportingEngine.generate_causal_model_json algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute generate_causal_model_json"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from generate_causal_model_json"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use generate_causal_model_json results for downstream analysis"
            ]
          }
        }
      }
    },
    "TeoriaCambio": {
      "file_path": "src/methods_dispensary/teoria_cambio.py",
      "line_number": 276,
      "class_docstring": "\n    Motor para la construcción y validación estructural de teorías de cambio.\n    Valida la coherencia lógica de grafos causales contra un modelo axiomático\n    de categorías jerárquicas, crucial para el análisis de políticas públicas.\n    ",
      "total_usage_in_300_contracts": 110,
      "unique_questions": 70,
      "questions": [
        "Q007",
        "Q012",
        "Q014",
        "Q015",
        "Q017",
        "Q021",
        "Q026",
        "Q037",
        "Q042",
        "Q044",
        "Q045",
        "Q047",
        "Q051",
        "Q056",
        "Q067",
        "Q072",
        "Q074",
        "Q075",
        "Q077",
        "Q081",
        "Q086",
        "Q097",
        "Q102",
        "Q104",
        "Q105",
        "Q107",
        "Q111",
        "Q116",
        "Q127",
        "Q132",
        "Q134",
        "Q135",
        "Q137",
        "Q141",
        "Q146",
        "Q157",
        "Q162",
        "Q164",
        "Q165",
        "Q167",
        "Q171",
        "Q176",
        "Q187",
        "Q192",
        "Q194",
        "Q195",
        "Q197",
        "Q201",
        "Q206",
        "Q217",
        "Q222",
        "Q224",
        "Q225",
        "Q227",
        "Q231",
        "Q236",
        "Q247",
        "Q252",
        "Q254",
        "Q255",
        "Q257",
        "Q261",
        "Q266",
        "Q277",
        "Q282",
        "Q284",
        "Q285",
        "Q287",
        "Q291",
        "Q296"
      ],
      "base_slots": [
        "D2-Q2",
        "D3-Q2",
        "D3-Q4",
        "D3-Q5",
        "D4-Q2",
        "D5-Q1",
        "D6-Q1"
      ],
      "policy_areas": [
        "PA01",
        "PA02",
        "PA03",
        "PA04",
        "PA05",
        "PA06",
        "PA07",
        "PA08",
        "PA09",
        "PA10"
      ],
      "dimensions": [
        "DIM02",
        "DIM03",
        "DIM04",
        "DIM05",
        "DIM06"
      ],
      "methods": {
        "construir_grafo_causal": {
          "canonical_abbreviation": "teoriacambio",
          "provides": "teoriacambio.construir_grafo_causal",
          "role": "construir_grafo_causal_execution",
          "usage_count_in_300": 20,
          "questions": [
            "Q007",
            "Q026",
            "Q037",
            "Q056",
            "Q067",
            "Q086",
            "Q097",
            "Q116",
            "Q127",
            "Q146",
            "Q157",
            "Q176",
            "Q187",
            "Q206",
            "Q217",
            "Q236",
            "Q247",
            "Q266",
            "Q277",
            "Q296"
          ],
          "base_slots": [
            "D2-Q2",
            "D6-Q1"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM02",
            "DIM06"
          ],
          "signature": {
            "line_number": 305,
            "is_async": false,
            "parameters": [],
            "return_type": "nx.DiGraph",
            "docstring": "Construye y cachea el grafo causal canónico."
          },
          "epistemological_foundation": {
            "paradigm": "TeoriaCambio analytical paradigm",
            "ontological_basis": "Analysis via TeoriaCambio.construir_grafo_causal",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method construir_grafo_causal implements structured analysis for D2-Q2"
            ],
            "justification": "This method contributes to D2-Q2 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "TeoriaCambio.construir_grafo_causal algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute construir_grafo_causal"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from construir_grafo_causal"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use construir_grafo_causal results for downstream analysis"
            ]
          }
        },
        "_es_conexion_valida": {
          "canonical_abbreviation": "teoriacambio",
          "provides": "teoriacambio.es_conexion_valida",
          "role": "_es_conexion_valida_execution",
          "usage_count_in_300": 10,
          "questions": [
            "Q007",
            "Q037",
            "Q067",
            "Q097",
            "Q127",
            "Q157",
            "Q187",
            "Q217",
            "Q247",
            "Q277"
          ],
          "base_slots": [
            "D2-Q2"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM02"
          ],
          "signature": {
            "line_number": 299,
            "is_async": false,
            "parameters": [
              {
                "name": "origen",
                "type": "CategoriaCausal"
              },
              {
                "name": "destino",
                "type": "CategoriaCausal"
              }
            ],
            "return_type": "bool",
            "docstring": "Verifica la validez de una conexión causal según la jerarquía estructural."
          },
          "epistemological_foundation": {
            "paradigm": "TeoriaCambio analytical paradigm",
            "ontological_basis": "Analysis via TeoriaCambio._es_conexion_valida",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _es_conexion_valida implements structured analysis for D2-Q2"
            ],
            "justification": "This method contributes to D2-Q2 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "TeoriaCambio._es_conexion_valida algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _es_conexion_valida"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _es_conexion_valida"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _es_conexion_valida results for downstream analysis"
            ]
          }
        },
        "_generar_sugerencias_internas": {
          "canonical_abbreviation": "teoriacambio",
          "provides": "teoriacambio.generar_sugerencias_internas",
          "role": "_generar_sugerencias_internas_execution",
          "usage_count_in_300": 10,
          "questions": [
            "Q012",
            "Q042",
            "Q072",
            "Q102",
            "Q132",
            "Q162",
            "Q192",
            "Q222",
            "Q252",
            "Q282"
          ],
          "base_slots": [
            "D3-Q2"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM03"
          ],
          "signature": {
            "line_number": 496,
            "is_async": false,
            "parameters": [
              {
                "name": "validacion",
                "type": "ValidacionResultado"
              }
            ],
            "return_type": "list[str]",
            "docstring": "Genera un listado de sugerencias accionables basadas en los resultados."
          },
          "epistemological_foundation": {
            "paradigm": "TeoriaCambio analytical paradigm",
            "ontological_basis": "Analysis via TeoriaCambio._generar_sugerencias_internas",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _generar_sugerencias_internas implements structured analysis for D3-Q2"
            ],
            "justification": "This method contributes to D3-Q2 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "TeoriaCambio._generar_sugerencias_internas algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _generar_sugerencias_internas"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _generar_sugerencias_internas"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _generar_sugerencias_internas results for downstream analysis"
            ]
          }
        },
        "_extraer_categorias": {
          "canonical_abbreviation": "teoriacambio",
          "provides": "teoriacambio.extraer_categorias",
          "role": "_extraer_categorias_execution",
          "usage_count_in_300": 10,
          "questions": [
            "Q014",
            "Q044",
            "Q074",
            "Q104",
            "Q134",
            "Q164",
            "Q194",
            "Q224",
            "Q254",
            "Q284"
          ],
          "base_slots": [
            "D3-Q4"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM03"
          ],
          "signature": {
            "line_number": 452,
            "is_async": false,
            "parameters": [
              {
                "name": "grafo",
                "type": "nx.DiGraph"
              }
            ],
            "return_type": "set[str]",
            "docstring": "Extrae el conjunto de categorías presentes en el grafo."
          },
          "epistemological_foundation": {
            "paradigm": "TeoriaCambio analytical paradigm",
            "ontological_basis": "Analysis via TeoriaCambio._extraer_categorias",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _extraer_categorias implements structured analysis for D3-Q4"
            ],
            "justification": "This method contributes to D3-Q4 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "TeoriaCambio._extraer_categorias algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _extraer_categorias"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _extraer_categorias"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _extraer_categorias results for downstream analysis"
            ]
          }
        },
        "_validar_orden_causal": {
          "canonical_abbreviation": "teoriacambio",
          "provides": "teoriacambio.validar_orden_causal",
          "role": "_validar_orden_causal_execution",
          "usage_count_in_300": 20,
          "questions": [
            "Q015",
            "Q021",
            "Q045",
            "Q051",
            "Q075",
            "Q081",
            "Q105",
            "Q111",
            "Q135",
            "Q141",
            "Q165",
            "Q171",
            "Q195",
            "Q201",
            "Q225",
            "Q231",
            "Q255",
            "Q261",
            "Q285",
            "Q291"
          ],
          "base_slots": [
            "D3-Q5",
            "D5-Q1"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM03",
            "DIM05"
          ],
          "signature": {
            "line_number": 461,
            "is_async": false,
            "parameters": [
              {
                "name": "grafo",
                "type": "nx.DiGraph"
              }
            ],
            "return_type": "list[tuple[str, str]]",
            "docstring": "Identifica las aristas que violan el orden causal axiomático."
          },
          "epistemological_foundation": {
            "paradigm": "TeoriaCambio analytical paradigm",
            "ontological_basis": "Analysis via TeoriaCambio._validar_orden_causal",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _validar_orden_causal implements structured analysis for D3-Q5"
            ],
            "justification": "This method contributes to D3-Q5 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "TeoriaCambio._validar_orden_causal algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _validar_orden_causal"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _validar_orden_causal"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _validar_orden_causal results for downstream analysis"
            ]
          }
        },
        "_encontrar_caminos_completos": {
          "canonical_abbreviation": "teoriacambio",
          "provides": "teoriacambio.encontrar_caminos_completos",
          "role": "_encontrar_caminos_completos_execution",
          "usage_count_in_300": 10,
          "questions": [
            "Q017",
            "Q047",
            "Q077",
            "Q107",
            "Q137",
            "Q167",
            "Q197",
            "Q227",
            "Q257",
            "Q287"
          ],
          "base_slots": [
            "D4-Q2"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM04"
          ],
          "signature": {
            "line_number": 472,
            "is_async": false,
            "parameters": [
              {
                "name": "grafo",
                "type": "nx.DiGraph"
              }
            ],
            "return_type": "list[list[str]]",
            "docstring": "Encuentra todos los caminos simples desde nodos INSUMOS a CAUSALIDAD."
          },
          "epistemological_foundation": {
            "paradigm": "TeoriaCambio analytical paradigm",
            "ontological_basis": "Analysis via TeoriaCambio._encontrar_caminos_completos",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _encontrar_caminos_completos implements structured analysis for D4-Q2"
            ],
            "justification": "This method contributes to D4-Q2 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "TeoriaCambio._encontrar_caminos_completos algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _encontrar_caminos_completos"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _encontrar_caminos_completos"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _encontrar_caminos_completos results for downstream analysis"
            ]
          }
        },
        "validacion_completa": {
          "canonical_abbreviation": "teoriacambio",
          "provides": "teoriacambio.validacion_completa",
          "role": "validacion_completa_execution",
          "usage_count_in_300": 20,
          "questions": [
            "Q017",
            "Q026",
            "Q047",
            "Q056",
            "Q077",
            "Q086",
            "Q107",
            "Q116",
            "Q137",
            "Q146",
            "Q167",
            "Q176",
            "Q197",
            "Q206",
            "Q227",
            "Q236",
            "Q257",
            "Q266",
            "Q287",
            "Q296"
          ],
          "base_slots": [
            "D4-Q2",
            "D6-Q1"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM04",
            "DIM06"
          ],
          "signature": {
            "line_number": 436,
            "is_async": false,
            "parameters": [
              {
                "name": "grafo",
                "type": "nx.DiGraph"
              }
            ],
            "return_type": "ValidacionResultado",
            "docstring": "Ejecuta una validación estructural exhaustiva de la teoría de cambio."
          },
          "epistemological_foundation": {
            "paradigm": "TeoriaCambio analytical paradigm",
            "ontological_basis": "Analysis via TeoriaCambio.validacion_completa",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method validacion_completa implements structured analysis for D4-Q2"
            ],
            "justification": "This method contributes to D4-Q2 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "TeoriaCambio.validacion_completa algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute validacion_completa"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from validacion_completa"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use validacion_completa results for downstream analysis"
            ]
          }
        },
        "export_nodes": {
          "canonical_abbreviation": "teoriacambio",
          "provides": "teoriacambio.export_nodes",
          "role": "export_nodes_execution",
          "usage_count_in_300": 10,
          "questions": [
            "Q026",
            "Q056",
            "Q086",
            "Q116",
            "Q146",
            "Q176",
            "Q206",
            "Q236",
            "Q266",
            "Q296"
          ],
          "base_slots": [
            "D6-Q1"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM06"
          ],
          "signature": {
            "line_number": null,
            "is_async": false,
            "parameters": [],
            "return_type": null,
            "docstring": null
          },
          "epistemological_foundation": {
            "paradigm": "TeoriaCambio analytical paradigm",
            "ontological_basis": "Analysis via TeoriaCambio.export_nodes",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method export_nodes implements structured analysis for D6-Q1"
            ],
            "justification": "This method contributes to D6-Q1 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "TeoriaCambio.export_nodes algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute export_nodes"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from export_nodes"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use export_nodes results for downstream analysis"
            ]
          }
        }
      }
    },
    "BeachEvidentialTest": {
      "file_path": "src/methods_dispensary/derek_beach.py",
      "line_number": 97,
      "class_docstring": "\n    Derek Beach evidential tests implementation (Beach & Pedersen 2019: Ch 5).\n\n    FOUR-FOLD TYPOLOGY calibrated by necessity (N) and sufficiency (S):\n\n    HOOP TEST [N: High, S: Low]:\n    - Fail → ELIMINATES hypothesis (definitive knock-out)\n    - Pass → Hypothesis survives but not proven\n    - Example: \"Responsible entity must be documented\"\n\n    SMOKING GUN [N: Low, S: High]:\n    - Pass → Strongly confirms hypothesis\n    - Fail → Doesn't eliminate (could be false negative)\n    - Example: \"U",
      "total_usage_in_300_contracts": 30,
      "unique_questions": 30,
      "questions": [
        "Q007",
        "Q011",
        "Q027",
        "Q037",
        "Q041",
        "Q057",
        "Q067",
        "Q071",
        "Q087",
        "Q097",
        "Q101",
        "Q117",
        "Q127",
        "Q131",
        "Q147",
        "Q157",
        "Q161",
        "Q177",
        "Q187",
        "Q191",
        "Q207",
        "Q217",
        "Q221",
        "Q237",
        "Q247",
        "Q251",
        "Q267",
        "Q277",
        "Q281",
        "Q297"
      ],
      "base_slots": [
        "D2-Q2",
        "D3-Q1",
        "D6-Q2"
      ],
      "policy_areas": [
        "PA01",
        "PA02",
        "PA03",
        "PA04",
        "PA05",
        "PA06",
        "PA07",
        "PA08",
        "PA09",
        "PA10"
      ],
      "dimensions": [
        "DIM02",
        "DIM03",
        "DIM06"
      ],
      "methods": {
        "classify_test": {
          "canonical_abbreviation": "beachevidentialtest",
          "provides": "beachevidentialtest.classify_test",
          "role": "classify_test_execution",
          "usage_count_in_300": 10,
          "questions": [
            "Q007",
            "Q037",
            "Q067",
            "Q097",
            "Q127",
            "Q157",
            "Q187",
            "Q217",
            "Q247",
            "Q277"
          ],
          "base_slots": [
            "D2-Q2"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM02"
          ],
          "signature": {
            "line_number": 126,
            "is_async": false,
            "parameters": [
              {
                "name": "necessity",
                "type": "float"
              },
              {
                "name": "sufficiency",
                "type": "float"
              }
            ],
            "return_type": "TestType",
            "docstring": "\n        Classify evidential test type based on necessity and sufficiency.\n\n        Beach calibration:\n        - Necessity > 0.7 → High necessity\n        - Sufficiency > 0.7 → High sufficiency\n        "
          },
          "epistemological_foundation": {
            "paradigm": "BeachEvidentialTest analytical paradigm",
            "ontological_basis": "Analysis via BeachEvidentialTest.classify_test",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method classify_test implements structured analysis for D2-Q2"
            ],
            "justification": "This method contributes to D2-Q2 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "BeachEvidentialTest.classify_test algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute classify_test"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from classify_test"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use classify_test results for downstream analysis"
            ]
          }
        },
        "apply_test_logic": {
          "canonical_abbreviation": "beachevidentialtest",
          "provides": "beachevidentialtest.apply_test_logic",
          "role": "apply_test_logic_execution",
          "usage_count_in_300": 20,
          "questions": [
            "Q011",
            "Q027",
            "Q041",
            "Q057",
            "Q071",
            "Q087",
            "Q101",
            "Q117",
            "Q131",
            "Q147",
            "Q161",
            "Q177",
            "Q191",
            "Q207",
            "Q221",
            "Q237",
            "Q251",
            "Q267",
            "Q281",
            "Q297"
          ],
          "base_slots": [
            "D3-Q1",
            "D6-Q2"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM03",
            "DIM06"
          ],
          "signature": {
            "line_number": 147,
            "is_async": false,
            "parameters": [
              {
                "name": "test_type",
                "type": "TestType"
              },
              {
                "name": "evidence_found",
                "type": "bool"
              },
              {
                "name": "prior",
                "type": "float"
              },
              {
                "name": "bayes_factor",
                "type": "float"
              }
            ],
            "return_type": "tuple[float, str]",
            "docstring": "\n        Apply Beach test-specific logic to Bayesian updating.\n\n        CRITICAL RULES:\n        1. Hoop Test FAIL → posterior ≈ 0 (knock-out)\n        2. Smoking Gun PASS → multiply prior by large BF (>10)\n        3. Doubly Decisive → extreme updates (BF > 100 or < 0.01)\n\n        Returns: (posterior_confidence, interpretation)\n        "
          },
          "epistemological_foundation": {
            "paradigm": "BeachEvidentialTest analytical paradigm",
            "ontological_basis": "Analysis via BeachEvidentialTest.apply_test_logic",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method apply_test_logic implements structured analysis for D3-Q1"
            ],
            "justification": "This method contributes to D3-Q1 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "BeachEvidentialTest.apply_test_logic algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute apply_test_logic"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from apply_test_logic"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use apply_test_logic results for downstream analysis"
            ]
          }
        }
      }
    },
    "SemanticAnalyzer": {
      "file_path": "src/methods_dispensary/analyzer_one.py",
      "line_number": 1041,
      "class_docstring": "Advanced semantic analysis for municipal documents.",
      "total_usage_in_300_contracts": 90,
      "unique_questions": 30,
      "questions": [
        "Q008",
        "Q013",
        "Q023",
        "Q038",
        "Q043",
        "Q053",
        "Q068",
        "Q073",
        "Q083",
        "Q098",
        "Q103",
        "Q113",
        "Q128",
        "Q133",
        "Q143",
        "Q158",
        "Q163",
        "Q173",
        "Q188",
        "Q193",
        "Q203",
        "Q218",
        "Q223",
        "Q233",
        "Q248",
        "Q253",
        "Q263",
        "Q278",
        "Q283",
        "Q293"
      ],
      "base_slots": [
        "D2-Q3",
        "D3-Q3",
        "D5-Q3"
      ],
      "policy_areas": [
        "PA01",
        "PA02",
        "PA03",
        "PA04",
        "PA05",
        "PA06",
        "PA07",
        "PA08",
        "PA09",
        "PA10"
      ],
      "dimensions": [
        "DIM02",
        "DIM03",
        "DIM05"
      ],
      "methods": {
        "extract_semantic_cube": {
          "canonical_abbreviation": "semanticanalyzer",
          "provides": "semanticanalyzer.extract_semantic_cube",
          "role": "extract_semantic_cube_extraction",
          "usage_count_in_300": 20,
          "questions": [
            "Q008",
            "Q023",
            "Q038",
            "Q053",
            "Q068",
            "Q083",
            "Q098",
            "Q113",
            "Q128",
            "Q143",
            "Q158",
            "Q173",
            "Q188",
            "Q203",
            "Q218",
            "Q233",
            "Q248",
            "Q263",
            "Q278",
            "Q293"
          ],
          "base_slots": [
            "D2-Q3",
            "D5-Q3"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM02",
            "DIM05"
          ],
          "signature": {
            "line_number": 1097,
            "is_async": false,
            "parameters": [
              {
                "name": "document_segments",
                "type": "list[str]"
              }
            ],
            "return_type": "dict[str, Any]",
            "docstring": "Extract multidimensional semantic cube from document segments."
          },
          "epistemological_foundation": {
            "paradigm": "SemanticAnalyzer analytical paradigm",
            "ontological_basis": "Analysis via SemanticAnalyzer.extract_semantic_cube",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method extract_semantic_cube implements structured analysis for D2-Q3"
            ],
            "justification": "This method contributes to D2-Q3 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "SemanticAnalyzer.extract_semantic_cube algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute extract_semantic_cube"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from extract_semantic_cube"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use extract_semantic_cube results for downstream analysis"
            ]
          }
        },
        "_empty_semantic_cube": {
          "canonical_abbreviation": "semanticanalyzer",
          "provides": "semanticanalyzer.empty_semantic_cube",
          "role": "_empty_semantic_cube_execution",
          "usage_count_in_300": 10,
          "questions": [
            "Q013",
            "Q043",
            "Q073",
            "Q103",
            "Q133",
            "Q163",
            "Q193",
            "Q223",
            "Q253",
            "Q283"
          ],
          "base_slots": [
            "D3-Q3"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM03"
          ],
          "signature": {
            "line_number": 1170,
            "is_async": false,
            "parameters": [],
            "return_type": "dict[str, Any]",
            "docstring": "Return empty semantic cube structure."
          },
          "epistemological_foundation": {
            "paradigm": "SemanticAnalyzer analytical paradigm",
            "ontological_basis": "Analysis via SemanticAnalyzer._empty_semantic_cube",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _empty_semantic_cube implements structured analysis for D3-Q3"
            ],
            "justification": "This method contributes to D3-Q3 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "SemanticAnalyzer._empty_semantic_cube algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _empty_semantic_cube"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _empty_semantic_cube"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _empty_semantic_cube results for downstream analysis"
            ]
          }
        },
        "_classify_policy_domain": {
          "canonical_abbreviation": "semanticanalyzer",
          "provides": "semanticanalyzer.classify_policy_domain",
          "role": "_classify_policy_domain_execution",
          "usage_count_in_300": 10,
          "questions": [
            "Q013",
            "Q043",
            "Q073",
            "Q103",
            "Q133",
            "Q163",
            "Q193",
            "Q223",
            "Q253",
            "Q283"
          ],
          "base_slots": [
            "D3-Q3"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM03"
          ],
          "signature": {
            "line_number": 1317,
            "is_async": false,
            "parameters": [
              {
                "name": "segment",
                "type": "str"
              }
            ],
            "return_type": "dict[str, float]",
            "docstring": "\n        Classify segment by Policy Area (PA01-PA10) using keyword matching.\n        \n        Refactored per questionnaire_monolith.json canonical notation.\n        \n        Args:\n            segment: Text segment to classify\n            \n        Returns:\n            dict[str, float]: Score per Policy Area.\n                Keys: PA01, PA02, PA03, PA04, PA05, PA06, PA07, PA08, PA09, PA10\n                Values: Normalized score [0.0-1.0] based on keyword matches\n                \n        Contract:"
          },
          "epistemological_foundation": {
            "paradigm": "SemanticAnalyzer analytical paradigm",
            "ontological_basis": "Analysis via SemanticAnalyzer._classify_policy_domain",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _classify_policy_domain implements structured analysis for D3-Q3"
            ],
            "justification": "This method contributes to D3-Q3 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "SemanticAnalyzer._classify_policy_domain algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _classify_policy_domain"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _classify_policy_domain"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _classify_policy_domain results for downstream analysis"
            ]
          }
        },
        "_classify_cross_cutting_themes": {
          "canonical_abbreviation": "semanticanalyzer",
          "provides": "semanticanalyzer.classify_cross_cutting_themes",
          "role": "_classify_cross_cutting_themes_execution",
          "usage_count_in_300": 10,
          "questions": [
            "Q013",
            "Q043",
            "Q073",
            "Q103",
            "Q133",
            "Q163",
            "Q193",
            "Q223",
            "Q253",
            "Q283"
          ],
          "base_slots": [
            "D3-Q3"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM03"
          ],
          "signature": {
            "line_number": 1365,
            "is_async": false,
            "parameters": [
              {
                "name": "segment",
                "type": "str"
              }
            ],
            "return_type": "dict[str, float]",
            "docstring": "\n        Classify segment by Cross-Cutting Themes / Enfoques Transversales (ET01-ET10).\n        \n        Refactored per Colombian PDT normative framework (Ley 152/1994, DNP guidelines).\n        \n        Args:\n            segment: Text segment to classify\n            \n        Returns:\n            dict[str, float]: Score per Enfoque Transversal.\n                Keys: ET01, ET02, ET03, ET04, ET05, ET06, ET07, ET08, ET09, ET10\n                Values: Normalized score [0.0-1.0] based on keyword match"
          },
          "epistemological_foundation": {
            "paradigm": "SemanticAnalyzer analytical paradigm",
            "ontological_basis": "Analysis via SemanticAnalyzer._classify_cross_cutting_themes",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _classify_cross_cutting_themes implements structured analysis for D3-Q3"
            ],
            "justification": "This method contributes to D3-Q3 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "SemanticAnalyzer._classify_cross_cutting_themes algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _classify_cross_cutting_themes"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _classify_cross_cutting_themes"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _classify_cross_cutting_themes results for downstream analysis"
            ]
          }
        },
        "_classify_value_chain_link": {
          "canonical_abbreviation": "semanticanalyzer",
          "provides": "semanticanalyzer.classify_value_chain_link",
          "role": "_classify_value_chain_link_execution",
          "usage_count_in_300": 10,
          "questions": [
            "Q013",
            "Q043",
            "Q073",
            "Q103",
            "Q133",
            "Q163",
            "Q193",
            "Q223",
            "Q253",
            "Q283"
          ],
          "base_slots": [
            "D3-Q3"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM03"
          ],
          "signature": {
            "line_number": 1246,
            "is_async": false,
            "parameters": [
              {
                "name": "segment",
                "type": "str"
              },
              {
                "name": "policy_area_id",
                "type": "str | None",
                "default": "None"
              }
            ],
            "return_type": "dict[str, float]",
            "docstring": "\n        Classify segment by value chain link using canonical patterns.\n        \n        Refactored for SLOT D3-Q3: Trazabilidad Presupuestal y Organizacional.\n        Maps to questions Q013, Q043, Q073, Q103, Q133, Q163, Q193, Q223, Q253, Q283\n        (one per Policy Area PA01-PA10).\n        \n        Args:\n            segment: Text segment to classify\n            policy_area_id: Canonical policy area code (PA01-PA10). If None, uses\n                           legacy ontology-based classification"
          },
          "epistemological_foundation": {
            "paradigm": "SemanticAnalyzer analytical paradigm",
            "ontological_basis": "Analysis via SemanticAnalyzer._classify_value_chain_link",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _classify_value_chain_link implements structured analysis for D3-Q3"
            ],
            "justification": "This method contributes to D3-Q3 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "SemanticAnalyzer._classify_value_chain_link algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _classify_value_chain_link"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _classify_value_chain_link"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _classify_value_chain_link results for downstream analysis"
            ]
          }
        },
        "_vectorize_segments": {
          "canonical_abbreviation": "semanticanalyzer",
          "provides": "semanticanalyzer.vectorize_segments",
          "role": "_vectorize_segments_execution",
          "usage_count_in_300": 10,
          "questions": [
            "Q013",
            "Q043",
            "Q073",
            "Q103",
            "Q133",
            "Q163",
            "Q193",
            "Q223",
            "Q253",
            "Q283"
          ],
          "base_slots": [
            "D3-Q3"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM03"
          ],
          "signature": {
            "line_number": 1192,
            "is_async": false,
            "parameters": [
              {
                "name": "segments",
                "type": "list[str]"
              }
            ],
            "return_type": "np.ndarray",
            "docstring": "Vectorize document segments using TF-IDF."
          },
          "epistemological_foundation": {
            "paradigm": "SemanticAnalyzer analytical paradigm",
            "ontological_basis": "Analysis via SemanticAnalyzer._vectorize_segments",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _vectorize_segments implements structured analysis for D3-Q3"
            ],
            "justification": "This method contributes to D3-Q3 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "SemanticAnalyzer._vectorize_segments algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _vectorize_segments"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _vectorize_segments"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _vectorize_segments results for downstream analysis"
            ]
          }
        },
        "_process_segment": {
          "canonical_abbreviation": "semanticanalyzer",
          "provides": "semanticanalyzer.process_segment",
          "role": "_process_segment_processing",
          "usage_count_in_300": 10,
          "questions": [
            "Q013",
            "Q043",
            "Q073",
            "Q103",
            "Q133",
            "Q163",
            "Q193",
            "Q223",
            "Q253",
            "Q283"
          ],
          "base_slots": [
            "D3-Q3"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM03"
          ],
          "signature": {
            "line_number": 1208,
            "is_async": false,
            "parameters": [
              {
                "name": "segment",
                "type": "str"
              },
              {
                "name": "idx",
                "type": "int"
              },
              {
                "name": "vector"
              }
            ],
            "return_type": "dict[str, Any]",
            "docstring": "Process individual segment and extract features."
          },
          "epistemological_foundation": {
            "paradigm": "SemanticAnalyzer analytical paradigm",
            "ontological_basis": "Analysis via SemanticAnalyzer._process_segment",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _process_segment implements structured analysis for D3-Q3"
            ],
            "justification": "This method contributes to D3-Q3 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "SemanticAnalyzer._process_segment algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _process_segment"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _process_segment"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _process_segment results for downstream analysis"
            ]
          }
        },
        "_calculate_semantic_complexity": {
          "canonical_abbreviation": "semanticanalyzer",
          "provides": "semanticanalyzer.calculate_semantic_complexity",
          "role": "_calculate_semantic_complexity_calculation",
          "usage_count_in_300": 10,
          "questions": [
            "Q013",
            "Q043",
            "Q073",
            "Q103",
            "Q133",
            "Q163",
            "Q193",
            "Q223",
            "Q253",
            "Q283"
          ],
          "base_slots": [
            "D3-Q3"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM03"
          ],
          "signature": {
            "line_number": 1412,
            "is_async": false,
            "parameters": [
              {
                "name": "semantic_cube",
                "type": "dict[str, Any]"
              }
            ],
            "return_type": "float",
            "docstring": "Calculate semantic complexity of the cube."
          },
          "epistemological_foundation": {
            "paradigm": "SemanticAnalyzer analytical paradigm",
            "ontological_basis": "Analysis via SemanticAnalyzer._calculate_semantic_complexity",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _calculate_semantic_complexity implements structured analysis for D3-Q3"
            ],
            "justification": "This method contributes to D3-Q3 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "SemanticAnalyzer._calculate_semantic_complexity algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _calculate_semantic_complexity"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _calculate_semantic_complexity"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _calculate_semantic_complexity results for downstream analysis"
            ]
          }
        }
      }
    },
    "AdaptivePriorCalculator": {
      "file_path": "src/methods_dispensary/derek_beach.py",
      "line_number": 4788,
      "class_docstring": "\n    AGUJA I - Prior Adaptativo con Bayes Factor y calibración\n\n    PROMPT I-1: Ponderación evidencial con BF y calibración\n    Mapea test_type→BayesFactor, calcula likelihood adaptativo combinando\n    dominios {semantic, temporal, financial, structural} con pesos normalizados.\n\n    PROMPT I-2: Sensibilidad, OOD y ablation evidencial\n    Perturba cada componente ±10% y reporta ∂p/∂component top-3.\n\n    PROMPT I-3: Trazabilidad y reproducibilidad\n    Con semilla fija, guarda bf_table_version, wei",
      "total_usage_in_300_contracts": 140,
      "unique_questions": 120,
      "questions": [
        "Q009",
        "Q010",
        "Q011",
        "Q012",
        "Q013",
        "Q018",
        "Q020",
        "Q023",
        "Q024",
        "Q025",
        "Q028",
        "Q030",
        "Q039",
        "Q040",
        "Q041",
        "Q042",
        "Q043",
        "Q048",
        "Q050",
        "Q053",
        "Q054",
        "Q055",
        "Q058",
        "Q060",
        "Q069",
        "Q070",
        "Q071",
        "Q072",
        "Q073",
        "Q078",
        "Q080",
        "Q083",
        "Q084",
        "Q085",
        "Q088",
        "Q090",
        "Q099",
        "Q100",
        "Q101",
        "Q102",
        "Q103",
        "Q108",
        "Q110",
        "Q113",
        "Q114",
        "Q115",
        "Q118",
        "Q120",
        "Q129",
        "Q130",
        "Q131",
        "Q132",
        "Q133",
        "Q138",
        "Q140",
        "Q143",
        "Q144",
        "Q145",
        "Q148",
        "Q150",
        "Q159",
        "Q160",
        "Q161",
        "Q162",
        "Q163",
        "Q168",
        "Q170",
        "Q173",
        "Q174",
        "Q175",
        "Q178",
        "Q180",
        "Q189",
        "Q190",
        "Q191",
        "Q192",
        "Q193",
        "Q198",
        "Q200",
        "Q203",
        "Q204",
        "Q205",
        "Q208",
        "Q210",
        "Q219",
        "Q220",
        "Q221",
        "Q222",
        "Q223",
        "Q228",
        "Q230",
        "Q233",
        "Q234",
        "Q235",
        "Q238",
        "Q240",
        "Q249",
        "Q250",
        "Q251",
        "Q252",
        "Q253",
        "Q258",
        "Q260",
        "Q263",
        "Q264",
        "Q265",
        "Q268",
        "Q270",
        "Q279",
        "Q280",
        "Q281",
        "Q282",
        "Q283",
        "Q288",
        "Q290",
        "Q293",
        "Q294",
        "Q295",
        "Q298",
        "Q300"
      ],
      "base_slots": [
        "D2-Q4",
        "D2-Q5",
        "D3-Q1",
        "D3-Q2",
        "D3-Q3",
        "D4-Q3",
        "D4-Q5",
        "D5-Q3",
        "D5-Q4",
        "D5-Q5",
        "D6-Q3",
        "D6-Q5"
      ],
      "policy_areas": [
        "PA01",
        "PA02",
        "PA03",
        "PA04",
        "PA05",
        "PA06",
        "PA07",
        "PA08",
        "PA09",
        "PA10"
      ],
      "dimensions": [
        "DIM02",
        "DIM03",
        "DIM04",
        "DIM05",
        "DIM06"
      ],
      "methods": {
        "sensitivity_analysis": {
          "canonical_abbreviation": "adaptivepriorcalculator",
          "provides": "adaptivepriorcalculator.sensitivity_analysis",
          "role": "sensitivity_analysis_execution",
          "usage_count_in_300": 30,
          "questions": [
            "Q009",
            "Q018",
            "Q024",
            "Q039",
            "Q048",
            "Q054",
            "Q069",
            "Q078",
            "Q084",
            "Q099",
            "Q108",
            "Q114",
            "Q129",
            "Q138",
            "Q144",
            "Q159",
            "Q168",
            "Q174",
            "Q189",
            "Q198",
            "Q204",
            "Q219",
            "Q228",
            "Q234",
            "Q249",
            "Q258",
            "Q264",
            "Q279",
            "Q288",
            "Q294"
          ],
          "base_slots": [
            "D2-Q4",
            "D4-Q3",
            "D5-Q4"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM02",
            "DIM04",
            "DIM05"
          ],
          "signature": {
            "line_number": 4922,
            "is_async": false,
            "parameters": [
              {
                "name": "evidence_dict",
                "type": "dict[str, Any]"
              },
              {
                "name": "test_type",
                "type": "str",
                "default": "'hoop'"
              },
              {
                "name": "perturbation",
                "type": "float",
                "default": "0.1"
              }
            ],
            "return_type": "dict[str, Any]",
            "docstring": "\n        PROMPT I-2: Sensibilidad, OOD y ablation evidencial\n\n        Perturba cada componente ±10% y reporta ∂p/∂component top-3.\n        Ejecuta ablaciones: sólo textual, sólo financiero, sólo estructural.\n\n        CRITERIA:\n        - |delta_p_sensitivity|_max ≤ 0.15)\n        - sign_concordance ≥ 2/3\n        - OOD_drop ≤ 0.10)\n        "
          },
          "epistemological_foundation": {
            "paradigm": "AdaptivePriorCalculator analytical paradigm",
            "ontological_basis": "Analysis via AdaptivePriorCalculator.sensitivity_analysis",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method sensitivity_analysis implements structured analysis for D2-Q4"
            ],
            "justification": "This method contributes to D2-Q4 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "AdaptivePriorCalculator.sensitivity_analysis algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute sensitivity_analysis"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from sensitivity_analysis"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use sensitivity_analysis results for downstream analysis"
            ]
          }
        },
        "calculate_likelihood_adaptativo": {
          "canonical_abbreviation": "adaptivepriorcalculator",
          "provides": "adaptivepriorcalculator.calculate_likelihood_adaptativo",
          "role": "calculate_likelihood_adaptativo_calculation",
          "usage_count_in_300": 20,
          "questions": [
            "Q010",
            "Q012",
            "Q040",
            "Q042",
            "Q070",
            "Q072",
            "Q100",
            "Q102",
            "Q130",
            "Q132",
            "Q160",
            "Q162",
            "Q190",
            "Q192",
            "Q220",
            "Q222",
            "Q250",
            "Q252",
            "Q280",
            "Q282"
          ],
          "base_slots": [
            "D2-Q5",
            "D3-Q2"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM02",
            "DIM03"
          ],
          "signature": {
            "line_number": 4827,
            "is_async": false,
            "parameters": [
              {
                "name": "evidence_dict",
                "type": "dict[str, Any]"
              },
              {
                "name": "test_type",
                "type": "str",
                "default": "'hoop'"
              }
            ],
            "return_type": "dict[str, Any]",
            "docstring": "\n        PROMPT I-1: Calcula likelihood adaptativo con BF y dominios\n\n        Args:\n            evidence_dict: Evidencia por caso {semantic, temporal, financial, structural}\n            test_type: Tipo de test evidencial (straw, hoop, smoking, doubly)\n\n        Returns:\n            Dict con p_mechanism, BF_used, domain_weights, triangulation_bonus, etc.\n        "
          },
          "epistemological_foundation": {
            "paradigm": "AdaptivePriorCalculator analytical paradigm",
            "ontological_basis": "Analysis via AdaptivePriorCalculator.calculate_likelihood_adaptativo",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method calculate_likelihood_adaptativo implements structured analysis for D2-Q5"
            ],
            "justification": "This method contributes to D2-Q5 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "AdaptivePriorCalculator.calculate_likelihood_adaptativo algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute calculate_likelihood_adaptativo"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from calculate_likelihood_adaptativo"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use calculate_likelihood_adaptativo results for downstream analysis"
            ]
          }
        },
        "generate_traceability_record": {
          "canonical_abbreviation": "adaptivepriorcalculator",
          "provides": "adaptivepriorcalculator.generate_traceability_record",
          "role": "generate_traceability_record_execution",
          "usage_count_in_300": 30,
          "questions": [
            "Q011",
            "Q013",
            "Q030",
            "Q041",
            "Q043",
            "Q060",
            "Q071",
            "Q073",
            "Q090",
            "Q101",
            "Q103",
            "Q120",
            "Q131",
            "Q133",
            "Q150",
            "Q161",
            "Q163",
            "Q180",
            "Q191",
            "Q193",
            "Q210",
            "Q221",
            "Q223",
            "Q240",
            "Q251",
            "Q253",
            "Q270",
            "Q281",
            "Q283",
            "Q300"
          ],
          "base_slots": [
            "D3-Q1",
            "D3-Q3",
            "D6-Q5"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM03",
            "DIM06"
          ],
          "signature": {
            "line_number": 5037,
            "is_async": false,
            "parameters": [
              {
                "name": "evidence_dict",
                "type": "dict[str, Any]"
              },
              {
                "name": "test_type",
                "type": "str"
              },
              {
                "name": "result",
                "type": "dict[str, Any]"
              },
              {
                "name": "seed",
                "type": "int",
                "default": "42"
              }
            ],
            "return_type": "dict[str, Any]",
            "docstring": "\n        PROMPT I-3: Trazabilidad y reproducibilidad\n\n        Con semilla fija, guarda bf_table_version, weights_version,\n        snippets textuales con offsets, campos financieros usados.\n\n        METRICS:\n        - Re-ejecución con misma semilla produce hash_result idéntico\n        - trace_completeness ≥ 0.95)\n        "
          },
          "epistemological_foundation": {
            "paradigm": "AdaptivePriorCalculator analytical paradigm",
            "ontological_basis": "Analysis via AdaptivePriorCalculator.generate_traceability_record",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method generate_traceability_record implements structured analysis for D3-Q1"
            ],
            "justification": "This method contributes to D3-Q1 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "AdaptivePriorCalculator.generate_traceability_record algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute generate_traceability_record"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from generate_traceability_record"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use generate_traceability_record results for downstream analysis"
            ]
          }
        },
        "_adjust_domain_weights": {
          "canonical_abbreviation": "adaptivepriorcalculator",
          "provides": "adaptivepriorcalculator.adjust_domain_weights",
          "role": "_adjust_domain_weights_execution",
          "usage_count_in_300": 10,
          "questions": [
            "Q012",
            "Q042",
            "Q072",
            "Q102",
            "Q132",
            "Q162",
            "Q192",
            "Q222",
            "Q252",
            "Q282"
          ],
          "base_slots": [
            "D3-Q2"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM03"
          ],
          "signature": {
            "line_number": 4895,
            "is_async": false,
            "parameters": [
              {
                "name": "domain_scores",
                "type": "dict[str, float]"
              }
            ],
            "return_type": "dict[str, float]",
            "docstring": "Ajusta pesos si falta dominio: baja a 0 y reparte"
          },
          "epistemological_foundation": {
            "paradigm": "AdaptivePriorCalculator analytical paradigm",
            "ontological_basis": "Analysis via AdaptivePriorCalculator._adjust_domain_weights",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _adjust_domain_weights implements structured analysis for D3-Q2"
            ],
            "justification": "This method contributes to D3-Q2 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "AdaptivePriorCalculator._adjust_domain_weights algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _adjust_domain_weights"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _adjust_domain_weights"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _adjust_domain_weights results for downstream analysis"
            ]
          }
        },
        "validate_quality_criteria": {
          "canonical_abbreviation": "adaptivepriorcalculator",
          "provides": "adaptivepriorcalculator.validate_quality_criteria",
          "role": "validate_quality_criteria_validation",
          "usage_count_in_300": 30,
          "questions": [
            "Q020",
            "Q025",
            "Q028",
            "Q050",
            "Q055",
            "Q058",
            "Q080",
            "Q085",
            "Q088",
            "Q110",
            "Q115",
            "Q118",
            "Q140",
            "Q145",
            "Q148",
            "Q170",
            "Q175",
            "Q178",
            "Q200",
            "Q205",
            "Q208",
            "Q230",
            "Q235",
            "Q238",
            "Q260",
            "Q265",
            "Q268",
            "Q290",
            "Q295",
            "Q298"
          ],
          "base_slots": [
            "D4-Q5",
            "D5-Q5",
            "D6-Q3"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM04",
            "DIM05",
            "DIM06"
          ],
          "signature": {
            "line_number": 5102,
            "is_async": false,
            "parameters": [
              {
                "name": "validation_samples",
                "type": "list[dict[str, Any]]"
              }
            ],
            "return_type": "dict[str, Any]",
            "docstring": "\n        Valida criterios de calidad en conjunto de validación sintética\n\n        QUALITY CRITERIA:\n        - BrierScore ≤ 0.20)\n        - ACE ∈ [−0.02), 0.02)]\n        - Cobertura CI95% ∈ [92%, 98%]\n        - Monotonicidad verificada\n        "
          },
          "epistemological_foundation": {
            "paradigm": "AdaptivePriorCalculator analytical paradigm",
            "ontological_basis": "Analysis via AdaptivePriorCalculator.validate_quality_criteria",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method validate_quality_criteria implements structured analysis for D4-Q5"
            ],
            "justification": "This method contributes to D4-Q5 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "AdaptivePriorCalculator.validate_quality_criteria algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute validate_quality_criteria"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from validate_quality_criteria"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use validate_quality_criteria results for downstream analysis"
            ]
          }
        },
        "_perturb_evidence": {
          "canonical_abbreviation": "adaptivepriorcalculator",
          "provides": "adaptivepriorcalculator.perturb_evidence",
          "role": "_perturb_evidence_execution",
          "usage_count_in_300": 10,
          "questions": [
            "Q023",
            "Q053",
            "Q083",
            "Q113",
            "Q143",
            "Q173",
            "Q203",
            "Q233",
            "Q263",
            "Q293"
          ],
          "base_slots": [
            "D5-Q3"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM05"
          ],
          "signature": {
            "line_number": 5009,
            "is_async": false,
            "parameters": [
              {
                "name": "evidence_dict",
                "type": "dict[str, Any]"
              },
              {
                "name": "domain",
                "type": "str"
              },
              {
                "name": "perturbation",
                "type": "float"
              }
            ],
            "return_type": "dict[str, Any]",
            "docstring": "Perturba un dominio específico"
          },
          "epistemological_foundation": {
            "paradigm": "AdaptivePriorCalculator analytical paradigm",
            "ontological_basis": "Analysis via AdaptivePriorCalculator._perturb_evidence",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _perturb_evidence implements structured analysis for D5-Q3"
            ],
            "justification": "This method contributes to D5-Q3 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "AdaptivePriorCalculator._perturb_evidence algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _perturb_evidence"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _perturb_evidence"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _perturb_evidence results for downstream analysis"
            ]
          }
        },
        "_add_ood_noise": {
          "canonical_abbreviation": "adaptivepriorcalculator",
          "provides": "adaptivepriorcalculator.add_ood_noise",
          "role": "_add_ood_noise_execution",
          "usage_count_in_300": 10,
          "questions": [
            "Q025",
            "Q055",
            "Q085",
            "Q115",
            "Q145",
            "Q175",
            "Q205",
            "Q235",
            "Q265",
            "Q295"
          ],
          "base_slots": [
            "D5-Q5"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM05"
          ],
          "signature": {
            "line_number": 5024,
            "is_async": false,
            "parameters": [
              {
                "name": "evidence_dict",
                "type": "dict[str, Any]"
              }
            ],
            "return_type": "dict[str, Any]",
            "docstring": "Genera set OOD con ruido semántico y tablas malformadas"
          },
          "epistemological_foundation": {
            "paradigm": "AdaptivePriorCalculator analytical paradigm",
            "ontological_basis": "Analysis via AdaptivePriorCalculator._add_ood_noise",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _add_ood_noise implements structured analysis for D5-Q5"
            ],
            "justification": "This method contributes to D5-Q5 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "AdaptivePriorCalculator._add_ood_noise algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _add_ood_noise"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _add_ood_noise"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _add_ood_noise results for downstream analysis"
            ]
          }
        }
      }
    },
    "AdvancedDAGValidator": {
      "file_path": "src/methods_dispensary/teoria_cambio.py",
      "line_number": 572,
      "class_docstring": "\n    Motor para la validación estocástica y análisis de sensibilidad de DAGs.\n    Utiliza simulaciones Monte Carlo para cuantificar la robustez y aciclicidad\n    de modelos causales complejos.\n    ",
      "total_usage_in_300_contracts": 190,
      "unique_questions": 70,
      "questions": [
        "Q012",
        "Q014",
        "Q017",
        "Q018",
        "Q026",
        "Q028",
        "Q029",
        "Q042",
        "Q044",
        "Q047",
        "Q048",
        "Q056",
        "Q058",
        "Q059",
        "Q072",
        "Q074",
        "Q077",
        "Q078",
        "Q086",
        "Q088",
        "Q089",
        "Q102",
        "Q104",
        "Q107",
        "Q108",
        "Q116",
        "Q118",
        "Q119",
        "Q132",
        "Q134",
        "Q137",
        "Q138",
        "Q146",
        "Q148",
        "Q149",
        "Q162",
        "Q164",
        "Q167",
        "Q168",
        "Q176",
        "Q178",
        "Q179",
        "Q192",
        "Q194",
        "Q197",
        "Q198",
        "Q206",
        "Q208",
        "Q209",
        "Q222",
        "Q224",
        "Q227",
        "Q228",
        "Q236",
        "Q238",
        "Q239",
        "Q252",
        "Q254",
        "Q257",
        "Q258",
        "Q266",
        "Q268",
        "Q269",
        "Q282",
        "Q284",
        "Q287",
        "Q288",
        "Q296",
        "Q298",
        "Q299"
      ],
      "base_slots": [
        "D3-Q2",
        "D3-Q4",
        "D4-Q2",
        "D4-Q3",
        "D6-Q1",
        "D6-Q3",
        "D6-Q4"
      ],
      "policy_areas": [
        "PA01",
        "PA02",
        "PA03",
        "PA04",
        "PA05",
        "PA06",
        "PA07",
        "PA08",
        "PA09",
        "PA10"
      ],
      "dimensions": [
        "DIM03",
        "DIM04",
        "DIM06"
      ],
      "methods": {
        "_calculate_bayesian_posterior": {
          "canonical_abbreviation": "advanceddagvalidator",
          "provides": "advanceddagvalidator.calculate_bayesian_posterior",
          "role": "_calculate_bayesian_posterior_calculation",
          "usage_count_in_300": 10,
          "questions": [
            "Q012",
            "Q042",
            "Q072",
            "Q102",
            "Q132",
            "Q162",
            "Q192",
            "Q222",
            "Q252",
            "Q282"
          ],
          "base_slots": [
            "D3-Q2"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM03"
          ],
          "signature": {
            "line_number": 913,
            "is_async": false,
            "parameters": [
              {
                "name": "likelihood",
                "type": "float"
              },
              {
                "name": "prior",
                "type": "float",
                "default": "0.5"
              }
            ],
            "return_type": "float",
            "docstring": "Calcula la probabilidad posterior Bayesiana simple."
          },
          "epistemological_foundation": {
            "paradigm": "AdvancedDAGValidator analytical paradigm",
            "ontological_basis": "Analysis via AdvancedDAGValidator._calculate_bayesian_posterior",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _calculate_bayesian_posterior implements structured analysis for D3-Q2"
            ],
            "justification": "This method contributes to D3-Q2 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "AdvancedDAGValidator._calculate_bayesian_posterior algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _calculate_bayesian_posterior"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _calculate_bayesian_posterior"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _calculate_bayesian_posterior results for downstream analysis"
            ]
          }
        },
        "_calculate_confidence_interval": {
          "canonical_abbreviation": "advanceddagvalidator",
          "provides": "advanceddagvalidator.calculate_confidence_interval",
          "role": "_calculate_confidence_interval_calculation",
          "usage_count_in_300": 10,
          "questions": [
            "Q012",
            "Q042",
            "Q072",
            "Q102",
            "Q132",
            "Q162",
            "Q192",
            "Q222",
            "Q252",
            "Q282"
          ],
          "base_slots": [
            "D3-Q2"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM03"
          ],
          "signature": {
            "line_number": 874,
            "is_async": false,
            "parameters": [
              {
                "name": "s",
                "type": "int"
              },
              {
                "name": "n",
                "type": "int"
              },
              {
                "name": "conf",
                "type": "float"
              }
            ],
            "return_type": "tuple[float, float]",
            "docstring": "Calcula el intervalo de confianza de Wilson."
          },
          "epistemological_foundation": {
            "paradigm": "AdvancedDAGValidator analytical paradigm",
            "ontological_basis": "Analysis via AdvancedDAGValidator._calculate_confidence_interval",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _calculate_confidence_interval implements structured analysis for D3-Q2"
            ],
            "justification": "This method contributes to D3-Q2 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "AdvancedDAGValidator._calculate_confidence_interval algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _calculate_confidence_interval"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _calculate_confidence_interval"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _calculate_confidence_interval results for downstream analysis"
            ]
          }
        },
        "calculate_acyclicity_pvalue": {
          "canonical_abbreviation": "advanceddagvalidator",
          "provides": "advanceddagvalidator.calculate_acyclicity_pvalue",
          "role": "calculate_acyclicity_pvalue_calculation",
          "usage_count_in_300": 20,
          "questions": [
            "Q014",
            "Q028",
            "Q044",
            "Q058",
            "Q074",
            "Q088",
            "Q104",
            "Q118",
            "Q134",
            "Q148",
            "Q164",
            "Q178",
            "Q194",
            "Q208",
            "Q224",
            "Q238",
            "Q254",
            "Q268",
            "Q284",
            "Q298"
          ],
          "base_slots": [
            "D3-Q4",
            "D6-Q3"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM03",
            "DIM06"
          ],
          "signature": {
            "line_number": 694,
            "is_async": false,
            "parameters": [
              {
                "name": "plan_name",
                "type": "str"
              },
              {
                "name": "iterations",
                "type": "int"
              }
            ],
            "return_type": "MonteCarloAdvancedResult",
            "docstring": "Cálculo avanzado de p-value con un marco estadístico completo."
          },
          "epistemological_foundation": {
            "paradigm": "AdvancedDAGValidator analytical paradigm",
            "ontological_basis": "Analysis via AdvancedDAGValidator.calculate_acyclicity_pvalue",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method calculate_acyclicity_pvalue implements structured analysis for D3-Q4"
            ],
            "justification": "This method contributes to D3-Q4 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "AdvancedDAGValidator.calculate_acyclicity_pvalue algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute calculate_acyclicity_pvalue"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from calculate_acyclicity_pvalue"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use calculate_acyclicity_pvalue results for downstream analysis"
            ]
          }
        },
        "_is_acyclic": {
          "canonical_abbreviation": "advanceddagvalidator",
          "provides": "advanceddagvalidator.is_acyclic",
          "role": "_is_acyclic_execution",
          "usage_count_in_300": 10,
          "questions": [
            "Q014",
            "Q044",
            "Q074",
            "Q104",
            "Q134",
            "Q164",
            "Q194",
            "Q224",
            "Q254",
            "Q284"
          ],
          "base_slots": [
            "D3-Q4"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM03"
          ],
          "signature": {
            "line_number": 650,
            "is_async": false,
            "parameters": [
              {
                "name": "nodes",
                "type": "dict[str, AdvancedGraphNode]"
              }
            ],
            "return_type": "bool",
            "docstring": "Detección de ciclos mediante el algoritmo de Kahn (ordenación topológica)."
          },
          "epistemological_foundation": {
            "paradigm": "AdvancedDAGValidator analytical paradigm",
            "ontological_basis": "Analysis via AdvancedDAGValidator._is_acyclic",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _is_acyclic implements structured analysis for D3-Q4"
            ],
            "justification": "This method contributes to D3-Q4 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "AdvancedDAGValidator._is_acyclic algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _is_acyclic"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _is_acyclic"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _is_acyclic results for downstream analysis"
            ]
          }
        },
        "get_graph_stats": {
          "canonical_abbreviation": "advanceddagvalidator",
          "provides": "advanceddagvalidator.get_graph_stats",
          "role": "get_graph_stats_execution",
          "usage_count_in_300": 10,
          "questions": [
            "Q014",
            "Q044",
            "Q074",
            "Q104",
            "Q134",
            "Q164",
            "Q194",
            "Q224",
            "Q254",
            "Q284"
          ],
          "base_slots": [
            "D3-Q4"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM03"
          ],
          "signature": {
            "line_number": 947,
            "is_async": false,
            "parameters": [],
            "return_type": "dict[str, Any]",
            "docstring": "Obtiene estadísticas estructurales del grafo."
          },
          "epistemological_foundation": {
            "paradigm": "AdvancedDAGValidator analytical paradigm",
            "ontological_basis": "Analysis via AdvancedDAGValidator.get_graph_stats",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method get_graph_stats implements structured analysis for D3-Q4"
            ],
            "justification": "This method contributes to D3-Q4 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "AdvancedDAGValidator.get_graph_stats algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute get_graph_stats"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from get_graph_stats"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use get_graph_stats results for downstream analysis"
            ]
          }
        },
        "_calculate_node_importance": {
          "canonical_abbreviation": "advanceddagvalidator",
          "provides": "advanceddagvalidator.calculate_node_importance",
          "role": "_calculate_node_importance_calculation",
          "usage_count_in_300": 20,
          "questions": [
            "Q014",
            "Q029",
            "Q044",
            "Q059",
            "Q074",
            "Q089",
            "Q104",
            "Q119",
            "Q134",
            "Q149",
            "Q164",
            "Q179",
            "Q194",
            "Q209",
            "Q224",
            "Q239",
            "Q254",
            "Q269",
            "Q284",
            "Q299"
          ],
          "base_slots": [
            "D3-Q4",
            "D6-Q4"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM03",
            "DIM06"
          ],
          "signature": {
            "line_number": 925,
            "is_async": false,
            "parameters": [],
            "return_type": "dict[str, float]",
            "docstring": "Calcula una métrica de importancia para cada nodo."
          },
          "epistemological_foundation": {
            "paradigm": "AdvancedDAGValidator analytical paradigm",
            "ontological_basis": "Analysis via AdvancedDAGValidator._calculate_node_importance",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _calculate_node_importance implements structured analysis for D3-Q4"
            ],
            "justification": "This method contributes to D3-Q4 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "AdvancedDAGValidator._calculate_node_importance algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _calculate_node_importance"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _calculate_node_importance"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _calculate_node_importance results for downstream analysis"
            ]
          }
        },
        "_generate_subgraph": {
          "canonical_abbreviation": "advanceddagvalidator",
          "provides": "advanceddagvalidator.generate_subgraph",
          "role": "_generate_subgraph_execution",
          "usage_count_in_300": 10,
          "questions": [
            "Q014",
            "Q044",
            "Q074",
            "Q104",
            "Q134",
            "Q164",
            "Q194",
            "Q224",
            "Q254",
            "Q284"
          ],
          "base_slots": [
            "D3-Q4"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM03"
          ],
          "signature": {
            "line_number": 674,
            "is_async": false,
            "parameters": [],
            "return_type": "dict[str, AdvancedGraphNode]",
            "docstring": "Genera un subgrafo aleatorio del grafo principal."
          },
          "epistemological_foundation": {
            "paradigm": "AdvancedDAGValidator analytical paradigm",
            "ontological_basis": "Analysis via AdvancedDAGValidator._generate_subgraph",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _generate_subgraph implements structured analysis for D3-Q4"
            ],
            "justification": "This method contributes to D3-Q4 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "AdvancedDAGValidator._generate_subgraph algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _generate_subgraph"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _generate_subgraph"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _generate_subgraph results for downstream analysis"
            ]
          }
        },
        "add_node": {
          "canonical_abbreviation": "advanceddagvalidator",
          "provides": "advanceddagvalidator.add_node",
          "role": "add_node_execution",
          "usage_count_in_300": 10,
          "questions": [
            "Q014",
            "Q044",
            "Q074",
            "Q104",
            "Q134",
            "Q164",
            "Q194",
            "Q224",
            "Q254",
            "Q284"
          ],
          "base_slots": [
            "D3-Q4"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM03"
          ],
          "signature": {
            "line_number": 600,
            "is_async": false,
            "parameters": [
              {
                "name": "name",
                "type": "str"
              },
              {
                "name": "dependencies",
                "type": "set[str] | None",
                "default": "None"
              },
              {
                "name": "role",
                "type": "str",
                "default": "'variable'"
              },
              {
                "name": "metadata",
                "type": "dict[str, Any] | None",
                "default": "None"
              }
            ],
            "return_type": "None",
            "docstring": "Agrega un nodo enriquecido al grafo."
          },
          "epistemological_foundation": {
            "paradigm": "AdvancedDAGValidator analytical paradigm",
            "ontological_basis": "Analysis via AdvancedDAGValidator.add_node",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method add_node implements structured analysis for D3-Q4"
            ],
            "justification": "This method contributes to D3-Q4 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "AdvancedDAGValidator.add_node algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute add_node"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from add_node"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use add_node results for downstream analysis"
            ]
          }
        },
        "add_edge": {
          "canonical_abbreviation": "advanceddagvalidator",
          "provides": "advanceddagvalidator.add_edge",
          "role": "add_edge_execution",
          "usage_count_in_300": 10,
          "questions": [
            "Q014",
            "Q044",
            "Q074",
            "Q104",
            "Q134",
            "Q164",
            "Q194",
            "Q224",
            "Q254",
            "Q284"
          ],
          "base_slots": [
            "D3-Q4"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM03"
          ],
          "signature": {
            "line_number": 613,
            "is_async": false,
            "parameters": [
              {
                "name": "from_node",
                "type": "str"
              },
              {
                "name": "to_node",
                "type": "str"
              },
              {
                "name": "weight",
                "type": "float",
                "default": "1.0"
              }
            ],
            "return_type": "None",
            "docstring": "Agrega una arista dirigida con peso opcional."
          },
          "epistemological_foundation": {
            "paradigm": "AdvancedDAGValidator analytical paradigm",
            "ontological_basis": "Analysis via AdvancedDAGValidator.add_edge",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method add_edge implements structured analysis for D3-Q4"
            ],
            "justification": "This method contributes to D3-Q4 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "AdvancedDAGValidator.add_edge algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute add_edge"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from add_edge"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use add_edge results for downstream analysis"
            ]
          }
        },
        "export_nodes": {
          "canonical_abbreviation": "advanceddagvalidator",
          "provides": "advanceddagvalidator.export_nodes",
          "role": "export_nodes_execution",
          "usage_count_in_300": 20,
          "questions": [
            "Q014",
            "Q026",
            "Q044",
            "Q056",
            "Q074",
            "Q086",
            "Q104",
            "Q116",
            "Q134",
            "Q146",
            "Q164",
            "Q176",
            "Q194",
            "Q206",
            "Q224",
            "Q236",
            "Q254",
            "Q266",
            "Q284",
            "Q296"
          ],
          "base_slots": [
            "D3-Q4",
            "D6-Q1"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM03",
            "DIM06"
          ],
          "signature": {
            "line_number": 763,
            "is_async": false,
            "parameters": [
              {
                "name": "validate",
                "type": "bool",
                "default": "False"
              },
              {
                "name": "schema_path",
                "type": "Path | None",
                "default": "None"
              }
            ],
            "return_type": "list[dict[str, Any]]",
            "docstring": "Serializa los nodos del grafo y opcionalmente valida contra JSON Schema."
          },
          "epistemological_foundation": {
            "paradigm": "AdvancedDAGValidator analytical paradigm",
            "ontological_basis": "Analysis via AdvancedDAGValidator.export_nodes",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method export_nodes implements structured analysis for D3-Q4"
            ],
            "justification": "This method contributes to D3-Q4 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "AdvancedDAGValidator.export_nodes algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute export_nodes"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from export_nodes"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use export_nodes results for downstream analysis"
            ]
          }
        },
        "_initialize_rng": {
          "canonical_abbreviation": "advanceddagvalidator",
          "provides": "advanceddagvalidator.initialize_rng",
          "role": "_initialize_rng_execution",
          "usage_count_in_300": 10,
          "questions": [
            "Q014",
            "Q044",
            "Q074",
            "Q104",
            "Q134",
            "Q164",
            "Q194",
            "Q224",
            "Q254",
            "Q284"
          ],
          "base_slots": [
            "D3-Q4"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM03"
          ],
          "signature": {
            "line_number": 623,
            "is_async": false,
            "parameters": [
              {
                "name": "plan_name",
                "type": "str"
              },
              {
                "name": "salt",
                "type": "str",
                "default": "''"
              }
            ],
            "return_type": "int",
            "docstring": "\n        Inicializa el generador de números aleatorios con una semilla determinista.\n\n        Audit Point 1.1: Deterministic Seeding (RNG)\n        Initializes numpy/random RNG with deterministic seed for reproducibility.\n        Sets reproducible=True in MonteCarloAdvancedResult.\n\n        Args:\n            plan_name: Plan identifier for seed derivation\n            salt: Optional salt for sensitivity analysis\n\n        Returns:\n            Generated seed value for audit logging\n        "
          },
          "epistemological_foundation": {
            "paradigm": "AdvancedDAGValidator analytical paradigm",
            "ontological_basis": "Analysis via AdvancedDAGValidator._initialize_rng",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _initialize_rng implements structured analysis for D3-Q4"
            ],
            "justification": "This method contributes to D3-Q4 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "AdvancedDAGValidator._initialize_rng algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _initialize_rng"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _initialize_rng"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _initialize_rng results for downstream analysis"
            ]
          }
        },
        "_calculate_statistical_power": {
          "canonical_abbreviation": "advanceddagvalidator",
          "provides": "advanceddagvalidator.calculate_statistical_power",
          "role": "_calculate_statistical_power_calculation",
          "usage_count_in_300": 20,
          "questions": [
            "Q014",
            "Q018",
            "Q044",
            "Q048",
            "Q074",
            "Q078",
            "Q104",
            "Q108",
            "Q134",
            "Q138",
            "Q164",
            "Q168",
            "Q194",
            "Q198",
            "Q224",
            "Q228",
            "Q254",
            "Q258",
            "Q284",
            "Q288"
          ],
          "base_slots": [
            "D3-Q4",
            "D4-Q3"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM03",
            "DIM04"
          ],
          "signature": {
            "line_number": 891,
            "is_async": false,
            "parameters": [
              {
                "name": "s",
                "type": "int"
              },
              {
                "name": "n",
                "type": "int"
              },
              {
                "name": "alpha",
                "type": "float",
                "default": "0.05"
              }
            ],
            "return_type": "float",
            "docstring": "Calcula el poder estadístico a posteriori."
          },
          "epistemological_foundation": {
            "paradigm": "AdvancedDAGValidator analytical paradigm",
            "ontological_basis": "Analysis via AdvancedDAGValidator._calculate_statistical_power",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _calculate_statistical_power implements structured analysis for D3-Q4"
            ],
            "justification": "This method contributes to D3-Q4 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "AdvancedDAGValidator._calculate_statistical_power algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _calculate_statistical_power"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _calculate_statistical_power"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _calculate_statistical_power results for downstream analysis"
            ]
          }
        },
        "_get_node_validator": {
          "canonical_abbreviation": "advanceddagvalidator",
          "provides": "advanceddagvalidator.get_node_validator",
          "role": "_get_node_validator_execution",
          "usage_count_in_300": 10,
          "questions": [
            "Q014",
            "Q044",
            "Q074",
            "Q104",
            "Q134",
            "Q164",
            "Q194",
            "Q224",
            "Q254",
            "Q284"
          ],
          "base_slots": [
            "D3-Q4"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM03"
          ],
          "signature": {
            "line_number": 796,
            "is_async": false,
            "parameters": [
              {
                "name": "schema_path",
                "type": "Path | None",
                "default": "None"
              }
            ],
            "return_type": "Optional['Draft7Validator']",
            "docstring": "Obtiene (y cachea) el validador JSON Schema para nodos avanzados."
          },
          "epistemological_foundation": {
            "paradigm": "AdvancedDAGValidator analytical paradigm",
            "ontological_basis": "Analysis via AdvancedDAGValidator._get_node_validator",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _get_node_validator implements structured analysis for D3-Q4"
            ],
            "justification": "This method contributes to D3-Q4 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "AdvancedDAGValidator._get_node_validator algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _get_node_validator"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _get_node_validator"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _get_node_validator results for downstream analysis"
            ]
          }
        },
        "_create_empty_result": {
          "canonical_abbreviation": "advanceddagvalidator",
          "provides": "advanceddagvalidator.create_empty_result",
          "role": "_create_empty_result_execution",
          "usage_count_in_300": 10,
          "questions": [
            "Q014",
            "Q044",
            "Q074",
            "Q104",
            "Q134",
            "Q164",
            "Q194",
            "Q224",
            "Q254",
            "Q284"
          ],
          "base_slots": [
            "D3-Q4"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM03"
          ],
          "signature": {
            "line_number": 957,
            "is_async": false,
            "parameters": [
              {
                "name": "plan_name",
                "type": "str"
              },
              {
                "name": "seed",
                "type": "int"
              },
              {
                "name": "timestamp",
                "type": "str"
              }
            ],
            "return_type": "MonteCarloAdvancedResult",
            "docstring": "Crea un resultado vacío para grafos sin nodos."
          },
          "epistemological_foundation": {
            "paradigm": "AdvancedDAGValidator analytical paradigm",
            "ontological_basis": "Analysis via AdvancedDAGValidator._create_empty_result",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _create_empty_result implements structured analysis for D3-Q4"
            ],
            "justification": "This method contributes to D3-Q4 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "AdvancedDAGValidator._create_empty_result algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _create_empty_result"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _create_empty_result"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _create_empty_result results for downstream analysis"
            ]
          }
        },
        "_perform_sensitivity_analysis_internal": {
          "canonical_abbreviation": "advanceddagvalidator",
          "provides": "advanceddagvalidator.perform_sensitivity_analysis_internal",
          "role": "_perform_sensitivity_analysis_internal_execution",
          "usage_count_in_300": 10,
          "questions": [
            "Q017",
            "Q047",
            "Q077",
            "Q107",
            "Q137",
            "Q167",
            "Q197",
            "Q227",
            "Q257",
            "Q287"
          ],
          "base_slots": [
            "D4-Q2"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM04"
          ],
          "signature": {
            "line_number": 831,
            "is_async": false,
            "parameters": [
              {
                "name": "plan_name",
                "type": "str"
              },
              {
                "name": "base_p_value",
                "type": "float"
              },
              {
                "name": "iterations",
                "type": "int"
              }
            ],
            "return_type": "dict[str, Any]",
            "docstring": "Análisis de sensibilidad interno optimizado para evitar cálculos redundantes."
          },
          "epistemological_foundation": {
            "paradigm": "AdvancedDAGValidator analytical paradigm",
            "ontological_basis": "Analysis via AdvancedDAGValidator._perform_sensitivity_analysis_internal",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _perform_sensitivity_analysis_internal implements structured analysis for D4-Q2"
            ],
            "justification": "This method contributes to D4-Q2 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "AdvancedDAGValidator._perform_sensitivity_analysis_internal algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _perform_sensitivity_analysis_internal"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _perform_sensitivity_analysis_internal"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _perform_sensitivity_analysis_internal results for downstream analysis"
            ]
          }
        }
      }
    },
    "PolicyAnalysisEmbedder": {
      "file_path": "src/methods_dispensary/embedding_policy.py",
      "line_number": 908,
      "class_docstring": "\n    Production-ready embedding system for Colombian PDM analysis.\n\n    Implements complete pipeline:\n    1. Advanced semantic chunking with P-D-Q awareness\n    2. Multilingual embedding (Spanish-optimized)\n    3. Bi-encoder retrieval + cross-encoder reranking\n    4. Bayesian numerical analysis with uncertainty quantification\n    5. MMR-based diversification\n\n    Thread-safe, production-grade, fully typed.\n    ",
      "total_usage_in_300_contracts": 160,
      "unique_questions": 50,
      "questions": [
        "Q013",
        "Q015",
        "Q022",
        "Q023",
        "Q027",
        "Q043",
        "Q045",
        "Q052",
        "Q053",
        "Q057",
        "Q073",
        "Q075",
        "Q082",
        "Q083",
        "Q087",
        "Q103",
        "Q105",
        "Q112",
        "Q113",
        "Q117",
        "Q133",
        "Q135",
        "Q142",
        "Q143",
        "Q147",
        "Q163",
        "Q165",
        "Q172",
        "Q173",
        "Q177",
        "Q193",
        "Q195",
        "Q202",
        "Q203",
        "Q207",
        "Q223",
        "Q225",
        "Q232",
        "Q233",
        "Q237",
        "Q253",
        "Q255",
        "Q262",
        "Q263",
        "Q267",
        "Q283",
        "Q285",
        "Q292",
        "Q293",
        "Q297"
      ],
      "base_slots": [
        "D3-Q3",
        "D3-Q5",
        "D5-Q2",
        "D5-Q3",
        "D6-Q2"
      ],
      "policy_areas": [
        "PA01",
        "PA02",
        "PA03",
        "PA04",
        "PA05",
        "PA06",
        "PA07",
        "PA08",
        "PA09",
        "PA10"
      ],
      "dimensions": [
        "DIM03",
        "DIM05",
        "DIM06"
      ],
      "methods": {
        "process_document": {
          "canonical_abbreviation": "policyanalysisembedder",
          "provides": "policyanalysisembedder.process_document",
          "role": "process_document_processing",
          "usage_count_in_300": 20,
          "questions": [
            "Q013",
            "Q022",
            "Q043",
            "Q052",
            "Q073",
            "Q082",
            "Q103",
            "Q112",
            "Q133",
            "Q142",
            "Q163",
            "Q172",
            "Q193",
            "Q202",
            "Q223",
            "Q232",
            "Q253",
            "Q262",
            "Q283",
            "Q292"
          ],
          "base_slots": [
            "D3-Q3",
            "D5-Q2"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM03",
            "DIM05"
          ],
          "signature": {
            "line_number": 982,
            "is_async": false,
            "parameters": [
              {
                "name": "document_text",
                "type": "str"
              },
              {
                "name": "document_metadata",
                "type": "dict[str, Any]"
              }
            ],
            "return_type": "list[SemanticChunk]",
            "docstring": "\n        Process complete PDM document into semantic chunks with embeddings.\n\n        Args:\n            document_text: Full document text\n            document_metadata: Metadata including doc_id, municipality, year\n\n        Returns:\n            List of semantic chunks with embeddings and P-D-Q context\n        "
          },
          "epistemological_foundation": {
            "paradigm": "PolicyAnalysisEmbedder analytical paradigm",
            "ontological_basis": "Analysis via PolicyAnalysisEmbedder.process_document",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method process_document implements structured analysis for D3-Q3"
            ],
            "justification": "This method contributes to D3-Q3 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "PolicyAnalysisEmbedder.process_document algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute process_document"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from process_document"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use process_document results for downstream analysis"
            ]
          }
        },
        "_generate_query_from_pdq": {
          "canonical_abbreviation": "policyanalysisembedder",
          "provides": "policyanalysisembedder.generate_query_from_pdq",
          "role": "_generate_query_from_pdq_execution",
          "usage_count_in_300": 20,
          "questions": [
            "Q013",
            "Q022",
            "Q043",
            "Q052",
            "Q073",
            "Q082",
            "Q103",
            "Q112",
            "Q133",
            "Q142",
            "Q163",
            "Q172",
            "Q193",
            "Q202",
            "Q223",
            "Q232",
            "Q253",
            "Q262",
            "Q283",
            "Q292"
          ],
          "base_slots": [
            "D3-Q3",
            "D5-Q2"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM03",
            "DIM05"
          ],
          "signature": {
            "line_number": 1485,
            "is_async": false,
            "parameters": [
              {
                "name": "pdq",
                "type": "PDQIdentifier"
              }
            ],
            "return_type": "str",
            "docstring": "Generate search query from P-D-Q identifier."
          },
          "epistemological_foundation": {
            "paradigm": "PolicyAnalysisEmbedder analytical paradigm",
            "ontological_basis": "Analysis via PolicyAnalysisEmbedder._generate_query_from_pdq",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _generate_query_from_pdq implements structured analysis for D3-Q3"
            ],
            "justification": "This method contributes to D3-Q3 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "PolicyAnalysisEmbedder._generate_query_from_pdq algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _generate_query_from_pdq"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _generate_query_from_pdq"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _generate_query_from_pdq results for downstream analysis"
            ]
          }
        },
        "semantic_search": {
          "canonical_abbreviation": "policyanalysisembedder",
          "provides": "policyanalysisembedder.semantic_search",
          "role": "semantic_search_execution",
          "usage_count_in_300": 10,
          "questions": [
            "Q013",
            "Q043",
            "Q073",
            "Q103",
            "Q133",
            "Q163",
            "Q193",
            "Q223",
            "Q253",
            "Q283"
          ],
          "base_slots": [
            "D3-Q3"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM03"
          ],
          "signature": {
            "line_number": 1030,
            "is_async": false,
            "parameters": [
              {
                "name": "query",
                "type": "str"
              },
              {
                "name": "document_chunks",
                "type": "list[SemanticChunk]"
              },
              {
                "name": "pdq_filter",
                "type": "PDQIdentifier | None",
                "default": "None"
              },
              {
                "name": "use_reranking",
                "type": "bool",
                "default": "True"
              }
            ],
            "return_type": "list[tuple[SemanticChunk, float]]",
            "docstring": "\n        Advanced semantic search with P-D-Q filtering and reranking.\n\n        Pipeline:\n        1. Bi-encoder retrieval (fast, approximate)\n        2. P-D-Q filtering (if specified)\n        3. Cross-encoder reranking (precise)\n        4. MMR diversification\n\n        Args:\n            query: Search query\n            document_chunks: Pool of chunks to search\n            pdq_filter: Optional P-D-Q context filter\n            use_reranking: Enable cross-encoder reranking\n\n        Returns:\n          "
          },
          "epistemological_foundation": {
            "paradigm": "PolicyAnalysisEmbedder analytical paradigm",
            "ontological_basis": "Analysis via PolicyAnalysisEmbedder.semantic_search",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method semantic_search implements structured analysis for D3-Q3"
            ],
            "justification": "This method contributes to D3-Q3 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "PolicyAnalysisEmbedder.semantic_search algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute semantic_search"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from semantic_search"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use semantic_search results for downstream analysis"
            ]
          }
        },
        "_apply_mmr": {
          "canonical_abbreviation": "policyanalysisembedder",
          "provides": "policyanalysisembedder.apply_mmr",
          "role": "_apply_mmr_execution",
          "usage_count_in_300": 10,
          "questions": [
            "Q013",
            "Q043",
            "Q073",
            "Q103",
            "Q133",
            "Q163",
            "Q193",
            "Q223",
            "Q253",
            "Q283"
          ],
          "base_slots": [
            "D3-Q3"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM03"
          ],
          "signature": {
            "line_number": 1374,
            "is_async": false,
            "parameters": [
              {
                "name": "ranked_results",
                "type": "list[tuple[SemanticChunk, float]]"
              }
            ],
            "return_type": "list[tuple[SemanticChunk, float]]",
            "docstring": "\n        Apply Maximal Marginal Relevance for diversification.\n\n        Balances relevance with diversity to avoid redundant results.\n        "
          },
          "epistemological_foundation": {
            "paradigm": "PolicyAnalysisEmbedder analytical paradigm",
            "ontological_basis": "Analysis via PolicyAnalysisEmbedder._apply_mmr",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _apply_mmr implements structured analysis for D3-Q3"
            ],
            "justification": "This method contributes to D3-Q3 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "PolicyAnalysisEmbedder._apply_mmr algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _apply_mmr"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _apply_mmr"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _apply_mmr results for downstream analysis"
            ]
          }
        },
        "generate_pdq_report": {
          "canonical_abbreviation": "policyanalysisembedder",
          "provides": "policyanalysisembedder.generate_pdq_report",
          "role": "generate_pdq_report_execution",
          "usage_count_in_300": 10,
          "questions": [
            "Q013",
            "Q043",
            "Q073",
            "Q103",
            "Q133",
            "Q163",
            "Q193",
            "Q223",
            "Q253",
            "Q283"
          ],
          "base_slots": [
            "D3-Q3"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM03"
          ],
          "signature": {
            "line_number": 1172,
            "is_async": false,
            "parameters": [
              {
                "name": "document_chunks",
                "type": "list[SemanticChunk]"
              },
              {
                "name": "target_pdq",
                "type": "PDQIdentifier"
              }
            ],
            "return_type": "dict[str, Any]",
            "docstring": "\n        Generate comprehensive analytical report for P-D-Q question.\n\n        Combines semantic search, numerical analysis, and evidence synthesis.\n        "
          },
          "epistemological_foundation": {
            "paradigm": "PolicyAnalysisEmbedder analytical paradigm",
            "ontological_basis": "Analysis via PolicyAnalysisEmbedder.generate_pdq_report",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method generate_pdq_report implements structured analysis for D3-Q3"
            ],
            "justification": "This method contributes to D3-Q3 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "PolicyAnalysisEmbedder.generate_pdq_report algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute generate_pdq_report"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from generate_pdq_report"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use generate_pdq_report results for downstream analysis"
            ]
          }
        },
        "compare_policy_interventions": {
          "canonical_abbreviation": "policyanalysisembedder",
          "provides": "policyanalysisembedder.compare_policy_interventions",
          "role": "compare_policy_interventions_comparison",
          "usage_count_in_300": 10,
          "questions": [
            "Q015",
            "Q045",
            "Q075",
            "Q105",
            "Q135",
            "Q165",
            "Q195",
            "Q225",
            "Q255",
            "Q285"
          ],
          "base_slots": [
            "D3-Q5"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM03"
          ],
          "signature": {
            "line_number": 1152,
            "is_async": false,
            "parameters": [
              {
                "name": "intervention_a_chunks",
                "type": "list[SemanticChunk]"
              },
              {
                "name": "intervention_b_chunks",
                "type": "list[SemanticChunk]"
              },
              {
                "name": "pdq_context",
                "type": "PDQIdentifier"
              }
            ],
            "return_type": "dict[str, Any]",
            "docstring": "\n        Bayesian comparison of two policy interventions.\n\n        Returns probability and evidence for superiority.\n        "
          },
          "epistemological_foundation": {
            "paradigm": "PolicyAnalysisEmbedder analytical paradigm",
            "ontological_basis": "Analysis via PolicyAnalysisEmbedder.compare_policy_interventions",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method compare_policy_interventions implements structured analysis for D3-Q5"
            ],
            "justification": "This method contributes to D3-Q5 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "PolicyAnalysisEmbedder.compare_policy_interventions algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute compare_policy_interventions"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from compare_policy_interventions"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use compare_policy_interventions results for downstream analysis"
            ]
          }
        },
        "evaluate_policy_numerical_consistency": {
          "canonical_abbreviation": "policyanalysisembedder",
          "provides": "policyanalysisembedder.evaluate_policy_numerical_consistency",
          "role": "evaluate_policy_numerical_consistency_evaluation",
          "usage_count_in_300": 10,
          "questions": [
            "Q022",
            "Q052",
            "Q082",
            "Q112",
            "Q142",
            "Q172",
            "Q202",
            "Q232",
            "Q262",
            "Q292"
          ],
          "base_slots": [
            "D5-Q2"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM05"
          ],
          "signature": {
            "line_number": 1100,
            "is_async": false,
            "parameters": [
              {
                "name": "chunks",
                "type": "list[SemanticChunk]"
              },
              {
                "name": "pdq_context",
                "type": "PDQIdentifier"
              }
            ],
            "return_type": "BayesianEvaluation",
            "docstring": "\n        Bayesian evaluation of numerical consistency for policy metric.\n\n        Extracts numerical values from chunks matching P-D-Q context,\n        performs rigorous statistical analysis with uncertainty quantification.\n\n        Args:\n            chunks: Document chunks to analyze\n            pdq_context: P-D-Q context to filter relevant chunks\n\n        Returns:\n            Bayesian evaluation with credible intervals and evidence strength\n        "
          },
          "epistemological_foundation": {
            "paradigm": "PolicyAnalysisEmbedder analytical paradigm",
            "ontological_basis": "Analysis via PolicyAnalysisEmbedder.evaluate_policy_numerical_consistency",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method evaluate_policy_numerical_consistency implements structured analysis for D5-Q2"
            ],
            "justification": "This method contributes to D5-Q2 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "PolicyAnalysisEmbedder.evaluate_policy_numerical_consistency algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute evaluate_policy_numerical_consistency"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from evaluate_policy_numerical_consistency"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use evaluate_policy_numerical_consistency results for downstream analysis"
            ]
          }
        },
        "get_diagnostics": {
          "canonical_abbreviation": "policyanalysisembedder",
          "provides": "policyanalysisembedder.get_diagnostics",
          "role": "get_diagnostics_execution",
          "usage_count_in_300": 20,
          "questions": [
            "Q022",
            "Q023",
            "Q052",
            "Q053",
            "Q082",
            "Q083",
            "Q112",
            "Q113",
            "Q142",
            "Q143",
            "Q172",
            "Q173",
            "Q202",
            "Q203",
            "Q232",
            "Q233",
            "Q262",
            "Q263",
            "Q292",
            "Q293"
          ],
          "base_slots": [
            "D5-Q2",
            "D5-Q3"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM05"
          ],
          "signature": {
            "line_number": 1544,
            "is_async": false,
            "parameters": [],
            "return_type": "dict[str, Any]",
            "docstring": "Get system diagnostics and performance metrics."
          },
          "epistemological_foundation": {
            "paradigm": "PolicyAnalysisEmbedder analytical paradigm",
            "ontological_basis": "Analysis via PolicyAnalysisEmbedder.get_diagnostics",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method get_diagnostics implements structured analysis for D5-Q2"
            ],
            "justification": "This method contributes to D5-Q2 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "PolicyAnalysisEmbedder.get_diagnostics algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute get_diagnostics"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from get_diagnostics"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use get_diagnostics results for downstream analysis"
            ]
          }
        },
        "_filter_by_pdq": {
          "canonical_abbreviation": "policyanalysisembedder",
          "provides": "policyanalysisembedder.filter_by_pdq",
          "role": "_filter_by_pdq_execution",
          "usage_count_in_300": 10,
          "questions": [
            "Q022",
            "Q052",
            "Q082",
            "Q112",
            "Q142",
            "Q172",
            "Q202",
            "Q232",
            "Q262",
            "Q292"
          ],
          "base_slots": [
            "D5-Q2"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM05"
          ],
          "signature": {
            "line_number": 1284,
            "is_async": false,
            "parameters": [
              {
                "name": "chunks",
                "type": "list[SemanticChunk]"
              },
              {
                "name": "pdq_filter",
                "type": "PDQIdentifier"
              }
            ],
            "return_type": "list[SemanticChunk]",
            "docstring": "Filter chunks by P-D-Q context."
          },
          "epistemological_foundation": {
            "paradigm": "PolicyAnalysisEmbedder analytical paradigm",
            "ontological_basis": "Analysis via PolicyAnalysisEmbedder._filter_by_pdq",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _filter_by_pdq implements structured analysis for D5-Q2"
            ],
            "justification": "This method contributes to D5-Q2 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "PolicyAnalysisEmbedder._filter_by_pdq algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _filter_by_pdq"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _filter_by_pdq"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _filter_by_pdq results for downstream analysis"
            ]
          }
        },
        "_extract_numerical_values": {
          "canonical_abbreviation": "policyanalysisembedder",
          "provides": "policyanalysisembedder.extract_numerical_values",
          "role": "_extract_numerical_values_extraction",
          "usage_count_in_300": 10,
          "questions": [
            "Q022",
            "Q052",
            "Q082",
            "Q112",
            "Q142",
            "Q172",
            "Q202",
            "Q232",
            "Q262",
            "Q292"
          ],
          "base_slots": [
            "D5-Q2"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM05"
          ],
          "signature": {
            "line_number": 1429,
            "is_async": false,
            "parameters": [
              {
                "name": "chunks",
                "type": "list[SemanticChunk]"
              }
            ],
            "return_type": "list[float]",
            "docstring": "\n        Extract numerical values from chunks using advanced patterns.\n\n        Focuses on policy-relevant metrics: percentages, amounts, counts.\n        "
          },
          "epistemological_foundation": {
            "paradigm": "PolicyAnalysisEmbedder analytical paradigm",
            "ontological_basis": "Analysis via PolicyAnalysisEmbedder._extract_numerical_values",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _extract_numerical_values implements structured analysis for D5-Q2"
            ],
            "justification": "This method contributes to D5-Q2 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "PolicyAnalysisEmbedder._extract_numerical_values algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _extract_numerical_values"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _extract_numerical_values"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _extract_numerical_values results for downstream analysis"
            ]
          }
        },
        "_embed_texts": {
          "canonical_abbreviation": "policyanalysisembedder",
          "provides": "policyanalysisembedder.embed_texts",
          "role": "_embed_texts_embedding",
          "usage_count_in_300": 10,
          "questions": [
            "Q022",
            "Q052",
            "Q082",
            "Q112",
            "Q142",
            "Q172",
            "Q202",
            "Q232",
            "Q262",
            "Q292"
          ],
          "base_slots": [
            "D5-Q2"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM05"
          ],
          "signature": {
            "line_number": 1227,
            "is_async": false,
            "parameters": [
              {
                "name": "texts",
                "type": "list[str]"
              }
            ],
            "return_type": "NDArray[np.float32]",
            "docstring": "Generate embeddings with caching and retry logic."
          },
          "epistemological_foundation": {
            "paradigm": "PolicyAnalysisEmbedder analytical paradigm",
            "ontological_basis": "Analysis via PolicyAnalysisEmbedder._embed_texts",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _embed_texts implements structured analysis for D5-Q2"
            ],
            "justification": "This method contributes to D5-Q2 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "PolicyAnalysisEmbedder._embed_texts algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _embed_texts"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _embed_texts"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _embed_texts results for downstream analysis"
            ]
          }
        },
        "_compute_overall_confidence": {
          "canonical_abbreviation": "policyanalysisembedder",
          "provides": "policyanalysisembedder.compute_overall_confidence",
          "role": "_compute_overall_confidence_execution",
          "usage_count_in_300": 20,
          "questions": [
            "Q022",
            "Q027",
            "Q052",
            "Q057",
            "Q082",
            "Q087",
            "Q112",
            "Q117",
            "Q142",
            "Q147",
            "Q172",
            "Q177",
            "Q202",
            "Q207",
            "Q232",
            "Q237",
            "Q262",
            "Q267",
            "Q292",
            "Q297"
          ],
          "base_slots": [
            "D5-Q2",
            "D6-Q2"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM05",
            "DIM06"
          ],
          "signature": {
            "line_number": 1493,
            "is_async": false,
            "parameters": [
              {
                "name": "relevant_chunks",
                "type": "list[tuple[SemanticChunk, float]]"
              },
              {
                "name": "numerical_eval",
                "type": "BayesianEvaluation"
              }
            ],
            "return_type": "float",
            "docstring": "\n        Compute overall confidence score combining semantic and numerical evidence.\n\n        Considers:\n        - Number of relevant chunks\n        - Semantic relevance scores\n        - Numerical evidence strength\n        - Statistical coherence\n        "
          },
          "epistemological_foundation": {
            "paradigm": "PolicyAnalysisEmbedder analytical paradigm",
            "ontological_basis": "Analysis via PolicyAnalysisEmbedder._compute_overall_confidence",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _compute_overall_confidence implements structured analysis for D5-Q2"
            ],
            "justification": "This method contributes to D5-Q2 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "PolicyAnalysisEmbedder._compute_overall_confidence algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _compute_overall_confidence"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _compute_overall_confidence"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _compute_overall_confidence results for downstream analysis"
            ]
          }
        }
      }
    },
    "IndustrialGradeValidator": {
      "file_path": "src/methods_dispensary/teoria_cambio.py",
      "line_number": 991,
      "class_docstring": "\n    Orquesta una validación de grado industrial para el motor de Teoría de Cambio.\n    ",
      "total_usage_in_300_contracts": 110,
      "unique_questions": 30,
      "questions": [
        "Q014",
        "Q027",
        "Q028",
        "Q044",
        "Q057",
        "Q058",
        "Q074",
        "Q087",
        "Q088",
        "Q104",
        "Q117",
        "Q118",
        "Q134",
        "Q147",
        "Q148",
        "Q164",
        "Q177",
        "Q178",
        "Q194",
        "Q207",
        "Q208",
        "Q224",
        "Q237",
        "Q238",
        "Q254",
        "Q267",
        "Q268",
        "Q284",
        "Q297",
        "Q298"
      ],
      "base_slots": [
        "D3-Q4",
        "D6-Q2",
        "D6-Q3"
      ],
      "policy_areas": [
        "PA01",
        "PA02",
        "PA03",
        "PA04",
        "PA05",
        "PA06",
        "PA07",
        "PA08",
        "PA09",
        "PA10"
      ],
      "dimensions": [
        "DIM03",
        "DIM06"
      ],
      "methods": {
        "execute_suite": {
          "canonical_abbreviation": "industrialgradevalidator",
          "provides": "industrialgradevalidator.execute_suite",
          "role": "execute_suite_execution",
          "usage_count_in_300": 20,
          "questions": [
            "Q014",
            "Q028",
            "Q044",
            "Q058",
            "Q074",
            "Q088",
            "Q104",
            "Q118",
            "Q134",
            "Q148",
            "Q164",
            "Q178",
            "Q194",
            "Q208",
            "Q224",
            "Q238",
            "Q254",
            "Q268",
            "Q284",
            "Q298"
          ],
          "base_slots": [
            "D3-Q4",
            "D6-Q3"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM03",
            "DIM06"
          ],
          "signature": {
            "line_number": 1007,
            "is_async": false,
            "parameters": [],
            "return_type": "bool",
            "docstring": "Ejecuta la suite completa de validación industrial."
          },
          "epistemological_foundation": {
            "paradigm": "IndustrialGradeValidator analytical paradigm",
            "ontological_basis": "Analysis via IndustrialGradeValidator.execute_suite",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method execute_suite implements structured analysis for D3-Q4"
            ],
            "justification": "This method contributes to D3-Q4 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "IndustrialGradeValidator.execute_suite algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute execute_suite"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from execute_suite"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use execute_suite results for downstream analysis"
            ]
          }
        },
        "validate_connection_matrix": {
          "canonical_abbreviation": "industrialgradevalidator",
          "provides": "industrialgradevalidator.validate_connection_matrix",
          "role": "validate_connection_matrix_validation",
          "usage_count_in_300": 20,
          "questions": [
            "Q014",
            "Q027",
            "Q044",
            "Q057",
            "Q074",
            "Q087",
            "Q104",
            "Q117",
            "Q134",
            "Q147",
            "Q164",
            "Q177",
            "Q194",
            "Q207",
            "Q224",
            "Q237",
            "Q254",
            "Q267",
            "Q284",
            "Q297"
          ],
          "base_slots": [
            "D3-Q4",
            "D6-Q2"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM03",
            "DIM06"
          ],
          "signature": {
            "line_number": 1078,
            "is_async": false,
            "parameters": [],
            "return_type": "bool",
            "docstring": "Valida la matriz de transiciones causales."
          },
          "epistemological_foundation": {
            "paradigm": "IndustrialGradeValidator analytical paradigm",
            "ontological_basis": "Analysis via IndustrialGradeValidator.validate_connection_matrix",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method validate_connection_matrix implements structured analysis for D3-Q4"
            ],
            "justification": "This method contributes to D3-Q4 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "IndustrialGradeValidator.validate_connection_matrix algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute validate_connection_matrix"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from validate_connection_matrix"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use validate_connection_matrix results for downstream analysis"
            ]
          }
        },
        "run_performance_benchmarks": {
          "canonical_abbreviation": "industrialgradevalidator",
          "provides": "industrialgradevalidator.run_performance_benchmarks",
          "role": "run_performance_benchmarks_execution",
          "usage_count_in_300": 10,
          "questions": [
            "Q014",
            "Q044",
            "Q074",
            "Q104",
            "Q134",
            "Q164",
            "Q194",
            "Q224",
            "Q254",
            "Q284"
          ],
          "base_slots": [
            "D3-Q4"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM03"
          ],
          "signature": {
            "line_number": 1099,
            "is_async": false,
            "parameters": [],
            "return_type": "bool",
            "docstring": "Ejecuta benchmarks de rendimiento para las operaciones críticas del motor."
          },
          "epistemological_foundation": {
            "paradigm": "IndustrialGradeValidator analytical paradigm",
            "ontological_basis": "Analysis via IndustrialGradeValidator.run_performance_benchmarks",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method run_performance_benchmarks implements structured analysis for D3-Q4"
            ],
            "justification": "This method contributes to D3-Q4 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "IndustrialGradeValidator.run_performance_benchmarks algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute run_performance_benchmarks"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from run_performance_benchmarks"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use run_performance_benchmarks results for downstream analysis"
            ]
          }
        },
        "_benchmark_operation": {
          "canonical_abbreviation": "industrialgradevalidator",
          "provides": "industrialgradevalidator.benchmark_operation",
          "role": "_benchmark_operation_execution",
          "usage_count_in_300": 20,
          "questions": [
            "Q014",
            "Q028",
            "Q044",
            "Q058",
            "Q074",
            "Q088",
            "Q104",
            "Q118",
            "Q134",
            "Q148",
            "Q164",
            "Q178",
            "Q194",
            "Q208",
            "Q224",
            "Q238",
            "Q254",
            "Q268",
            "Q284",
            "Q298"
          ],
          "base_slots": [
            "D3-Q4",
            "D6-Q3"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM03",
            "DIM06"
          ],
          "signature": {
            "line_number": 1128,
            "is_async": false,
            "parameters": [
              {
                "name": "operation_name",
                "type": "str"
              },
              {
                "name": "callable_obj"
              },
              {
                "name": "threshold",
                "type": "float"
              },
              {
                "name": "*args"
              },
              {
                "name": "**kwargs"
              }
            ],
            "return_type": null,
            "docstring": "Mide el tiempo de ejecución de una operación y registra la métrica."
          },
          "epistemological_foundation": {
            "paradigm": "IndustrialGradeValidator analytical paradigm",
            "ontological_basis": "Analysis via IndustrialGradeValidator._benchmark_operation",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _benchmark_operation implements structured analysis for D3-Q4"
            ],
            "justification": "This method contributes to D3-Q4 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "IndustrialGradeValidator._benchmark_operation algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _benchmark_operation"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _benchmark_operation"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _benchmark_operation results for downstream analysis"
            ]
          }
        },
        "_log_metric": {
          "canonical_abbreviation": "industrialgradevalidator",
          "provides": "industrialgradevalidator.log_metric",
          "role": "_log_metric_execution",
          "usage_count_in_300": 10,
          "questions": [
            "Q014",
            "Q044",
            "Q074",
            "Q104",
            "Q134",
            "Q164",
            "Q194",
            "Q224",
            "Q254",
            "Q284"
          ],
          "base_slots": [
            "D3-Q4"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM03"
          ],
          "signature": {
            "line_number": 1139,
            "is_async": false,
            "parameters": [
              {
                "name": "name",
                "type": "str"
              },
              {
                "name": "value",
                "type": "float"
              },
              {
                "name": "unit",
                "type": "str"
              },
              {
                "name": "threshold",
                "type": "float"
              }
            ],
            "return_type": null,
            "docstring": "Registra y reporta una métrica de validación."
          },
          "epistemological_foundation": {
            "paradigm": "IndustrialGradeValidator analytical paradigm",
            "ontological_basis": "Analysis via IndustrialGradeValidator._log_metric",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _log_metric implements structured analysis for D3-Q4"
            ],
            "justification": "This method contributes to D3-Q4 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "IndustrialGradeValidator._log_metric algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _log_metric"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _log_metric"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _log_metric results for downstream analysis"
            ]
          }
        },
        "validate_engine_readiness": {
          "canonical_abbreviation": "industrialgradevalidator",
          "provides": "industrialgradevalidator.validate_engine_readiness",
          "role": "validate_engine_readiness_validation",
          "usage_count_in_300": 20,
          "questions": [
            "Q014",
            "Q028",
            "Q044",
            "Q058",
            "Q074",
            "Q088",
            "Q104",
            "Q118",
            "Q134",
            "Q148",
            "Q164",
            "Q178",
            "Q194",
            "Q208",
            "Q224",
            "Q238",
            "Q254",
            "Q268",
            "Q284",
            "Q298"
          ],
          "base_slots": [
            "D3-Q4",
            "D6-Q3"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM03",
            "DIM06"
          ],
          "signature": {
            "line_number": 1040,
            "is_async": false,
            "parameters": [],
            "return_type": "bool",
            "docstring": "Valida la disponibilidad y tiempo de instanciación de los motores de análisis."
          },
          "epistemological_foundation": {
            "paradigm": "IndustrialGradeValidator analytical paradigm",
            "ontological_basis": "Analysis via IndustrialGradeValidator.validate_engine_readiness",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method validate_engine_readiness implements structured analysis for D3-Q4"
            ],
            "justification": "This method contributes to D3-Q4 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "IndustrialGradeValidator.validate_engine_readiness algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute validate_engine_readiness"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from validate_engine_readiness"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use validate_engine_readiness results for downstream analysis"
            ]
          }
        },
        "validate_causal_categories": {
          "canonical_abbreviation": "industrialgradevalidator",
          "provides": "industrialgradevalidator.validate_causal_categories",
          "role": "validate_causal_categories_validation",
          "usage_count_in_300": 10,
          "questions": [
            "Q014",
            "Q044",
            "Q074",
            "Q104",
            "Q134",
            "Q164",
            "Q194",
            "Q224",
            "Q254",
            "Q284"
          ],
          "base_slots": [
            "D3-Q4"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM03"
          ],
          "signature": {
            "line_number": 1060,
            "is_async": false,
            "parameters": [],
            "return_type": "bool",
            "docstring": "Valida la completitud y el orden axiomático de las categorías causales."
          },
          "epistemological_foundation": {
            "paradigm": "IndustrialGradeValidator analytical paradigm",
            "ontological_basis": "Analysis via IndustrialGradeValidator.validate_causal_categories",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method validate_causal_categories implements structured analysis for D3-Q4"
            ],
            "justification": "This method contributes to D3-Q4 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "IndustrialGradeValidator.validate_causal_categories algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute validate_causal_categories"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from validate_causal_categories"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use validate_causal_categories results for downstream analysis"
            ]
          }
        }
      }
    },
    "HierarchicalGenerativeModel": {
      "file_path": "src/methods_dispensary/derek_beach.py",
      "line_number": 5203,
      "class_docstring": "\n    AGUJA II - Modelo Generativo Jerárquico con inferencia MCMC\n\n    PROMPT II-1: Inferencia jerárquica con incertidumbre\n    Estima posterior(mechanism_type, activity_sequence | obs) con MCMC.\n\n    PROMPT II-2: Posterior Predictive Checks + Ablation\n    Genera datos simulados desde posterior y compara con observados.\n\n    PROMPT II-3: Independencias y parsimonia\n    Verifica d-separaciones y calcula ΔWAIC.\n\n    QUALITY CRITERIA:\n    - R-hat ≤ 1.10\n    - ESS ≥ 200\n    - entropy/entropy_max < 0.",
      "total_usage_in_300_contracts": 120,
      "unique_questions": 50,
      "questions": [
        "Q014",
        "Q017",
        "Q018",
        "Q025",
        "Q028",
        "Q044",
        "Q047",
        "Q048",
        "Q055",
        "Q058",
        "Q074",
        "Q077",
        "Q078",
        "Q085",
        "Q088",
        "Q104",
        "Q107",
        "Q108",
        "Q115",
        "Q118",
        "Q134",
        "Q137",
        "Q138",
        "Q145",
        "Q148",
        "Q164",
        "Q167",
        "Q168",
        "Q175",
        "Q178",
        "Q194",
        "Q197",
        "Q198",
        "Q205",
        "Q208",
        "Q224",
        "Q227",
        "Q228",
        "Q235",
        "Q238",
        "Q254",
        "Q257",
        "Q258",
        "Q265",
        "Q268",
        "Q284",
        "Q287",
        "Q288",
        "Q295",
        "Q298"
      ],
      "base_slots": [
        "D3-Q4",
        "D4-Q2",
        "D4-Q3",
        "D5-Q5",
        "D6-Q3"
      ],
      "policy_areas": [
        "PA01",
        "PA02",
        "PA03",
        "PA04",
        "PA05",
        "PA06",
        "PA07",
        "PA08",
        "PA09",
        "PA10"
      ],
      "dimensions": [
        "DIM03",
        "DIM04",
        "DIM05",
        "DIM06"
      ],
      "methods": {
        "_calculate_likelihood": {
          "canonical_abbreviation": "hierarchicalgenerativemodel",
          "provides": "hierarchicalgenerativemodel.calculate_likelihood",
          "role": "_calculate_likelihood_calculation",
          "usage_count_in_300": 10,
          "questions": [
            "Q014",
            "Q044",
            "Q074",
            "Q104",
            "Q134",
            "Q164",
            "Q194",
            "Q224",
            "Q254",
            "Q284"
          ],
          "base_slots": [
            "D3-Q4"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM03"
          ],
          "signature": {
            "line_number": 5408,
            "is_async": false,
            "parameters": [
              {
                "name": "mechanism_type",
                "type": "str"
              },
              {
                "name": "observations",
                "type": "dict[str, Any]"
              }
            ],
            "return_type": "float",
            "docstring": "Calcula likelihood de observations dado mechanism_type"
          },
          "epistemological_foundation": {
            "paradigm": "HierarchicalGenerativeModel analytical paradigm",
            "ontological_basis": "Analysis via HierarchicalGenerativeModel._calculate_likelihood",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _calculate_likelihood implements structured analysis for D3-Q4"
            ],
            "justification": "This method contributes to D3-Q4 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "HierarchicalGenerativeModel._calculate_likelihood algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _calculate_likelihood"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _calculate_likelihood"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _calculate_likelihood results for downstream analysis"
            ]
          }
        },
        "_calculate_ess": {
          "canonical_abbreviation": "hierarchicalgenerativemodel",
          "provides": "hierarchicalgenerativemodel.calculate_ess",
          "role": "_calculate_ess_calculation",
          "usage_count_in_300": 30,
          "questions": [
            "Q014",
            "Q018",
            "Q028",
            "Q044",
            "Q048",
            "Q058",
            "Q074",
            "Q078",
            "Q088",
            "Q104",
            "Q108",
            "Q118",
            "Q134",
            "Q138",
            "Q148",
            "Q164",
            "Q168",
            "Q178",
            "Q194",
            "Q198",
            "Q208",
            "Q224",
            "Q228",
            "Q238",
            "Q254",
            "Q258",
            "Q268",
            "Q284",
            "Q288",
            "Q298"
          ],
          "base_slots": [
            "D3-Q4",
            "D4-Q3",
            "D6-Q3"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM03",
            "DIM04",
            "DIM06"
          ],
          "signature": {
            "line_number": 5484,
            "is_async": false,
            "parameters": [
              {
                "name": "samples",
                "type": "list[dict[str, Any]]"
              }
            ],
            "return_type": "float",
            "docstring": "Calcula Effective Sample Size (simplificado)"
          },
          "epistemological_foundation": {
            "paradigm": "HierarchicalGenerativeModel analytical paradigm",
            "ontological_basis": "Analysis via HierarchicalGenerativeModel._calculate_ess",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _calculate_ess implements structured analysis for D3-Q4"
            ],
            "justification": "This method contributes to D3-Q4 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "HierarchicalGenerativeModel._calculate_ess algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _calculate_ess"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _calculate_ess"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _calculate_ess results for downstream analysis"
            ]
          }
        },
        "_calculate_r_hat": {
          "canonical_abbreviation": "hierarchicalgenerativemodel",
          "provides": "hierarchicalgenerativemodel.calculate_r_hat",
          "role": "_calculate_r_hat_calculation",
          "usage_count_in_300": 30,
          "questions": [
            "Q014",
            "Q018",
            "Q028",
            "Q044",
            "Q048",
            "Q058",
            "Q074",
            "Q078",
            "Q088",
            "Q104",
            "Q108",
            "Q118",
            "Q134",
            "Q138",
            "Q148",
            "Q164",
            "Q168",
            "Q178",
            "Q194",
            "Q198",
            "Q208",
            "Q224",
            "Q228",
            "Q238",
            "Q254",
            "Q258",
            "Q268",
            "Q284",
            "Q288",
            "Q298"
          ],
          "base_slots": [
            "D3-Q4",
            "D4-Q3",
            "D6-Q3"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM03",
            "DIM04",
            "DIM06"
          ],
          "signature": {
            "line_number": 5449,
            "is_async": false,
            "parameters": [
              {
                "name": "chains",
                "type": "list[list[dict[str, Any]]]"
              }
            ],
            "return_type": "float",
            "docstring": "Calcula Gelman-Rubin R-hat para diagnóstico de convergencia"
          },
          "epistemological_foundation": {
            "paradigm": "HierarchicalGenerativeModel analytical paradigm",
            "ontological_basis": "Analysis via HierarchicalGenerativeModel._calculate_r_hat",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _calculate_r_hat implements structured analysis for D3-Q4"
            ],
            "justification": "This method contributes to D3-Q4 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "HierarchicalGenerativeModel._calculate_r_hat algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _calculate_r_hat"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _calculate_r_hat"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _calculate_r_hat results for downstream analysis"
            ]
          }
        },
        "verify_conditional_independence": {
          "canonical_abbreviation": "hierarchicalgenerativemodel",
          "provides": "hierarchicalgenerativemodel.verify_conditional_independence",
          "role": "verify_conditional_independence_execution",
          "usage_count_in_300": 10,
          "questions": [
            "Q017",
            "Q047",
            "Q077",
            "Q107",
            "Q137",
            "Q167",
            "Q197",
            "Q227",
            "Q257",
            "Q287"
          ],
          "base_slots": [
            "D4-Q2"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM04"
          ],
          "signature": {
            "line_number": 5601,
            "is_async": false,
            "parameters": [
              {
                "name": "dag",
                "type": "nx.DiGraph"
              },
              {
                "name": "independence_tests",
                "type": "list[tuple[str, str, list[str]]] | None",
                "default": "None"
              }
            ],
            "return_type": "dict[str, Any]",
            "docstring": "\n        PROMPT II-3: Independencias y parsimonia\n\n        Verifica d-separaciones implicadas por el DAG.\n        Calcula ΔWAIC entre modelo jerárquico vs. nulo.\n\n        Args:\n            dag: NetworkX DiGraph del modelo causal\n            independence_tests: Lista de tuplas (X, Y, Z) para test X ⊥ Y | Z\n\n        Returns:\n            Dict con independence_tests, delta_waic, model_preference, criteria_met\n        "
          },
          "epistemological_foundation": {
            "paradigm": "HierarchicalGenerativeModel analytical paradigm",
            "ontological_basis": "Analysis via HierarchicalGenerativeModel.verify_conditional_independence",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method verify_conditional_independence implements structured analysis for D4-Q2"
            ],
            "justification": "This method contributes to D4-Q2 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "HierarchicalGenerativeModel.verify_conditional_independence algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute verify_conditional_independence"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from verify_conditional_independence"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use verify_conditional_independence results for downstream analysis"
            ]
          }
        },
        "_generate_independence_tests": {
          "canonical_abbreviation": "hierarchicalgenerativemodel",
          "provides": "hierarchicalgenerativemodel.generate_independence_tests",
          "role": "_generate_independence_tests_execution",
          "usage_count_in_300": 10,
          "questions": [
            "Q017",
            "Q047",
            "Q077",
            "Q107",
            "Q137",
            "Q167",
            "Q197",
            "Q227",
            "Q257",
            "Q287"
          ],
          "base_slots": [
            "D4-Q2"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM04"
          ],
          "signature": {
            "line_number": 5682,
            "is_async": false,
            "parameters": [
              {
                "name": "dag",
                "type": "nx.DiGraph"
              },
              {
                "name": "n_tests",
                "type": "int",
                "default": "3"
              }
            ],
            "return_type": "list[tuple[str, str, list[str]]]",
            "docstring": "Genera tests de independencia automáticamente desde DAG"
          },
          "epistemological_foundation": {
            "paradigm": "HierarchicalGenerativeModel analytical paradigm",
            "ontological_basis": "Analysis via HierarchicalGenerativeModel._generate_independence_tests",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _generate_independence_tests implements structured analysis for D4-Q2"
            ],
            "justification": "This method contributes to D4-Q2 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "HierarchicalGenerativeModel._generate_independence_tests algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _generate_independence_tests"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _generate_independence_tests"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _generate_independence_tests results for downstream analysis"
            ]
          }
        },
        "posterior_predictive_check": {
          "canonical_abbreviation": "hierarchicalgenerativemodel",
          "provides": "hierarchicalgenerativemodel.posterior_predictive_check",
          "role": "posterior_predictive_check_execution",
          "usage_count_in_300": 10,
          "questions": [
            "Q025",
            "Q055",
            "Q085",
            "Q115",
            "Q145",
            "Q175",
            "Q205",
            "Q235",
            "Q265",
            "Q295"
          ],
          "base_slots": [
            "D5-Q5"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM05"
          ],
          "signature": {
            "line_number": 5509,
            "is_async": false,
            "parameters": [
              {
                "name": "posterior_samples",
                "type": "list[dict[str, Any]]"
              },
              {
                "name": "observed_data",
                "type": "dict[str, Any]"
              }
            ],
            "return_type": "dict[str, Any]",
            "docstring": "\n        PROMPT II-2: Posterior Predictive Checks + Ablation\n\n        Genera datos simulados desde posterior y compara con observados.\n        Realiza ablation de pasos de secuencia.\n\n        Args:\n            posterior_samples: Samples del posterior MCMC\n            observed_data: Datos observados reales\n\n        Returns:\n            Dict con ppd_p_value, distance_metric, ablation_curve, criteria_met\n        "
          },
          "epistemological_foundation": {
            "paradigm": "HierarchicalGenerativeModel analytical paradigm",
            "ontological_basis": "Analysis via HierarchicalGenerativeModel.posterior_predictive_check",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method posterior_predictive_check implements structured analysis for D5-Q5"
            ],
            "justification": "This method contributes to D5-Q5 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "HierarchicalGenerativeModel.posterior_predictive_check algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute posterior_predictive_check"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from posterior_predictive_check"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use posterior_predictive_check results for downstream analysis"
            ]
          }
        },
        "_ablation_analysis": {
          "canonical_abbreviation": "hierarchicalgenerativemodel",
          "provides": "hierarchicalgenerativemodel.ablation_analysis",
          "role": "_ablation_analysis_execution",
          "usage_count_in_300": 10,
          "questions": [
            "Q025",
            "Q055",
            "Q085",
            "Q115",
            "Q145",
            "Q175",
            "Q205",
            "Q235",
            "Q265",
            "Q295"
          ],
          "base_slots": [
            "D5-Q5"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM05"
          ],
          "signature": {
            "line_number": 5582,
            "is_async": false,
            "parameters": [
              {
                "name": "posterior_samples",
                "type": "list[dict[str, Any]]"
              },
              {
                "name": "observed_data",
                "type": "dict[str, Any]"
              }
            ],
            "return_type": "dict[str, float]",
            "docstring": "Mide caída en coherence al quitar pasos de secuencia"
          },
          "epistemological_foundation": {
            "paradigm": "HierarchicalGenerativeModel analytical paradigm",
            "ontological_basis": "Analysis via HierarchicalGenerativeModel._ablation_analysis",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _ablation_analysis implements structured analysis for D5-Q5"
            ],
            "justification": "This method contributes to D5-Q5 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "HierarchicalGenerativeModel._ablation_analysis algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _ablation_analysis"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _ablation_analysis"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _ablation_analysis results for downstream analysis"
            ]
          }
        },
        "_calculate_waic_difference": {
          "canonical_abbreviation": "hierarchicalgenerativemodel",
          "provides": "hierarchicalgenerativemodel.calculate_waic_difference",
          "role": "_calculate_waic_difference_calculation",
          "usage_count_in_300": 10,
          "questions": [
            "Q025",
            "Q055",
            "Q085",
            "Q115",
            "Q145",
            "Q175",
            "Q205",
            "Q235",
            "Q265",
            "Q295"
          ],
          "base_slots": [
            "D5-Q5"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM05"
          ],
          "signature": {
            "line_number": 5709,
            "is_async": false,
            "parameters": [
              {
                "name": "dag",
                "type": "nx.DiGraph"
              }
            ],
            "return_type": "float",
            "docstring": "\n        Calcula ΔWAIC = WAIC_hierarchical - WAIC_null (simplificado)\n\n        En producción: usar arviz.waic() con trace real de PyMC/Stan\n        "
          },
          "epistemological_foundation": {
            "paradigm": "HierarchicalGenerativeModel analytical paradigm",
            "ontological_basis": "Analysis via HierarchicalGenerativeModel._calculate_waic_difference",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _calculate_waic_difference implements structured analysis for D5-Q5"
            ],
            "justification": "This method contributes to D5-Q5 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "HierarchicalGenerativeModel._calculate_waic_difference algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _calculate_waic_difference"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _calculate_waic_difference"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _calculate_waic_difference results for downstream analysis"
            ]
          }
        }
      }
    },
    "BayesFactorTable": {
      "file_path": "src/methods_dispensary/derek_beach.py",
      "line_number": 4766,
      "class_docstring": "Tabla fija de Bayes Factors por tipo de test evidencial (Beach & Pedersen 2019)",
      "total_usage_in_300_contracts": 20,
      "unique_questions": 20,
      "questions": [
        "Q017",
        "Q029",
        "Q047",
        "Q059",
        "Q077",
        "Q089",
        "Q107",
        "Q119",
        "Q137",
        "Q149",
        "Q167",
        "Q179",
        "Q197",
        "Q209",
        "Q227",
        "Q239",
        "Q257",
        "Q269",
        "Q287",
        "Q299"
      ],
      "base_slots": [
        "D4-Q2",
        "D6-Q4"
      ],
      "policy_areas": [
        "PA01",
        "PA02",
        "PA03",
        "PA04",
        "PA05",
        "PA06",
        "PA07",
        "PA08",
        "PA09",
        "PA10"
      ],
      "dimensions": [
        "DIM04",
        "DIM06"
      ],
      "methods": {
        "get_bayes_factor": {
          "canonical_abbreviation": "bayesfactortable",
          "provides": "bayesfactortable.get_bayes_factor",
          "role": "get_bayes_factor_execution",
          "usage_count_in_300": 20,
          "questions": [
            "Q017",
            "Q029",
            "Q047",
            "Q059",
            "Q077",
            "Q089",
            "Q107",
            "Q119",
            "Q137",
            "Q149",
            "Q167",
            "Q179",
            "Q197",
            "Q209",
            "Q227",
            "Q239",
            "Q257",
            "Q269",
            "Q287",
            "Q299"
          ],
          "base_slots": [
            "D4-Q2",
            "D6-Q4"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM04",
            "DIM06"
          ],
          "signature": {
            "line_number": 4776,
            "is_async": false,
            "parameters": [
              {
                "name": "test_type",
                "type": "str"
              }
            ],
            "return_type": "float",
            "docstring": "Obtiene BF medio para tipo de test"
          },
          "epistemological_foundation": {
            "paradigm": "BayesFactorTable analytical paradigm",
            "ontological_basis": "Analysis via BayesFactorTable.get_bayes_factor",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method get_bayes_factor implements structured analysis for D4-Q2"
            ],
            "justification": "This method contributes to D4-Q2 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "BayesFactorTable.get_bayes_factor algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute get_bayes_factor"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from get_bayes_factor"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use get_bayes_factor results for downstream analysis"
            ]
          }
        }
      }
    },
    "CDAFFramework": {
      "file_path": "src/methods_dispensary/derek_beach.py",
      "line_number": 4102,
      "class_docstring": "Main orchestrator for the CDAF pipeline",
      "total_usage_in_300_contracts": 30,
      "unique_questions": 20,
      "questions": [
        "Q020",
        "Q029",
        "Q050",
        "Q059",
        "Q080",
        "Q089",
        "Q110",
        "Q119",
        "Q140",
        "Q149",
        "Q170",
        "Q179",
        "Q200",
        "Q209",
        "Q230",
        "Q239",
        "Q260",
        "Q269",
        "Q290",
        "Q299"
      ],
      "base_slots": [
        "D4-Q5",
        "D6-Q4"
      ],
      "policy_areas": [
        "PA01",
        "PA02",
        "PA03",
        "PA04",
        "PA05",
        "PA06",
        "PA07",
        "PA08",
        "PA09",
        "PA10"
      ],
      "dimensions": [
        "DIM04",
        "DIM06"
      ],
      "methods": {
        "_validate_dnp_compliance": {
          "canonical_abbreviation": "cdafframework",
          "provides": "cdafframework.validate_dnp_compliance",
          "role": "_validate_dnp_compliance_validation",
          "usage_count_in_300": 10,
          "questions": [
            "Q020",
            "Q050",
            "Q080",
            "Q110",
            "Q140",
            "Q170",
            "Q200",
            "Q230",
            "Q260",
            "Q290"
          ],
          "base_slots": [
            "D4-Q5"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM04"
          ],
          "signature": {
            "line_number": 4387,
            "is_async": false,
            "parameters": [
              {
                "name": "nodes",
                "type": "dict[str, MetaNode]"
              },
              {
                "name": "graph",
                "type": "nx.DiGraph"
              },
              {
                "name": "policy_code",
                "type": "str"
              }
            ],
            "return_type": "None",
            "docstring": "\n        Validate DNP compliance for all nodes/projects\n        Generates DNP compliance report\n        "
          },
          "epistemological_foundation": {
            "paradigm": "CDAFFramework analytical paradigm",
            "ontological_basis": "Analysis via CDAFFramework._validate_dnp_compliance",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _validate_dnp_compliance implements structured analysis for D4-Q5"
            ],
            "justification": "This method contributes to D4-Q5 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "CDAFFramework._validate_dnp_compliance algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _validate_dnp_compliance"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _validate_dnp_compliance"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _validate_dnp_compliance results for downstream analysis"
            ]
          }
        },
        "_generate_dnp_report": {
          "canonical_abbreviation": "cdafframework",
          "provides": "cdafframework.generate_dnp_report",
          "role": "_generate_dnp_report_execution",
          "usage_count_in_300": 10,
          "questions": [
            "Q020",
            "Q050",
            "Q080",
            "Q110",
            "Q140",
            "Q170",
            "Q200",
            "Q230",
            "Q260",
            "Q290"
          ],
          "base_slots": [
            "D4-Q5"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM04"
          ],
          "signature": {
            "line_number": 4464,
            "is_async": false,
            "parameters": [
              {
                "name": "dnp_results",
                "type": "list[dict]"
              },
              {
                "name": "policy_code",
                "type": "str"
              }
            ],
            "return_type": "None",
            "docstring": "Generate comprehensive DNP compliance report"
          },
          "epistemological_foundation": {
            "paradigm": "CDAFFramework analytical paradigm",
            "ontological_basis": "Analysis via CDAFFramework._generate_dnp_report",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _generate_dnp_report implements structured analysis for D4-Q5"
            ],
            "justification": "This method contributes to D4-Q5 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "CDAFFramework._generate_dnp_report algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _generate_dnp_report"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _generate_dnp_report"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _generate_dnp_report results for downstream analysis"
            ]
          }
        },
        "_extract_feedback_from_audit": {
          "canonical_abbreviation": "cdafframework",
          "provides": "cdafframework.extract_feedback_from_audit",
          "role": "_extract_feedback_from_audit_extraction",
          "usage_count_in_300": 10,
          "questions": [
            "Q029",
            "Q059",
            "Q089",
            "Q119",
            "Q149",
            "Q179",
            "Q209",
            "Q239",
            "Q269",
            "Q299"
          ],
          "base_slots": [
            "D6-Q4"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM06"
          ],
          "signature": {
            "line_number": 4288,
            "is_async": false,
            "parameters": [
              {
                "name": "inferred_mechanisms",
                "type": "dict[str, dict[str, Any]]"
              },
              {
                "name": "counterfactual_audit",
                "type": "dict[str, Any]"
              },
              {
                "name": "audit_results",
                "type": "dict[str, AuditResult]"
              }
            ],
            "return_type": "dict[str, Any]",
            "docstring": "\n        Extract feedback data from audit results for self-reflective prior updating\n\n        This implements the frontier paradigm of learning from audit results\n        to improve future inference accuracy.\n\n        HARMONIC FRONT 4 ENHANCEMENT:\n        - Reduces mechanism_type_priors for mechanisms with implementation_failure flags\n        - Tracks necessity/sufficiency test failures\n        - Penalizes \"miracle\" mechanisms that fail counterfactual tests\n        "
          },
          "epistemological_foundation": {
            "paradigm": "CDAFFramework analytical paradigm",
            "ontological_basis": "Analysis via CDAFFramework._extract_feedback_from_audit",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _extract_feedback_from_audit implements structured analysis for D6-Q4"
            ],
            "justification": "This method contributes to D6-Q4 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "CDAFFramework._extract_feedback_from_audit algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _extract_feedback_from_audit"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _extract_feedback_from_audit"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _extract_feedback_from_audit results for downstream analysis"
            ]
          }
        }
      }
    },
    "PolicyTextProcessor": {
      "file_path": "src/methods_dispensary/policy_processor.py",
      "line_number": 562,
      "class_docstring": "\n    Industrial-grade text processing with multi-scale segmentation and\n    coherence-preserving normalization for policy document analysis.\n    ",
      "total_usage_in_300_contracts": 40,
      "unique_questions": 10,
      "questions": [
        "Q022",
        "Q052",
        "Q082",
        "Q112",
        "Q142",
        "Q172",
        "Q202",
        "Q232",
        "Q262",
        "Q292"
      ],
      "base_slots": [
        "D5-Q2"
      ],
      "policy_areas": [
        "PA01",
        "PA02",
        "PA03",
        "PA04",
        "PA05",
        "PA06",
        "PA07",
        "PA08",
        "PA09",
        "PA10"
      ],
      "dimensions": [
        "DIM05"
      ],
      "methods": {
        "normalize_unicode": {
          "canonical_abbreviation": "policytextprocessor",
          "provides": "policytextprocessor.normalize_unicode",
          "role": "normalize_unicode_execution",
          "usage_count_in_300": 10,
          "questions": [
            "Q022",
            "Q052",
            "Q082",
            "Q112",
            "Q142",
            "Q172",
            "Q202",
            "Q232",
            "Q262",
            "Q292"
          ],
          "base_slots": [
            "D5-Q2"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM05"
          ],
          "signature": {
            "line_number": 577,
            "is_async": false,
            "parameters": [
              {
                "name": "text",
                "type": "str"
              }
            ],
            "return_type": "str",
            "docstring": "Apply canonical Unicode normalization (NFC/NFKC)."
          },
          "epistemological_foundation": {
            "paradigm": "PolicyTextProcessor analytical paradigm",
            "ontological_basis": "Analysis via PolicyTextProcessor.normalize_unicode",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method normalize_unicode implements structured analysis for D5-Q2"
            ],
            "justification": "This method contributes to D5-Q2 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "PolicyTextProcessor.normalize_unicode algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute normalize_unicode"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from normalize_unicode"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use normalize_unicode results for downstream analysis"
            ]
          }
        },
        "segment_into_sentences": {
          "canonical_abbreviation": "policytextprocessor",
          "provides": "policytextprocessor.segment_into_sentences",
          "role": "segment_into_sentences_execution",
          "usage_count_in_300": 10,
          "questions": [
            "Q022",
            "Q052",
            "Q082",
            "Q112",
            "Q142",
            "Q172",
            "Q202",
            "Q232",
            "Q262",
            "Q292"
          ],
          "base_slots": [
            "D5-Q2"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM05"
          ],
          "signature": {
            "line_number": 582,
            "is_async": false,
            "parameters": [
              {
                "name": "text",
                "type": "str"
              },
              {
                "name": "**kwargs",
                "type": "Any"
              }
            ],
            "return_type": "list[str]",
            "docstring": "\n        Segment text into sentences with context-aware boundary detection.\n        Handles abbreviations, numerical lists, and Colombian naming conventions.\n\n        Args:\n            text: Input text to segment\n            **kwargs: Additional optional parameters for compatibility\n\n        Returns:\n            List of sentence strings\n        "
          },
          "epistemological_foundation": {
            "paradigm": "PolicyTextProcessor analytical paradigm",
            "ontological_basis": "Analysis via PolicyTextProcessor.segment_into_sentences",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method segment_into_sentences implements structured analysis for D5-Q2"
            ],
            "justification": "This method contributes to D5-Q2 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "PolicyTextProcessor.segment_into_sentences algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute segment_into_sentences"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from segment_into_sentences"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use segment_into_sentences results for downstream analysis"
            ]
          }
        },
        "compile_pattern": {
          "canonical_abbreviation": "policytextprocessor",
          "provides": "policytextprocessor.compile_pattern",
          "role": "compile_pattern_execution",
          "usage_count_in_300": 10,
          "questions": [
            "Q022",
            "Q052",
            "Q082",
            "Q112",
            "Q142",
            "Q172",
            "Q202",
            "Q232",
            "Q262",
            "Q292"
          ],
          "base_slots": [
            "D5-Q2"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM05"
          ],
          "signature": {
            "line_number": 632,
            "is_async": false,
            "parameters": [
              {
                "name": "pattern_str",
                "type": "str"
              }
            ],
            "return_type": "re.Pattern",
            "docstring": "Cache and compile regex patterns for performance."
          },
          "epistemological_foundation": {
            "paradigm": "PolicyTextProcessor analytical paradigm",
            "ontological_basis": "Analysis via PolicyTextProcessor.compile_pattern",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method compile_pattern implements structured analysis for D5-Q2"
            ],
            "justification": "This method contributes to D5-Q2 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "PolicyTextProcessor.compile_pattern algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute compile_pattern"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from compile_pattern"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use compile_pattern results for downstream analysis"
            ]
          }
        },
        "extract_contextual_window": {
          "canonical_abbreviation": "policytextprocessor",
          "provides": "policytextprocessor.extract_contextual_window",
          "role": "extract_contextual_window_extraction",
          "usage_count_in_300": 10,
          "questions": [
            "Q022",
            "Q052",
            "Q082",
            "Q112",
            "Q142",
            "Q172",
            "Q202",
            "Q232",
            "Q262",
            "Q292"
          ],
          "base_slots": [
            "D5-Q2"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM05"
          ],
          "signature": {
            "line_number": 615,
            "is_async": false,
            "parameters": [
              {
                "name": "text",
                "type": "str"
              },
              {
                "name": "match_position",
                "type": "int"
              },
              {
                "name": "window_size",
                "type": "int"
              }
            ],
            "return_type": "str",
            "docstring": "Extract semantically coherent context window around a match."
          },
          "epistemological_foundation": {
            "paradigm": "PolicyTextProcessor analytical paradigm",
            "ontological_basis": "Analysis via PolicyTextProcessor.extract_contextual_window",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method extract_contextual_window implements structured analysis for D5-Q2"
            ],
            "justification": "This method contributes to D5-Q2 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "PolicyTextProcessor.extract_contextual_window algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute extract_contextual_window"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from extract_contextual_window"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use extract_contextual_window results for downstream analysis"
            ]
          }
        }
      }
    },
    "ConfigLoader": {
      "file_path": "src/methods_dispensary/derek_beach.py",
      "line_number": 433,
      "class_docstring": "External configuration management with Pydantic schema validation",
      "total_usage_in_300_contracts": 40,
      "unique_questions": 10,
      "questions": [
        "Q029",
        "Q059",
        "Q089",
        "Q119",
        "Q149",
        "Q179",
        "Q209",
        "Q239",
        "Q269",
        "Q299"
      ],
      "base_slots": [
        "D6-Q4"
      ],
      "policy_areas": [
        "PA01",
        "PA02",
        "PA03",
        "PA04",
        "PA05",
        "PA06",
        "PA07",
        "PA08",
        "PA09",
        "PA10"
      ],
      "dimensions": [
        "DIM06"
      ],
      "methods": {
        "update_priors_from_feedback": {
          "canonical_abbreviation": "configloader",
          "provides": "configloader.update_priors_from_feedback",
          "role": "update_priors_from_feedback_execution",
          "usage_count_in_300": 10,
          "questions": [
            "Q029",
            "Q059",
            "Q089",
            "Q119",
            "Q149",
            "Q179",
            "Q209",
            "Q239",
            "Q269",
            "Q299"
          ],
          "base_slots": [
            "D6-Q4"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM06"
          ],
          "signature": {
            "line_number": 619,
            "is_async": false,
            "parameters": [
              {
                "name": "feedback_data",
                "type": "dict[str, Any]"
              }
            ],
            "return_type": "None",
            "docstring": "\n        Self-reflective loop: Update priors based on audit feedback\n        Implements frontier paradigm of learning from results\n\n        HARMONIC FRONT 4 ENHANCEMENT:\n        - Applies penalties to mechanism types with implementation_failure flags\n        - Heavily penalizes \"miracle\" mechanisms failing necessity/sufficiency tests\n        - Ensures mean mech_uncertainty decreases by ≥5% over iterations\n        "
          },
          "epistemological_foundation": {
            "paradigm": "ConfigLoader analytical paradigm",
            "ontological_basis": "Analysis via ConfigLoader.update_priors_from_feedback",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method update_priors_from_feedback implements structured analysis for D6-Q4"
            ],
            "justification": "This method contributes to D6-Q4 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "ConfigLoader.update_priors_from_feedback algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute update_priors_from_feedback"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from update_priors_from_feedback"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use update_priors_from_feedback results for downstream analysis"
            ]
          }
        },
        "check_uncertainty_reduction_criterion": {
          "canonical_abbreviation": "configloader",
          "provides": "configloader.check_uncertainty_reduction_criterion",
          "role": "check_uncertainty_reduction_criterion_execution",
          "usage_count_in_300": 10,
          "questions": [
            "Q029",
            "Q059",
            "Q089",
            "Q119",
            "Q149",
            "Q179",
            "Q209",
            "Q239",
            "Q269",
            "Q299"
          ],
          "base_slots": [
            "D6-Q4"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM06"
          ],
          "signature": {
            "line_number": 806,
            "is_async": false,
            "parameters": [
              {
                "name": "current_uncertainty",
                "type": "float"
              }
            ],
            "return_type": "dict[str, Any]",
            "docstring": "\n        Check if mean mechanism_type uncertainty has decreased ≥5% over 10 iterations\n\n        HARMONIC FRONT 4 QUALITY CRITERIA:\n        Success verified if mean mech_uncertainty decreases by ≥5% over 10 sequential PDM analyses\n        "
          },
          "epistemological_foundation": {
            "paradigm": "ConfigLoader analytical paradigm",
            "ontological_basis": "Analysis via ConfigLoader.check_uncertainty_reduction_criterion",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method check_uncertainty_reduction_criterion implements structured analysis for D6-Q4"
            ],
            "justification": "This method contributes to D6-Q4 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "ConfigLoader.check_uncertainty_reduction_criterion algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute check_uncertainty_reduction_criterion"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from check_uncertainty_reduction_criterion"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use check_uncertainty_reduction_criterion results for downstream analysis"
            ]
          }
        },
        "_save_prior_history": {
          "canonical_abbreviation": "configloader",
          "provides": "configloader.save_prior_history",
          "role": "_save_prior_history_execution",
          "usage_count_in_300": 10,
          "questions": [
            "Q029",
            "Q059",
            "Q089",
            "Q119",
            "Q149",
            "Q179",
            "Q209",
            "Q239",
            "Q269",
            "Q299"
          ],
          "base_slots": [
            "D6-Q4"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM06"
          ],
          "signature": {
            "line_number": 716,
            "is_async": false,
            "parameters": [
              {
                "name": "feedback_data",
                "type": "dict[str, Any] | None",
                "default": "None"
              },
              {
                "name": "uncertainty_reduction",
                "type": "float | None",
                "default": "None"
              }
            ],
            "return_type": "None",
            "docstring": "\n        Save prior history for learning across documents\n\n        HARMONIC FRONT 4 ENHANCEMENT:\n        - Tracks uncertainty reduction over iterations\n        - Records penalty applications and test failures\n        "
          },
          "epistemological_foundation": {
            "paradigm": "ConfigLoader analytical paradigm",
            "ontological_basis": "Analysis via ConfigLoader._save_prior_history",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _save_prior_history implements structured analysis for D6-Q4"
            ],
            "justification": "This method contributes to D6-Q4 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "ConfigLoader._save_prior_history algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _save_prior_history"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _save_prior_history"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _save_prior_history results for downstream analysis"
            ]
          }
        },
        "_load_uncertainty_history": {
          "canonical_abbreviation": "configloader",
          "provides": "configloader.load_uncertainty_history",
          "role": "_load_uncertainty_history_execution",
          "usage_count_in_300": 10,
          "questions": [
            "Q029",
            "Q059",
            "Q089",
            "Q119",
            "Q149",
            "Q179",
            "Q209",
            "Q239",
            "Q269",
            "Q299"
          ],
          "base_slots": [
            "D6-Q4"
          ],
          "policy_areas": [
            "PA01",
            "PA02",
            "PA03",
            "PA04",
            "PA05",
            "PA06",
            "PA07",
            "PA08",
            "PA09",
            "PA10"
          ],
          "dimensions": [
            "DIM06"
          ],
          "signature": {
            "line_number": 780,
            "is_async": false,
            "parameters": [],
            "return_type": "None",
            "docstring": "\n        Load historical uncertainty measurements\n\n        HARMONIC FRONT 4: Required for tracking ≥5% reduction over 10 iterations\n        "
          },
          "epistemological_foundation": {
            "paradigm": "ConfigLoader analytical paradigm",
            "ontological_basis": "Analysis via ConfigLoader._load_uncertainty_history",
            "epistemological_stance": "Empirical-analytical approach",
            "theoretical_framework": [
              "Method _load_uncertainty_history implements structured analysis for D6-Q4"
            ],
            "justification": "This method contributes to D6-Q4 analysis"
          },
          "technical_approach": {
            "method_type": "analytical_processing",
            "algorithm": "ConfigLoader._load_uncertainty_history algorithm",
            "steps": [
              {
                "step": 1,
                "description": "Execute _load_uncertainty_history"
              },
              {
                "step": 2,
                "description": "Process results"
              },
              {
                "step": 3,
                "description": "Return structured output"
              }
            ],
            "assumptions": [
              "Input data is preprocessed and valid"
            ],
            "limitations": [
              "Method-specific limitations apply"
            ],
            "complexity": "O(n) where n=input size"
          },
          "output_interpretation": {
            "output_structure": {
              "result": "Structured output from _load_uncertainty_history"
            },
            "interpretation_guide": {
              "high_confidence": "≥0.8: Strong evidence",
              "medium_confidence": "0.5-0.79: Moderate evidence",
              "low_confidence": "<0.5: Weak evidence"
            },
            "actionable_insights": [
              "Use _load_uncertainty_history results for downstream analysis"
            ]
          }
        }
      }
    }
  }
}