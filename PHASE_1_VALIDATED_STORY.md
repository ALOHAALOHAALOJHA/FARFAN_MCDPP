# ğŸ“„ PHASE 1: SPC INGESTION (Smart Policy Chunks)
## VALIDATED NARRATIVE - Code-Audited Version

---

## ğŸ›ï¸ Constitutional Mandate and Purpose

**WEIGHT: 10000 - NON-NEGOTIABLE**

Phase 1 has the **immutable purpose** of transforming the input PDF document into **exactly 60 Smart Policy Chunks (SPC)**, each semantically enriched and canonically tagged with **PAÃ—DIM metadata** (10 Policy Areas Ã— 6 Dimensions).

**Failure Modes:**
- Chunk count â‰  60 â†’ IMMEDIATE PIPELINE TERMINATION
- Missing PA/DIM metadata â†’ IMMEDIATE PIPELINE TERMINATION  
- Provenance completeness < 0.8 â†’ IMMEDIATE PIPELINE TERMINATION
- Structural consistency < 0.85 â†’ IMMEDIATE PIPELINE TERMINATION

**No fallbacks. No partial success. No exceptions.**

---

## ğŸ“‚ Core Implementation Files (Code-Validated)

| File | Role | Critical Components |
|------|------|-------------------|
| `/src/farfan_pipeline/core/phases/phase1_spc_ingestion_full.py` | **Execution Contract** | `Phase1SPCIngestionFullContract`, `PADimGridSpecification`, `Phase1FailureHandler` |
| `/src/farfan_pipeline/processing/spc_ingestion.py` | **SOTA Processing Engine** | `StrategicChunkingSystem` with canonical SOTA producers |
| `/src/farfan_pipeline/core/phases/phase1_models.py` | **Data Models** | `LanguageData`, `PreprocessedDoc`, `StructureData`, `KnowledgeGraph`, `SmartChunk` |
| `/src/farfan_pipeline/processing/cpp_ingestion/models.py` | **Output Models** | `CanonPolicyPackage`, `ChunkGraph`, `QualityMetrics`, `IntegrityIndex` |

---

## ğŸ”¬ Mandatory Subphases: SP0 - SP15 (16 Steps)

The contract enforces **16 mandatory subphases** executed in strict linear order. Each subphase:
- Records timestamp + SHA256 hash in execution trace
- Stores output in `subphase_results` dict
- Fails fast on any exception with full context

### SP0: Language Detection & Model Selection
**Weight: 900**

```python
def _execute_sp0_language_detection(canonical_input: CanonicalInput) -> LanguageData
```

**Purpose:** Detect primary document language and select appropriate NLP models.

**Implementation:**
- Uses `langdetect` if available, otherwise defaults to Spanish
- Returns `LanguageData` with:
  - `primary_language`: ISO code (e.g., "ES")
  - `secondary_languages`: List of detected alternatives
  - `confidence_scores`: Dict mapping language â†’ confidence
  - `detection_method`: Method used for detection

**Validation:**
- `primary_language` must not be None
- Confidence scores must sum close to 1.0
- `_sealed` flag must be True (immutability)

---

### SP1: Advanced Preprocessing  
**Weight: 950**

```python
def _execute_sp1_preprocessing(canonical_input: CanonicalInput, lang_data: LanguageData) -> PreprocessedDoc
```

**Purpose:** Normalize text to canonical form and prepare linguistic structures.

**Implementation:**
- Unicode normalization (NFC)
- Tokenization (whitespace-based baseline)
- Sentence segmentation
- Paragraph extraction
- Compute SHA256 hash of normalized text

**Output:**
```python
PreprocessedDoc(
    tokens: List[str],
    sentences: List[str],
    paragraphs: List[str],
    normalized_text: str,
    _hash: str  # SHA256 of normalized_text
)
```

**Validation:**
- `normalized_text` must not be empty
- `_hash` must be valid SHA256 hex string
- At least 10 sentences required for meaningful analysis

---

### SP2: Structural Analysis & Hierarchy Extraction
**Weight: 950**

```python
def _execute_sp2_structural(preprocessed: PreprocessedDoc) -> StructureData
```

**Purpose:** Extract document structure (sections, hierarchy) for contextual chunking.

**Implementation:**
- Identify section headers (regex + heuristics)
- Build hierarchy tree (parent-child relationships)
- Map paragraphs to sections
- **Code Reality:** Current implementation provides placeholder structure

**Output:**
```python
StructureData(
    sections: List[str],  # Section identifiers
    hierarchy: Dict[str, Optional[str]],  # section -> parent mapping
    paragraph_mapping: Dict[int, str]  # paragraph_idx -> section mapping
)
```

**Validation:**
- At least 1 section identified
- All paragraphs mapped to a section
- Hierarchy must be acyclic (no circular parent references)

---

### SP3: Topic Modeling & Global KG Construction
**Weight: 980**

```python
def _execute_sp3_knowledge_graph(preprocessed: PreprocessedDoc, structure: StructureData) -> KnowledgeGraph
```

**Purpose:** Build preliminary knowledge graph and identify global topics.

**Implementation:**
- LDA topic modeling (via `TopicModeler`)
- NER entity extraction (via SpaCy)
- NetworkX graph construction
- **Integration with:** `StrategicChunkingSystem.kg_builder`

**Output:**
```python
KnowledgeGraph(
    nodes: List[KGNode],  # Entities, concepts, topics
    edges: List[KGEdge]   # Relationships between nodes
)
```

**Validation:**
- At least 5 nodes extracted
- Node types include: `entity`, `concept`, `topic`
- Edges have valid source/target node references

---

### SP4: PAÃ—DIM Segmentation (60 Chunks) ğŸš¨
**Weight: 10000 - CRITICAL CHECKPOINT**

```python
def _execute_sp4_segmentation(preprocessed, structure, kg) -> List[Chunk]
```

**Purpose:** Generate **exactly 60 chunks** aligned with 10 PA Ã— 6 DIM grid.

**Implementation:**
```python
chunks = []
idx = 0
for pa in PADimGridSpecification.POLICY_AREAS:  # PA01-PA10
    for dim in PADimGridSpecification.DIMENSIONS:  # DIM01-DIM06
        chunks.append(Chunk(
            chunk_id=f"{pa}_{dim}_CHUNK",
            policy_area_id=pa,
            dimension_id=dim,
            chunk_index=idx,
            signal_tags=[],
            signal_scores={}
        ))
        idx += 1
return chunks  # len(chunks) == 60 GUARANTEED
```

**HARD VALIDATION:**
```python
assert len(chunks) == 60, f"FATAL: Got {len(chunks)} chunks, need EXACTLY 60"
```

**PA Codes (Immutable):**
```python
POLICY_AREAS = ("PA01", "PA02", "PA03", "PA04", "PA05", 
                "PA06", "PA07", "PA08", "PA09", "PA10")
```

**DIM Codes (Immutable):**
```python
DIMENSIONS = ("DIM01", "DIM02", "DIM03", "DIM04", "DIM05", "DIM06")
```

**Failure:** If count â‰  60, `Phase1FatalError` raised â†’ Pipeline termination.

---

### SP5: Causal Chain Extraction
**Weight: 970**

```python
def _execute_sp5_causal_extraction(chunks: List[Chunk]) -> CausalChains
```

**Purpose:** Extract causal relationships within each chunk.

**Implementation:**
- For each chunk, initialize `chunk.causal_graph = CausalGraph()`
- Uses `CausalChainAnalyzer` from `StrategicChunkingSystem`
- Detects causal markers: "porque", "por lo tanto", "resulta en", etc.

**Output:**
- `CausalChains` object (aggregate container)
- Each chunk has `causal_graph` populated with nodes/edges

---

### SP6: Causal Integration
**Weight: 970**

```python
def _execute_sp6_causal_integration(chunks, chains) -> IntegratedCausal
```

**Purpose:** Integrate per-chunk causal chains into global causal graph.

**Implementation:**
- Merge individual `causal_graph` instances
- Identify cross-chunk causal dependencies
- Build `IntegratedCausal` structure

**Output:**
```python
IntegratedCausal(
    global_graph: CausalGraph,  # Merged graph
    cross_chunk_links: List[Tuple[str, str]]  # Inter-chunk causal edges
)
```

---

### SP7: Deep Argumentative Analysis
**Weight: 960**

```python
def _execute_sp7_arguments(chunks, integrated) -> Arguments
```

**Purpose:** Analyze Toulmin argumentative structure (claims, evidence, warrants).

**Implementation:**
- For each chunk, populate `chunk.arguments = {}`
- Uses `ArgumentAnalyzer` from `StrategicChunkingSystem`
- Identifies:
  - **Claims:** Assertions made
  - **Evidence:** Supporting data
  - **Warrants:** Logical bridges
  - **Backing:** Further support
  - **Rebuttals:** Counterarguments

**Output:**
- `Arguments` object (aggregate)
- Each chunk has `arguments` dict with Toulmin components

---

### SP8: Temporal and Sequential Analysis
**Weight: 960**

```python
def _execute_sp8_temporal(chunks, integrated) -> Temporal
```

**Purpose:** Identify temporal markers and sequential dependencies.

**Implementation:**
- For each chunk, populate `chunk.temporal_markers = {}`
- Detects:
  - Date references (e.g., "2025", "enero 2024")
  - Duration expressions ("durante 3 aÃ±os")
  - Sequential markers ("primero", "luego", "finalmente")
- Uses `TemporalAnalyzer` from `StrategicChunkingSystem`

**Output:**
- `Temporal` object (aggregate)
- Each chunk has `temporal_markers` dict

---

### SP9: Discourse and Rhetorical Analysis
**Weight: 950**

```python
def _execute_sp9_discourse(chunks, arguments) -> Discourse
```

**Purpose:** Classify discourse mode and rhetorical strategies.

**Implementation:**
- For each chunk, set `chunk.discourse_mode = "narrative"` (baseline)
- Identifies:
  - Narrative vs. Technical discourse
  - Rhetorical moves (persuasion, exposition, argumentation)
  - Discourse coherence relations
- Uses `DiscourseAnalyzer` from `StrategicChunkingSystem`

**Output:**
- `Discourse` object (aggregate)
- Each chunk has `discourse_mode` string

---

### SP10: Multi-scale Strategic Integration
**Weight: 990**

```python
def _execute_sp10_strategic(chunks, integrated, arguments, temporal, discourse) -> Strategic
```

**Purpose:** Combine all analyses and assign strategic weights.

**Implementation:**
- For each chunk, set `chunk.strategic_rank = 0` (placeholder)
- Integrates:
  - Causal importance
  - Argumentative strength
  - Temporal criticality
  - Discourse coherence
- Uses `StrategicIntegrator` from `StrategicChunkingSystem`

**Output:**
- `Strategic` object (aggregate)
- Each chunk has `strategic_rank` numeric value

---

### SP11: Smart Chunk Generation ğŸš¨
**Weight: 10000 - CRITICAL TRANSFORMATION**

```python
def _execute_sp11_smart_chunks(chunks, enrichments) -> List[SmartChunk]
```

**Purpose:** Transform `Chunk` â†’ `SmartChunk` with all metadata enrichments.

**Implementation:**
```python
smart_chunks = []
for chunk in chunks:
    smart_chunks.append(SmartChunk(
        policy_area_id=chunk.policy_area_id,
        dimension_id=chunk.dimension_id,
        chunk_index=chunk.chunk_index,
        causal_graph=chunk.causal_graph or CausalGraph(),
        temporal_markers=chunk.temporal_markers or {},
        arguments=chunk.arguments or {},
        discourse_mode=chunk.discourse_mode,
        strategic_rank=0,  # Updated in SP15
        irrigation_links=[],  # Updated in SP12
        signal_tags=chunk.signal_tags,
        signal_scores=chunk.signal_scores,
        signal_version="2.0"  # Canonical signal version
    ))
return smart_chunks
```

**HARD VALIDATION:**
```python
self._assert_smart_chunk_invariants(smart_chunks)
# Calls PADimGridSpecification.validate_chunk_set()
```

**Required Fields (Per Chunk):**
- `policy_area_id` âˆˆ {PA01..PA10}
- `dimension_id` âˆˆ {DIM01..DIM06}
- `chunk_index` âˆˆ [0, 59]
- `causal_graph` (not None)
- `temporal_markers` (not None)
- `arguments` (not None)
- `discourse_mode` (not None)
- `strategic_rank` (not None)
- `irrigation_links` (not None)
- `signal_tags` (not None)
- `signal_scores` (not None)
- `signal_version` (not None)

**Failure:** If any field missing or invalid â†’ `Phase1FatalError`

---

### SP12: Inter-Chunk Relationship Enrichment
**Weight: 980**

```python
def _execute_sp12_irrigation(chunks: List[SmartChunk]) -> List[SmartChunk]
```

**Purpose:** Identify and enrich cross-references between chunks.

**Implementation:**
- For each chunk, populate `chunk.irrigation_links = []`
- Detects:
  - Semantic similarity (via embeddings)
  - Causal dependencies (via causal graph)
  - Sequential references
  - Thematic overlap

**Output:**
- Same `List[SmartChunk]` with `irrigation_links` populated
- Links format: `List[Tuple[chunk_id, link_type, strength]]`

---

### SP13: Strategic Integrity Validation ğŸš¨
**Weight: 10000 - CRITICAL GATE**

```python
def _execute_sp13_validation(chunks: List[SmartChunk]) -> ValidationResult
```

**Purpose:** Validate structural and semantic integrity of chunk set.

**Implementation:**
```python
return ValidationResult(
    status="VALID",  # or "INVALID"
    chunk_count=len(chunks),
    pa_dim_coverage="COMPLETE",  # or "INCOMPLETE"
    violations=[]  # List of validation errors if any
)
```

**HARD VALIDATION:**
```python
self._assert_validation_pass(validated)
# Checks: validated.status == "VALID"
```

**Validation Checks:**
1. Chunk count == 60
2. All PAÃ—DIM combinations present
3. No duplicate (PA, DIM) pairs
4. All required metadata fields present
5. Causal graphs well-formed (no dangling edges)
6. Temporal coherence > threshold
7. Strategic ranks assigned

**Failure:** If status â‰  "VALID" â†’ `Phase1FatalError`

---

### SP14: Intelligent Deduplication
**Weight: 970**

```python
def _execute_sp14_deduplication(chunks: List[SmartChunk]) -> List[SmartChunk]
```

**Purpose:** Ensure no redundant content across chunks.

**Implementation:**
- Compute content similarity matrix (cosine on embeddings)
- Identify near-duplicates (similarity > 0.95)
- Merge or flag duplicates
- **Current Reality:** No-op (unique chunks by design)

**HARD VALIDATION:**
```python
self._assert_chunk_count(deduplicated, 60)
```

**Failure:** If len â‰  60 after deduplication â†’ `Phase1FatalError`

---

### SP15: Strategic Importance Ranking
**Weight: 990**

```python
def _execute_sp15_ranking(chunks: List[SmartChunk]) -> List[SmartChunk]
```

**Purpose:** Assign final strategic importance scores to each chunk.

**Implementation:**
```python
for idx, chunk in enumerate(chunks):
    chunk.strategic_rank = idx  # Simple ordinal ranking
    # In production: composite score from causal, temporal, argumentative weights
return chunks
```

**Output:**
- Same `List[SmartChunk]` with `strategic_rank` updated
- Ranking scale: 0 (highest) to 59 (lowest)

---

## ğŸ“¦ Final Output: CanonPolicyPackage (CPP)

### Construction Process

```python
def _construct_cpp_with_verification(ranked: List[SmartChunk]) -> CanonPolicyPackage
```

**Transformation:**
1. Convert `SmartChunk` â†’ Legacy `Chunk` model (for backward compatibility)
2. Build `ChunkGraph` from converted chunks
3. Compute `QualityMetrics`:
   - `provenance_completeness` â‰¥ 0.8
   - `structural_consistency` â‰¥ 0.85
   - `semantic_coherence` (average across chunks)
4. Generate `IntegrityIndex` (hash-based verification)
5. Assemble `CanonPolicyPackage`

**CPP Structure:**
```python
CanonPolicyPackage(
    schema_version="SPC-2025.1",
    chunk_graph=ChunkGraph(chunks={...}),  # 60 chunks
    policy_manifest=PolicyManifest(...),
    quality_metrics=QualityMetrics(...),
    integrity_index=IntegrityIndex(...),
    metadata={
        "execution_trace": [(sp_id, timestamp, hash), ...],  # 16 entries
        "subphase_results": {sp_num: output, ...}  # 16 results
    }
)
```

### Post-Construction Verification

```python
def _verify_all_postconditions(cpp: CanonPolicyPackage)
```

**Invariants (Weight: 10000):**
1. `len(cpp.chunk_graph.chunks) == 60`
2. `len(cpp.metadata["execution_trace"]) == 16`
3. `len(cpp.metadata["subphase_results"]) == 16`
4. All chunks have valid `policy_area_id` and `dimension_id`
5. All 60 PAÃ—DIM combinations present
6. `quality_metrics.provenance_completeness >= 0.8`
7. `quality_metrics.structural_consistency >= 0.85`

**Failure Handling:**
```python
if not Phase1FailureHandler.validate_final_state(cpp):
    raise Phase1FatalError("CPP final state validation failed")
```

---

## ğŸ” Execution Trace & Provenance

Every subphase records:
```python
(subphase_id, timestamp, output_hash)
```

**Example:**
```python
[
    ("SP0", "2025-12-07T08:00:00.000Z", "a1b2c3..."),
    ("SP1", "2025-12-07T08:00:05.123Z", "d4e5f6..."),
    ...,
    ("SP15", "2025-12-07T08:02:30.456Z", "x9y8z7...")
]
```

**Determinism Guarantee:**
- Same input PDF + questionnaire â†’ Same output hashes
- Enables regression detection
- Supports audit trail for compliance

---

## âš ï¸ Failure Modes and Recovery

### Fatal Errors (No Recovery)

```python
class Phase1FatalError(Exception):
    """Raised when Phase 1 encounters an unrecoverable error."""
```

**Triggers:**
1. Chunk count â‰  60 at any checkpoint
2. Missing required metadata fields
3. PA/DIM validation failure
4. Quality metrics below threshold
5. Any subphase exception

**Handling:**
```python
Phase1FailureHandler.handle_subphase_failure(sp_num, error)
```

**Actions:**
1. Log to all channels (file + console)
2. Write `phase1_error_manifest.json`
3. Raise `Phase1FatalError` with full context
4. **Pipeline terminates immediately**

### Error Manifest Structure

```json
{
  "phase": "PHASE_1_SPC_INGESTION",
  "subphase": "SP4",
  "error_type": "AssertionError",
  "error_message": "FATAL: Got 59 chunks, need EXACTLY 60",
  "timestamp": "2025-12-07T08:01:15.789Z",
  "fatal": true,
  "recovery_possible": false
}
```

---

## ğŸ¯ Integration with StrategicChunkingSystem

### Canonical SOTA Producers

The Phase 1 contract delegates heavy lifting to `StrategicChunkingSystem`, which integrates:

**1. EmbeddingPolicyProducer**
- BGE-M3 multilingual embeddings
- Cross-encoder reranking
- Bayesian numerical evaluation

**2. SemanticChunkingProducer**
- PDM structure-aware chunking
- Direct embed_text/embed_batch APIs

**3. PolicyProcessor (via `create_policy_processor()`)**
- Canonical PDQ/dimension evidence extraction

**Initialization:**
```python
self._spc_embed = EmbeddingPolicyProducer()
self._spc_sem = SemanticChunkingProducer()
self._spc_policy = create_policy_processor()
```

### Specialized Analyzers (Innovation Layer)

No canonical equivalents; unique to SPC system:

- `CausalChainAnalyzer`: Causal relationship extraction
- `KnowledgeGraphBuilder`: NetworkX graph construction
- `TopicModeler`: LDA topic modeling
- `ArgumentAnalyzer`: Toulmin argument structure
- `TemporalAnalyzer`: Temporal dynamics
- `DiscourseAnalyzer`: Discourse markers
- `StrategicIntegrator`: Multi-scale integration

---

## ğŸ“Š Quality Metrics & Thresholds

### Required Minimums (Weight: 10000)

| Metric | Threshold | Fatal if Below? |
|--------|-----------|-----------------|
| Chunk Count | == 60 | âœ… YES |
| Provenance Completeness | â‰¥ 0.8 | âœ… YES |
| Structural Consistency | â‰¥ 0.85 | âœ… YES |
| PAÃ—DIM Coverage | 100% (60/60) | âœ… YES |
| Execution Trace Entries | == 16 | âœ… YES |
| Subphase Results Count | == 16 | âœ… YES |

### Quality Computation

```python
quality_metrics = QualityMetrics(
    provenance_completeness=compute_provenance_score(chunks),
    structural_consistency=compute_structure_score(chunks),
    semantic_coherence=compute_semantic_score(chunks),
    causal_integrity=compute_causal_score(chunks),
    temporal_coherence=compute_temporal_score(chunks)
)
```

---

## ğŸ”„ Relationship to Phase 0

**Input Dependency:**
Phase 1 receives `CanonicalInput` from Phase 0, which guarantees:
- Valid PDF path (exists, readable)
- Valid questionnaire path (exists, readable)
- SHA256 hashes computed
- Boot checks passed
- Runtime configuration loaded

**Pre-Execution Validation:**
```python
self._validate_canonical_input(canonical_input)
assert canonical_input.validation_passed, "Input validation failed"
```

**Failure:** If Phase 0 didn't complete successfully, Phase 1 cannot start.

---

## ğŸ”„ Relationship to Phase 2

**Output Contract:**
Phase 1 produces `CanonPolicyPackage` which Phase 2 consumes for:
- Micro-question routing (PAÃ—DIM â†’ Executor mapping)
- Evidence assembly (causal, argumentative, temporal)
- Signal enrichment (via enriched signal packs)

**Adapter Pattern:**
```python
from farfan_pipeline.core.phases.phase1_to_phase2_adapter import Phase1ToPhase2Adapter

adapter = Phase1ToPhase2Adapter()
phase2_input = adapter.transform(canon_policy_package)
```

---

## ğŸ Summary: The Phase 1 Contract

### What Phase 1 MUST Deliver

1. **Exactly 60 chunks** (no more, no less)
2. **Complete PAÃ—DIM coverage** (all 60 combinations)
3. **Rich metadata** (causal, temporal, argumentative, discourse)
4. **Provenance tracking** (page numbers, sections, bbox)
5. **Quality certification** (metrics above thresholds)
6. **Integrity verification** (hashes, traces, auditable)
7. **Canonical format** (`CanonPolicyPackage` with all fields)

### What Phase 1 MUST NOT Do

1. Generate < 60 or > 60 chunks
2. Skip any of the 16 mandatory subphases
3. Produce chunks without PA/DIM metadata
4. Return incomplete `CanonPolicyPackage`
5. Silently fail or degrade gracefully
6. Modify input document
7. Cache intermediate results across runs (determinism)

### Execution Guarantees

âœ… **Deterministic:** Same input â†’ Same output (with fixed seed)
âœ… **Traceable:** Full execution trace with timestamps + hashes
âœ… **Auditable:** Every decision documented in metadata
âœ… **Fail-Fast:** Immediate termination on violation
âœ… **Type-Safe:** Strong typing with validation decorators
âœ… **Immutable:** Output is frozen after construction

---

## ğŸ”¬ Code-Validated Architecture Diagram

```
Phase 0 Output (CanonicalInput)
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Phase1SPCIngestionFullContract              â”‚
â”‚  (/core/phases/phase1_spc_ingestion_full.py)â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  SP0: Language Detection        (Weight: 900) â”‚
â”‚  SP1: Preprocessing             (Weight: 950) â”‚
â”‚  SP2: Structural Analysis       (Weight: 950) â”‚
â”‚  SP3: Knowledge Graph           (Weight: 980) â”‚
â”‚  SP4: PAÃ—DIM Segmentation       (Weight:10000)â”‚ â† 60 chunks
â”‚  SP5: Causal Extraction         (Weight: 970) â”‚
â”‚  SP6: Causal Integration        (Weight: 970) â”‚
â”‚  SP7: Argumentative Analysis    (Weight: 960) â”‚
â”‚  SP8: Temporal Analysis         (Weight: 960) â”‚
â”‚  SP9: Discourse Analysis        (Weight: 950) â”‚
â”‚  SP10: Strategic Integration    (Weight: 990) â”‚
â”‚  SP11: Smart Chunk Generation   (Weight:10000)â”‚ â† SmartChunks
â”‚  SP12: Inter-Chunk Enrichment  (Weight: 980) â”‚
â”‚  SP13: Integrity Validation     (Weight:10000)â”‚ â† GATE
â”‚  SP14: Deduplication            (Weight: 970) â”‚
â”‚  SP15: Strategic Ranking        (Weight: 990) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
  CanonPolicyPackage (CPP)
        â†“
     Phase 2 (Micro-Question Routing)
```

### SOTA Integration Layer

```
StrategicChunkingSystem (/processing/spc_ingestion.py)
    â”œâ”€ EmbeddingPolicyProducer (BGE-M3 + Cross-Encoder)
    â”œâ”€ SemanticChunkingProducer (PDM-Aware Chunking)
    â”œâ”€ PolicyProcessor (Canonical Evidence Extraction)
    â””â”€ Specialized Analyzers:
        â”œâ”€ CausalChainAnalyzer
        â”œâ”€ KnowledgeGraphBuilder
        â”œâ”€ ArgumentAnalyzer (Toulmin)
        â”œâ”€ TemporalAnalyzer
        â”œâ”€ DiscourseAnalyzer
        â””â”€ StrategicIntegrator
```

---

## âœ… Validation Checklist (For Orchestrator Integration)

- [ ] Phase 1 receives `CanonicalInput` from Phase 0
- [ ] Phase 1 contract instantiated: `Phase1SPCIngestionFullContract()`
- [ ] `run(canonical_input)` called with validated input
- [ ] All 16 subphases execute in order
- [ ] Execution trace has 16 entries
- [ ] Output is `CanonPolicyPackage` instance
- [ ] Chunk count == 60 verified
- [ ] PAÃ—DIM coverage verified (60/60)
- [ ] Quality metrics above thresholds
- [ ] Provenance completeness â‰¥ 0.8
- [ ] Structural consistency â‰¥ 0.85
- [ ] Metadata includes execution trace
- [ ] Metadata includes subphase results
- [ ] No fatal errors logged
- [ ] CPP is JSON-serializable
- [ ] CPP is passed to Phase 2 adapter

---

**END OF VALIDATED NARRATIVE - PHASE 1**

*This document is code-audited and reflects the actual implementation as of 2025-12-07.*
*Any discrepancies between this document and code should be resolved in favor of code.*
