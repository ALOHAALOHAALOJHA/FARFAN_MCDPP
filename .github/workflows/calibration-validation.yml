name: Calibration System Validation

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
  workflow_dispatch:
  schedule:
    - cron: '0 3 * * *'

env:
  PYTHON_VERSION: '3.12'

jobs:
  calibration-validation:
    name: All-or-Nothing Calibration Quality Gate
    runs-on: ubuntu-latest
    timeout-minutes: 30
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        
      - name: Set up Python ${{ env.PYTHON_VERSION }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
          
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip setuptools wheel
          pip install -r requirements.txt
          pip install -r requirements-dev.txt || pip install pytest pytest-cov pytest-timeout
          pip install -e .
          
      - name: Verify calibration system files exist
        run: |
          echo "Checking for required calibration files..."
          test -f system/config/calibration/intrinsic_calibration.json || echo "WARNING: intrinsic_calibration.json not found"
          test -f src/farfan_pipeline/core/orchestrator/executors_methods.json || (echo "ERROR: executors_methods.json not found" && exit 1)
          test -f config/intrinsic_calibration_rubric.json || echo "WARNING: calibration rubric not found"
          
      - name: Run Test 1 - Inventory Consistency
        id: test1
        run: |
          python -m pytest tests/calibration_system/test_inventory_consistency.py -v \
            --tb=short \
            --color=yes \
            --junit-xml=test-results/test1-inventory.xml
        continue-on-error: true
        
      - name: Run Test 2 - Layer Correctness
        id: test2
        run: |
          python -m pytest tests/calibration_system/test_layer_correctness.py -v \
            --tb=short \
            --color=yes \
            --junit-xml=test-results/test2-layers.xml
        continue-on-error: true
        
      - name: Run Test 3 - Intrinsic Coverage
        id: test3
        run: |
          python -m pytest tests/calibration_system/test_intrinsic_coverage.py -v \
            --tb=short \
            --color=yes \
            --junit-xml=test-results/test3-coverage.xml
        continue-on-error: true
        
      - name: Run Test 4 - AST Extraction Accuracy
        id: test4
        run: |
          python -m pytest tests/calibration_system/test_ast_extraction_accuracy.py -v \
            --tb=short \
            --color=yes \
            --junit-xml=test-results/test4-ast.xml
        continue-on-error: true
        
      - name: Run Test 5 - Orchestrator Runtime
        id: test5
        run: |
          python -m pytest tests/calibration_system/test_orchestrator_runtime.py -v \
            --tb=short \
            --color=yes \
            --junit-xml=test-results/test5-runtime.xml
        continue-on-error: true
        
      - name: Run Test 6 - No Hardcoded Calibrations
        id: test6
        run: |
          python -m pytest tests/calibration_system/test_no_hardcoded_calibrations.py -v \
            --tb=short \
            --color=yes \
            --junit-xml=test-results/test6-hardcoded.xml
        continue-on-error: true
        
      - name: Run Test 7 - Performance Benchmarks
        id: test7
        run: |
          python -m pytest tests/calibration_system/test_performance_benchmarks.py -v \
            --tb=short \
            --color=yes \
            --junit-xml=test-results/test7-performance.xml
        continue-on-error: true
        
      - name: Upload test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: calibration-test-results
          path: test-results/
          retention-days: 30
          
      - name: Generate failure report
        if: always()
        run: |
          python tests/calibration_system/generate_failure_report.py \
            --test1 ${{ steps.test1.outcome }} \
            --test2 ${{ steps.test2.outcome }} \
            --test3 ${{ steps.test3.outcome }} \
            --test4 ${{ steps.test4.outcome }} \
            --test5 ${{ steps.test5.outcome }} \
            --test6 ${{ steps.test6.outcome }} \
            --test7 ${{ steps.test7.outcome }} \
            --output calibration_system_failure_report.md
            
      - name: Display failure report
        if: always()
        run: |
          if [ -f calibration_system_failure_report.md ]; then
            cat calibration_system_failure_report.md
          fi
          
      - name: Upload failure report
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: calibration-failure-report
          path: calibration_system_failure_report.md
          retention-days: 90
          
      - name: Check all tests passed
        if: always()
        run: |
          if [ "${{ steps.test1.outcome }}" != "success" ] || \
             [ "${{ steps.test2.outcome }}" != "success" ] || \
             [ "${{ steps.test3.outcome }}" != "success" ] || \
             [ "${{ steps.test4.outcome }}" != "success" ] || \
             [ "${{ steps.test5.outcome }}" != "success" ] || \
             [ "${{ steps.test6.outcome }}" != "success" ] || \
             [ "${{ steps.test7.outcome }}" != "success" ]; then
            echo "❌ CALIBRATION SYSTEM NOT READY FOR PRODUCTION"
            echo "One or more validation tests failed."
            echo "Review the failure report for details."
            exit 1
          else
            echo "✅ ALL CALIBRATION VALIDATION TESTS PASSED"
            echo "System is ready for production."
          fi
          
      - name: Comment PR with status
        if: github.event_name == 'pull_request' && always()
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            let report = '# Calibration System Validation Results\n\n';
            
            const tests = {
              'Test 1 - Inventory Consistency': '${{ steps.test1.outcome }}',
              'Test 2 - Layer Correctness': '${{ steps.test2.outcome }}',
              'Test 3 - Intrinsic Coverage': '${{ steps.test3.outcome }}',
              'Test 4 - AST Extraction Accuracy': '${{ steps.test4.outcome }}',
              'Test 5 - Orchestrator Runtime': '${{ steps.test5.outcome }}',
              'Test 6 - No Hardcoded Calibrations': '${{ steps.test6.outcome }}',
              'Test 7 - Performance Benchmarks': '${{ steps.test7.outcome }}'
            };
            
            let allPassed = true;
            for (const [name, outcome] of Object.entries(tests)) {
              const icon = outcome === 'success' ? '✅' : '❌';
              report += `${icon} ${name}: **${outcome}**\n`;
              if (outcome !== 'success') allPassed = false;
            }
            
            report += '\n## Status\n\n';
            if (allPassed) {
              report += '✅ **ALL TESTS PASSED** - System ready for production\n';
            } else {
              report += '❌ **SYSTEM NOT READY FOR PRODUCTION**\n\n';
              report += 'One or more validation tests failed. Review the failure report artifact for details.\n';
            }
            
            if (fs.existsSync('calibration_system_failure_report.md')) {
              const failureReport = fs.readFileSync('calibration_system_failure_report.md', 'utf8');
              report += '\n## Failure Details\n\n';
              report += failureReport;
            }
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: report
            });
            
      - name: Block merge if tests failed
        if: always()
        run: |
          if [ "${{ steps.test1.outcome }}" != "success" ] || \
             [ "${{ steps.test2.outcome }}" != "success" ] || \
             [ "${{ steps.test3.outcome }}" != "success" ] || \
             [ "${{ steps.test4.outcome }}" != "success" ] || \
             [ "${{ steps.test5.outcome }}" != "success" ] || \
             [ "${{ steps.test6.outcome }}" != "success" ] || \
             [ "${{ steps.test7.outcome }}" != "success" ]; then
            echo "::error::Calibration validation failed - blocking merge"
            exit 1
          fi
