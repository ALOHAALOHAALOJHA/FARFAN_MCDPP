# F.A.R.F.A.N: Framework for Advanced Retrieval and Forensic Analysis of Administrative Narratives

> **Canonical Documentation Artifact**
> *Generated by Deterministic Documentation Engine*

## 1. Core Doctrine

The F.A.R.F.A.N. framework operates under the strict epistemological doctrine of SIN_CARRETA (Sistema de Integridad No-Compensable para Análisis de Reproducibilidad, Rastreabilidad y Trazabilidad Absoluta), which mandates that policy analysis be treated as a deterministic computational process rather than an interpretive art. This doctrine enforces a non-compensable architecture where excellence in higher-level rhetoric cannot mask failures in lower-level structural validity; if a territorial development plan fails the pre-execution gatekeeper (Phase 0) due to format corruption or missing cryptographic signatures, it is rejected in its entirety regardless of its content. Every analytical conclusion produced by the pipeline must be traceable back to specific token ranges in the source PDF through a provenance map, ensuring that no claim exists without empirical evidence. Furthermore, the system enforces immutable calibration, freezing all weights, thresholds, and analytical methods before execution begins to prevent post-hoc adjustment of criteria to favor specific outcomes.

## 2. System Architecture

The pipeline is structured as a directed acyclic graph of eleven distinct phases (0 through 10), each governed by a formal input-output contract that prohibits side effects and ensures type safety across transitions. This linear flow is augmented by the Signal Irrigation System (SISAS), a cross-cutting 'nervous system' that injects domain-specific heuristics and pattern recognition signals into the execution context without hard-coding them into the logic. The architecture decouples the 'what' (the question) from the 'how' (the method) through a Method Dispensary pattern, where 240+ specialized analyzer classes are dynamically bound to execution contracts at runtime. All intermediate evidence, from raw text extraction to complex causal inferences, is stored in the Evidence Nexus, a persistent graph database that prevents orphan conclusions and enables deep auditability of the reasoning chain.

## 3. Constitutional Foundations

### Determinism & Reproducibility
To address the reproducibility crisis in public policy evaluation, the framework implements a kernel-level deterministic execution model that guarantees bitwise-identical outputs given identical inputs, regardless of the underlying hardware or operating system. This is achieved through a hierarchical cryptographic seed derivation strategy where a single master run identifier is hashed using BLAKE3 to derive independent seeds for every subsystem, including Python's random module, NumPy, and PyTorch. The system actively fights environmental non-determinism by enforcing strict resource limits via setrlimit, mandating UTC timestamps, and requiring pinned dependencies with hash verification. Any execution that deviates from the canonical execution trace, even by a single floating-point bit, is flagged as a determinism violation, ensuring that the 'truth' of a policy evaluation is mathematically fixed rather than environmentally contingent.

### Phase 0: The Constitutional Layer
Phase 0 serves as the constitutional validation layer, executing a seven-stage boot sequence that verifies the integrity of the runtime environment before any user data is processed. This phase enforces four invariants: determinism of the random number generators, strict kernel-level resource boundedness (memory and CPU), immutability of the configuration, and completeness of the input validation. The system employs a multi-tier structural integrity validator that checks for dependency resolution, signature verification, and content hashing, rejecting any execution context that shows signs of 'drift' or tampering. By treating validation as a first-class architectural phase with fail-stop semantics, Phase 0 prevents the propagation of errors into the analytical layers, ensuring that the subsequent phases operate on a foundation of absolute trust.

## 4. The Analytical Pipeline

### Phase 1: Ingestion & Acquisition
Phase 1 transforms the raw input document into the 'CanonPolicyPackage' (CPP) through a strictly defined 16-subphase pipeline that enforces a constitutional invariant of producing exactly 300 question-aligned chunks (10 Policy Areas × 6 Dimensions × 5 Questions). This question-aware chunking architecture ensures that every analytical unit is explicitly mapped to the questionnaire structure, preventing the 'optimization' drift where chunks are merged or dropped for performance. A key feature is the mandatory Colombian PDM Enhancement (SP4.1), which executes as default behavior to recognize domain-specific patterns such as 'Ley 152', 'SGP', and 'Enfoque Diferencial'. The execution is governed by the 'Tríada' system, which clearly distinguishes between Parametrization (configurable structure profiles), Calibration (tunable thresholds based on empirical evidence), and Invariants (constitutional mandates like the 300-chunk count that cannot be modified).

### Phase 2: Orchestration & Epistemology
Phase 2 serves as the deterministic engine of the pipeline, executing 300 individual JSON contracts that bind specific analytical questions to the 'Method Dispensary', a library of 240+ specialized analyzer classes. This architecture replaces legacy hard-coded executor classes with a dynamic 'DynamicContractExecutor' that loads contracts at runtime, enabling an 'Epistemological Pipeline' that progresses from Empirical Verification (N1-EMP) to Inferential Processing (N2-INF) and finally Audit (N3-AUD). All findings are aggregated into the 'Evidence Nexus', a graph-native database that uses Dempster-Shafer belief propagation to resolve conflicting evidence and ensure that every synthesized narrative is supported by a traceable chain of custody. The entire process is irrigated by SISAS, which injects context-specific signals into the execution flow without altering the underlying logic.

### Phase 3: Normalization & Layer Scoring
Phase 3 acts as the 'Great Equalizer', transforming the heterogeneous outputs of Phase 2—which may range from boolean flags and probability distributions to integer counts—into a unified [0.0, 100.0] quality scale. This process is governed by an 8-Layer Quality Model that evaluates every evidence atom across eight distinct dimensions: Intrinsic Quality (@b), Unit Quality (@u), Question Fit (@q), Dimension Fit (@d), Policy Fit (@p), Contract Compliance (@C), Chain Integrity (@chain), and Governance (@m). This multi-dimensional scoring ensures that a high score reflects not just the presence of a keyword, but the structural, contextual, and epistemological validity of the finding.

### Phase 4: Dimensional Aggregation
Phase 4 aggregates the 300 normalized micro-scores into the six canonical dimensions of development: Inputs, Activities, Products, Outcomes, Impacts, and Causal Logic. Unlike simplistic arithmetic averages which mask contradictions, this phase employs Choquet Integrals to mathematically model the interaction between criteria, explicitly accounting for synergies (where A and B together are worth more than their sum) and redundancies (where A and B duplicate each other). The aggregation engine also applies an 'Adaptive Penalty' mechanism that reduces the aggregate score in proportion to the coefficient of variation (CV) of the underlying indicators, ensuring that a plan cannot achieve a high rating through 'cherry-picking' easy wins while ignoring critical failures.

### Phase 5: Policy Area Integration
Phase 5 shifts the analytical lens from structural dimensions to thematic content, integrating findings across 10 strategic Policy Areas ranging from Gender Equality (PA01) to Migration (PA10). By creating a matrix of 6 Dimensions × 10 Policy Areas, this phase exposes specific implementation gaps, such as a policy area that has robust 'Activities' but zero allocated 'Inputs' (Budget). This cross-cutting view prevents the 'hollow shell' phenomenon where plans appear structurally sound in the aggregate but are fiscally vacuous in specific critical sectors.

### Phase 6: Cluster Aggregation & Validation
Phase 6 performs the final aggregation of Policy Areas into four high-level 'Meso-Clusters' while acting as the system's internal auditor. It implements rigorous consistency checks to detect statistical anomalies, such as a Policy Area score that deviates more than three standard deviations from the plan's mean. If significant 'incoherence' is detected—defined as high variance between the 'Saying' (Activities) and the 'Doing' (Budget)—Phase 6 triggers a penalty protocol that downgrades the final cluster scores, enforcing the doctrine that a coherent mediocre plan is preferable to a disjointed one.

### Phase 7: Strategic Analysis
Phase 7 synthesizes the four Meso-Cluster scores into a single holistic 'MacroScore' that represents the final verdict on policy compliance. Beyond simple aggregation, this phase employs three advanced analytical engines: Cross-Cutting Coherence Analysis (CCCA) to quantify the alignment between strategic, operational, and institutional dimensions; Systemic Gap Detection (SGD) to identify critical failure patterns that span multiple clusters; and Strategic Alignment Scoring (SAS) to measure the plan's vertical consistency with national priorities and horizontal consistency across sectors. This holistic evaluation produces a definitive classification on a 3-point scale (EXCELENTE to INSUFICIENTE) and triggers the downstream recommendation logic.

### Phase 8: Recommendation Engine
Phase 8 translates the analytical gaps identified in Phase 7 into actionable, context-aware recommendations. Powered by a 'Generic Rule Engine' and a schema-driven validation architecture, this phase uses a memoized matching strategy to instantly select relevant interventions from a library of templates. It generates recommendations at three levels: Micro (specific indicator fixes), Meso (cluster-level strategic shifts), and Macro (systemic governance reforms). The engine is strictly deterministic, employing a 'Template Compilation' window to render thousands of tailored recommendations with zero runtime variance, ensuring that the same gap always produces the same advice.

### Phase 9: Report Assembly & Final Verification
Phase 9 serves as the pipeline's publishing house, assembling all analytical outputs into a comprehensive CanonPolicyPackage (CPP) and generating human-readable deliverables including the Executive Summary, Detailed Findings, and Institutional Entity Annex. It is coupled with the 'Final Verification' (conceptually Phase 10) which seals the analysis by computing a final cryptographic hash of the generated report, signing it with the system's private key, and ensuring that no data was corrupted during the assembly process. This phase enforces the 'Seal of Authenticity', guaranteeing that the distributed report is a bitwise-identical representation of the validated execution state.

## 5. Cross-Cutting Systems

### SISAS (Signal Irrigation)
The Signal-Irrigated Smart Augmentation System (SISAS) acts as the cross-cutting 'nervous system' of the pipeline, decoupling domain knowledge from execution logic. Instead of hard-coding heuristics like 'check for Ley 152' directly into the analyzers, SISAS injects these as 'Signals'—contextual metadata packets (Structural, Integrity, Epistemic, Contrast, Operational, Consumption)—that flow from the Questionnaire into the execution context. This publish-subscribe architecture allows the analytical engines to remain generic while being 'irrigated' with highly specific domain intelligence, governed by a constitutional invariant that requires a Signal Hit Rate of at least 95% for a valid execution.

## 6. Operational Infrastructure

The operational core of the framework is built upon a 'Method Dispensary' containing 74 specialized analyzer classes and over 350 analytical methods, ranging from 'BayesianCounterfactualAuditor' to 'FinancialAggregator'. These methods are dynamically bound to execution contracts at runtime, ensuring that the system can evolve its analytical capabilities without requiring kernel-level recompilation. The entire infrastructure is rigorously documented in the Technical Runbook, which provides canonical procedures for installation, verification, and troubleshooting, enforcing a standardized operational environment where every dependency is pinned and every execution path is deterministic.

---
*Copyright © 2026 Policy Analytics Research Unit. All Rights Reserved.*