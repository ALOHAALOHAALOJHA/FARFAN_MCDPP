{
  "calibration": {
    "note": "Contract does not embed calibration scores. Actual calibration managed via src/farfan_core/core/calibration/ + config/intrinsic_calibration.json + config/fusion_specification.json",
    "source": {
      "canonical_spec": "canonic_calibration_methods.md",
      "fusion_specification": "config/fusion_specification.json",
      "intrinsic_calibration": "config/intrinsic_calibration.json",
      "layer_calibrations_dir": "config/layer_calibrations/"
    },
    "status": "placeholder"
  },
  "compatibility": {
    "method_executor_min_version": "TODO_VERSION",
    "orchestrator_min_version": "TODO_VERSION",
    "phase2_types_version": "TODO_VERSION",
    "questionnaire_monolith_version": "3.0.0",
    "signal_registry_min_version": "TODO_VERSION"
  },
  "error_handling": {
    "failure_contract": {
      "abort_if": [
        "missing_required_element",
        "incomplete_text"
      ],
      "emit_code": "ABORT-Q091-REQ"
    },
    "on_assembly_failure": "propagate_with_trace",
    "on_method_failure": "propagate_with_trace",
    "on_method_not_found": "raise"
  },
  "evidence_assembly": {
    "assembly_rules": [
      {
        "description": "Combine all evidence elements from multiple method invocations",
        "merge_strategy": "concat",
        "sources": [
          "text_mining.diagnose_critical_links",
          "text_mining.analyze_link_text",
          "industrial_policy.process",
          "industrial_policy.match_patterns_in_sentences",
          "industrial_policy.extract_point_evidence",
          "causal_extraction.extract_goals",
          "causal_extraction.parse_goal_context",
          "financial_audit.parse_amount",
          "pdet_analysis.extract_financial_amounts",
          "pdet_analysis.extract_from_budget_table",
          "contradiction_detection.extract_quantitative_claims",
          "contradiction_detection.parse_number",
          "contradiction_detection.statistical_significance_test",
          "bayesian_analysis.evaluate_policy_metric",
          "bayesian_analysis.compare_policies",
          "semantic_processing.chunk_text",
          "semantic_processing.embed_single"
        ],
        "target": "elements_found"
      },
      {
        "default": [],
        "description": "Aggregate confidence scores across all methods",
        "merge_strategy": "weighted_mean",
        "sources": [],
        "target": "confidence_scores"
      },
      {
        "default": {},
        "description": "Combine pattern matches from text mining methods",
        "merge_strategy": "concat",
        "sources": [
          "text_mining.diagnose_critical_links",
          "industrial_policy.match_patterns_in_sentences"
        ],
        "target": "pattern_matches"
      },
      {
        "description": "Combine metadata from all multiple methods for full traceability",
        "merge_strategy": "concat",
        "sources": [],
        "target": "metadata"
      }
    ],
    "class_name": "EvidenceNexus",
    "engine": "EVIDENCE_NEXUS",
    "method_name": "assemble",
    "module": "canonic_phases.Phase_two.evidence_nexus",
    "output_schema": {
      "additionalProperties": true,
      "properties": {
        "elements": {
          "description": "Lista de elementos de evidencia encontrados para esta micro-pregunta.",
          "type": "array"
        },
        "raw_results": {
          "additionalProperties": true,
          "properties": {
            "confidence_scores": {
              "description": "Scores de confianza usados por el scorer.",
              "type": "array"
            },
            "metadata": {
              "description": "Metadatos arbitrarios pasados al scorer.",
              "type": "object"
            },
            "pattern_matches": {
              "description": "Matches de patrones esperados vs texto.",
              "type": "object"
            },
            "semantic_similarity": {
              "description": "Métrica de similitud semántica (si aplica)."
            }
          },
          "type": "object"
        }
      },
      "required": [
        "elements",
        "raw_results"
      ],
      "type": "object"
    }
  },
  "executor_binding": {
    "executor_class": "D1_Q1_Executor",
    "executor_module": "farfan_core.core.orchestrator.executors"
  },
  "fallback_strategy": {
    "note": "No fallback strategies enabled. All failures propagate according to error_handling configuration.",
    "use_heuristics": false,
    "use_llm_direct": false
  },
  "human_answer_structure": {
    "assembly_flow": "Add missing steps from golden",
    "concrete_example": "Merge missing keys from golden",
    "description": "Expected structure of evidence dict after all multiple methods execute and evidence is assembled according to assembly_rules",
    "evidence_structure_post_nexus": {
      "edges": [
        {
          "from": "text_mining",
          "to": "final_evidence"
        },
        {
          "from": "industrial_policy",
          "to": "final_evidence"
        },
        {
          "from": "causal_extraction",
          "to": "final_evidence"
        },
        {
          "from": "financial_audit",
          "to": "final_evidence"
        },
        {
          "from": "bayesian_analysis",
          "to": "final_evidence"
        },
        {
          "from": "semantic_processing",
          "to": "final_evidence"
        }
      ],
      "nodes": {
        "bayesian_analysis": {
          "provides": [
            "metrics",
            "comparisons"
          ],
          "type": "analysis"
        },
        "causal_extraction": {
          "provides": [
            "goals",
            "contexts"
          ],
          "type": "source"
        },
        "final_evidence": {
          "aggregates": "all",
          "type": "sink"
        },
        "financial_audit": {
          "provides": [
            "amounts",
            "totals"
          ],
          "type": "source"
        },
        "industrial_policy": {
          "provides": [
            "processed_evidence",
            "patterns"
          ],
          "type": "source"
        },
        "semantic_processing": {
          "provides": [
            "chunks",
            "embeddings"
          ],
          "type": "processing"
        },
        "text_mining": {
          "provides": [
            "critical_links",
            "patterns"
          ],
          "type": "source"
        }
      },
      "type": "graph"
    },
    "evidence_structure_schema": {
      "description": "Assembled evidence after all methods complete",
      "properties": {
        "bayesian_insights": {
          "description": "Results from BayesianNumericalAnalyzer",
          "properties": {
            "metrics_with_high_uncertainty": {
              "type": "array"
            },
            "significant_comparisons": {
              "type": "integer"
            }
          },
          "type": "object"
        },
        "confidence_scores": {
          "description": "Aggregated confidence metrics (weighted_mean strategy)",
          "properties": {
            "by_method": {
              "description": "Average confidence per analyzer class",
              "type": "object"
            },
            "max": {
              "type": "number"
            },
            "mean": {
              "type": "number"
            },
            "min": {
              "type": "number"
            },
            "std": {
              "type": "number"
            }
          },
          "type": "object"
        },
        "contradictions": {
          "description": "Results from PolicyContradictionDetector",
          "properties": {
            "found": {
              "type": "integer"
            },
            "interpretation": {
              "type": "string"
            },
            "tests_performed": {
              "type": "integer"
            }
          },
          "type": "object"
        },
        "critical_links": {
          "description": "Causal links extracted by TextMiningEngine",
          "items": {
            "properties": {
              "cause": {
                "type": "string"
              },
              "coherence": {
                "type": "number"
              },
              "criticality": {
                "type": "number"
              },
              "effect": {
                "type": "string"
              }
            },
            "type": "object"
          },
          "type": "array"
        },
        "elements_found": {
          "description": "Concatenated evidence elements from multiple methods (assembly_rules target)",
          "example_count": "Expected 15-50 elements for a complete diagnostic",
          "items": {
            "properties": {
              "confidence": {
                "maximum": 1,
                "minimum": 0,
                "type": "number"
              },
              "context": {
                "type": "string"
              },
              "element_id": {
                "example": "E-001",
                "type": "string"
              },
              "sentence_id": {
                "type": "integer"
              },
              "source_method": {
                "example": "IndustrialPolicyProcessor._extract_point_evidence",
                "type": "string"
              },
              "type": {
                "enum": [
                  "fuentes_oficiales",
                  "indicadores_cuantitativos",
                  "series_temporales_años",
                  "cobertura_territorial_especificada",
                  "financial_amounts",
                  "policy_goals",
                  "causal_links"
                ],
                "type": "string"
              },
              "value": {
                "example": "DANE",
                "type": "string"
              }
            },
            "type": "object"
          },
          "type": "array"
        },
        "elements_summary": {
          "properties": {
            "by_type": {
              "properties": {
                "cobertura_territorial_especificada": {
                  "minimum_expected": 1,
                  "type": "integer"
                },
                "fuentes_oficiales": {
                  "minimum_expected": 2,
                  "type": "integer"
                },
                "indicadores_cuantitativos": {
                  "minimum_expected": 3,
                  "type": "integer"
                },
                "series_temporales_años": {
                  "minimum_expected": 3,
                  "type": "integer"
                }
              },
              "type": "object"
            },
            "total_count": {
              "type": "integer"
            }
          },
          "type": "object"
        },
        "financial_summary": {
          "description": "Aggregated financial data from FinancialAuditor and PDETMunicipalPlanAnalyzer",
          "properties": {
            "amounts_found": {
              "type": "integer"
            },
            "by_category": {
              "properties": {
                "SGR": {
                  "type": "number"
                },
                "recursos_propios": {
                  "type": "number"
                },
                "transferencias": {
                  "type": "number"
                }
              },
              "type": "object"
            },
            "total_budget_cop": {
              "type": "number"
            }
          },
          "type": "object"
        },
        "goals_summary": {
          "description": "Policy goals extracted by CausalExtractor",
          "properties": {
            "goals_with_complete_context": {
              "type": "integer"
            },
            "quantified_goals": {
              "type": "integer"
            },
            "total_goals": {
              "type": "integer"
            }
          },
          "type": "object"
        },
        "metadata": {
          "properties": {
            "analysis_timestamp": {
              "format": "date-time",
              "type": "string"
            },
            "document_length": {
              "type": "integer"
            },
            "execution_time_ms": {
              "type": "number"
            },
            "methods_executed": {
              "const": 17,
              "type": "integer"
            }
          },
          "type": "object"
        },
        "pattern_matches": {
          "description": "Aggregated pattern matches from text mining methods",
          "items": {
            "properties": {
              "avg_confidence": {
                "type": "number"
              },
              "count": {
                "type": "integer"
              },
              "pattern_id": {
                "type": "string"
              }
            },
            "type": "object"
          },
          "type": "array"
        },
        "semantic_processing": {
          "description": "Results from SemanticProcessor",
          "properties": {
            "avg_semantic_similarity_to_query": {
              "type": "number"
            },
            "chunks_created": {
              "type": "integer"
            },
            "embeddings_generated": {
              "type": "integer"
            }
          },
          "type": "object"
        }
      },
      "type": "object"
    },
    "template_variable_bindings": {
      "description": "These variables are available for human_readable_output template",
      "variables": {
        "{evidence.confidence_scores.mean}": "87.6%",
        "{evidence.elements_found_count}": 38,
        "{evidence.official_sources_count}": 5,
        "{evidence.pattern_matches_count}": 14,
        "{evidence.quantitative_indicators_count}": 12,
        "{evidence.temporal_series_count}": 4,
        "{evidence.territorial_coverage}": "municipal - zona rural y urbana",
        "{quality_level}": "ALTO",
        "{score}": "Calculated by scorer based on elements"
      }
    },
    "usage_notes": {
      "for_auditors": "This provides traceability from raw method outputs to final assembled evidence.",
      "for_developers": "This structure shows the expected evidence dict after BaseExecutorWithContract._execute_v3() completes all 17 method executions and evidence assembly.",
      "for_validators": "Use this to verify that actual execution output matches expected structure."
    },
    "validation_against_expected_elements": {
      "cobertura_territorial_especificada": {
        "example_element_id": "E-004",
        "found_in_example": true,
        "required": true
      },
      "fuentes_oficiales": {
        "found_in_example": 5,
        "minimum": 2,
        "status": "PASS"
      },
      "indicadores_cuantitativos": {
        "found_in_example": 12,
        "minimum": 3,
        "status": "PASS"
      },
      "overall_validation_result": "PASS - All required and minimum elements present",
      "series_temporales_años": {
        "found_in_example": 4,
        "minimum": 3,
        "status": "PASS"
      }
    }
  },
  "identity": {
    "base_slot": "D1-Q1",
    "cluster_id": "CL01",
    "contract_hash": "3ed2482ae46fcdd19263f1a8f517eb4750bc6056ad2eee0f3a314147b40ecf99",
    "contract_version": "3.0.0",
    "created_at": "2025-11-28T03:50:13.381382+00:00",
    "dimension_id": "DIM01",
    "policy_area_id": "PA03",
    "question_global": 91,
    "question_id": "Q091",
    "updated_at": "2025-12-18T07:16:03.134977+00:00",
    "validated_against_schema": "executor_contract.v3.schema.json"
  },
  "method_binding": {
    "method_count": 17,
    "methods": [
      {
        "class_name": "TextMiningEngine",
        "description": "TextMiningEngine.diagnose_critical_links",
        "method_name": "diagnose_critical_links",
        "priority": 1,
        "provides": "text_mining.diagnose_critical_links",
        "role": "diagnose_critical_links_diagnosis"
      },
      {
        "class_name": "TextMiningEngine",
        "description": "TextMiningEngine._analyze_link_text",
        "method_name": "_analyze_link_text",
        "priority": 2,
        "provides": "text_mining.analyze_link_text",
        "role": "_analyze_link_text_analysis"
      },
      {
        "class_name": "IndustrialPolicyProcessor",
        "description": "IndustrialPolicyProcessor.process",
        "method_name": "process",
        "priority": 3,
        "provides": "industrial_policy.process",
        "role": "process_processing"
      },
      {
        "class_name": "IndustrialPolicyProcessor",
        "description": "IndustrialPolicyProcessor._match_patterns_in_sentences",
        "method_name": "_match_patterns_in_sentences",
        "priority": 4,
        "provides": "industrial_policy.match_patterns_in_sentences",
        "role": "_match_patterns_in_sentences_matching"
      },
      {
        "class_name": "IndustrialPolicyProcessor",
        "description": "IndustrialPolicyProcessor._extract_point_evidence",
        "method_name": "_extract_point_evidence",
        "priority": 5,
        "provides": "industrial_policy.extract_point_evidence",
        "role": "_extract_point_evidence_extraction"
      },
      {
        "class_name": "CausalExtractor",
        "description": "CausalExtractor._extract_goals",
        "method_name": "_extract_goals",
        "priority": 6,
        "provides": "causal_extraction.extract_goals",
        "role": "_extract_goals_extraction"
      },
      {
        "class_name": "CausalExtractor",
        "description": "CausalExtractor._parse_goal_context",
        "method_name": "_parse_goal_context",
        "priority": 7,
        "provides": "causal_extraction.parse_goal_context",
        "role": "_parse_goal_context_parsing"
      },
      {
        "class_name": "FinancialAuditor",
        "description": "FinancialAuditor._parse_amount",
        "method_name": "_parse_amount",
        "priority": 8,
        "provides": "financial_audit.parse_amount",
        "role": "_parse_amount_parsing"
      },
      {
        "class_name": "PDETMunicipalPlanAnalyzer",
        "description": "PDETMunicipalPlanAnalyzer._extract_financial_amounts",
        "method_name": "_extract_financial_amounts",
        "priority": 9,
        "provides": "pdet_analysis.extract_financial_amounts",
        "role": "_extract_financial_amounts_extraction"
      },
      {
        "class_name": "PDETMunicipalPlanAnalyzer",
        "description": "PDETMunicipalPlanAnalyzer._extract_from_budget_table",
        "method_name": "_extract_from_budget_table",
        "priority": 10,
        "provides": "pdet_analysis.extract_from_budget_table",
        "role": "_extract_from_budget_table_extraction"
      },
      {
        "class_name": "PolicyContradictionDetector",
        "description": "PolicyContradictionDetector._extract_quantitative_claims",
        "method_name": "_extract_quantitative_claims",
        "priority": 11,
        "provides": "contradiction_detection.extract_quantitative_claims",
        "role": "_extract_quantitative_claims_extraction"
      },
      {
        "class_name": "PolicyContradictionDetector",
        "description": "PolicyContradictionDetector._parse_number",
        "method_name": "_parse_number",
        "priority": 12,
        "provides": "contradiction_detection.parse_number",
        "role": "_parse_number_parsing"
      },
      {
        "class_name": "PolicyContradictionDetector",
        "description": "PolicyContradictionDetector._statistical_significance_test",
        "method_name": "_statistical_significance_test",
        "priority": 13,
        "provides": "contradiction_detection.statistical_significance_test",
        "role": "_statistical_significance_test_execution"
      },
      {
        "class_name": "BayesianNumericalAnalyzer",
        "description": "BayesianNumericalAnalyzer.evaluate_policy_metric",
        "method_name": "evaluate_policy_metric",
        "priority": 14,
        "provides": "bayesian_analysis.evaluate_policy_metric",
        "role": "evaluate_policy_metric_evaluation"
      },
      {
        "class_name": "BayesianNumericalAnalyzer",
        "description": "BayesianNumericalAnalyzer.compare_policies",
        "method_name": "compare_policies",
        "priority": 15,
        "provides": "bayesian_analysis.compare_policies",
        "role": "compare_policies_comparison"
      },
      {
        "class_name": "SemanticProcessor",
        "description": "SemanticProcessor.chunk_text",
        "method_name": "chunk_text",
        "priority": 16,
        "provides": "semantic_processing.chunk_text",
        "role": "chunk_text_chunking"
      },
      {
        "class_name": "SemanticProcessor",
        "description": "SemanticProcessor.embed_single",
        "method_name": "embed_single",
        "priority": 17,
        "provides": "semantic_processing.embed_single",
        "role": "embed_single_embedding"
      }
    ],
    "note": "All multiple methods extracted from D1_Q1_Executor in executors.py",
    "orchestration_mode": "multi_method_pipeline"
  },
  "method_outputs": {
    "BayesianNumericalAnalyzer.compare_policies": {
      "output_type": "dict",
      "structure": {
        "description": "Output from BayesianNumericalAnalyzer.compare_policies",
        "properties": {
          "comparison_result": {
            "type": "object"
          },
          "confidence": {
            "type": "number"
          },
          "winner": {
            "type": "string"
          }
        },
        "type": "object"
      },
      "usage_in_assembly": {
        "merge_strategy": "replace",
        "provides_key": "bayesian_analysis.compare_policies"
      },
      "validation": {
        "non_empty": true,
        "required": true
      }
    },
    "BayesianNumericalAnalyzer.evaluate_policy_metric": {
      "output_type": "dict",
      "structure": {
        "description": "Output from BayesianNumericalAnalyzer.evaluate_policy_metric",
        "properties": {
          "confidence": {
            "type": "number"
          },
          "metric_value": {
            "type": "number"
          },
          "posterior_distribution": {
            "type": "object"
          }
        },
        "type": "object"
      },
      "usage_in_assembly": {
        "merge_strategy": "replace",
        "provides_key": "bayesian_analysis.evaluate_policy_metric"
      },
      "validation": {
        "non_empty": true,
        "required": true
      }
    },
    "CausalExtractor._extract_goals": {
      "output_type": "dict",
      "structure": {
        "description": "Output from CausalExtractor._extract_goals",
        "properties": {
          "goals": {
            "description": "Extracted policy goals",
            "type": "array"
          }
        },
        "type": "object"
      },
      "usage_in_assembly": {
        "merge_strategy": "concat",
        "provides_key": "causal_extraction.extract_goals"
      },
      "validation": {
        "non_empty": true,
        "required": true
      }
    },
    "CausalExtractor._parse_goal_context": {
      "output_type": "dict",
      "structure": {
        "description": "Output from CausalExtractor._parse_goal_context",
        "properties": {
          "goal_contexts": {
            "description": "Contextual information for goals",
            "type": "object"
          }
        },
        "type": "object"
      },
      "usage_in_assembly": {
        "merge_strategy": "replace",
        "provides_key": "causal_extraction.parse_goal_context"
      },
      "validation": {
        "non_empty": true,
        "required": true
      }
    },
    "FinancialAuditor._parse_amount": {
      "output_type": "dict",
      "structure": {
        "description": "Output from FinancialAuditor._parse_amount",
        "properties": {
          "amounts": {
            "description": "Parsed financial amounts",
            "type": "array"
          },
          "total": {
            "type": "number"
          }
        },
        "type": "object"
      },
      "usage_in_assembly": {
        "merge_strategy": "replace",
        "provides_key": "financial_audit.parse_amount"
      },
      "validation": {
        "non_empty": true,
        "required": true
      }
    },
    "IndustrialPolicyProcessor._extract_point_evidence": {
      "output_type": "dict",
      "structure": {
        "description": "Output from IndustrialPolicyProcessor._extract_point_evidence",
        "properties": {
          "evidence_points": {
            "type": "array"
          }
        },
        "type": "object"
      },
      "usage_in_assembly": {
        "merge_strategy": "concat",
        "provides_key": "industrial_policy.extract_point_evidence"
      },
      "validation": {
        "non_empty": true,
        "required": true
      }
    },
    "IndustrialPolicyProcessor._match_patterns_in_sentences": {
      "output_type": "dict",
      "structure": {
        "description": "Output from IndustrialPolicyProcessor._match_patterns_in_sentences",
        "properties": {
          "sentence_matches": {
            "type": "array"
          }
        },
        "type": "object"
      },
      "usage_in_assembly": {
        "merge_strategy": "replace",
        "provides_key": "industrial_policy.match_patterns_in_sentences"
      },
      "validation": {
        "non_empty": true,
        "required": true
      }
    },
    "IndustrialPolicyProcessor.process": {
      "output_type": "dict",
      "structure": {
        "description": "Output from IndustrialPolicyProcessor.process",
        "properties": {
          "extracted_segments": {
            "type": "object"
          },
          "pattern_matches": {
            "type": "object"
          },
          "structural_completeness": {
            "type": "number"
          }
        },
        "type": "object"
      },
      "usage_in_assembly": {
        "merge_strategy": "replace",
        "provides_key": "industrial_policy.process"
      },
      "validation": {
        "non_empty": true,
        "required": true
      }
    },
    "PDETMunicipalPlanAnalyzer._extract_financial_amounts": {
      "output_type": "dict",
      "structure": {
        "description": "Output from PDETMunicipalPlanAnalyzer._extract_financial_amounts",
        "properties": {
          "financial_amounts": {
            "type": "array"
          }
        },
        "type": "object"
      },
      "usage_in_assembly": {
        "merge_strategy": "concat",
        "provides_key": "pdet_analysis.extract_financial_amounts"
      },
      "validation": {
        "non_empty": true,
        "required": true
      }
    },
    "PDETMunicipalPlanAnalyzer._extract_from_budget_table": {
      "output_type": "dict",
      "structure": {
        "description": "Output from PDETMunicipalPlanAnalyzer._extract_from_budget_table",
        "properties": {
          "budget_items": {
            "type": "array"
          }
        },
        "type": "object"
      },
      "usage_in_assembly": {
        "merge_strategy": "concat",
        "provides_key": "pdet_analysis.extract_from_budget_table"
      },
      "validation": {
        "non_empty": true,
        "required": true
      }
    },
    "PolicyContradictionDetector._extract_quantitative_claims": {
      "output_type": "dict",
      "structure": {
        "description": "Output from PolicyContradictionDetector._extract_quantitative_claims",
        "properties": {
          "quantitative_claims": {
            "type": "array"
          }
        },
        "type": "object"
      },
      "usage_in_assembly": {
        "merge_strategy": "concat",
        "provides_key": "contradiction_detection.extract_quantitative_claims"
      },
      "validation": {
        "non_empty": true,
        "required": true
      }
    },
    "PolicyContradictionDetector._parse_number": {
      "output_type": "dict",
      "structure": {
        "description": "Output from PolicyContradictionDetector._parse_number",
        "properties": {
          "original_text": {
            "type": "string"
          },
          "parsed_value": {
            "type": "number"
          }
        },
        "type": "object"
      },
      "usage_in_assembly": {
        "merge_strategy": "replace",
        "provides_key": "contradiction_detection.parse_number"
      },
      "validation": {
        "non_empty": true,
        "required": true
      }
    },
    "PolicyContradictionDetector._statistical_significance_test": {
      "output_type": "dict",
      "structure": {
        "description": "Output from PolicyContradictionDetector._statistical_significance_test",
        "properties": {
          "is_significant": {
            "type": "boolean"
          },
          "p_value": {
            "type": "number"
          }
        },
        "type": "object"
      },
      "usage_in_assembly": {
        "merge_strategy": "replace",
        "provides_key": "contradiction_detection.statistical_significance_test"
      },
      "validation": {
        "non_empty": true,
        "required": true
      }
    },
    "SemanticProcessor.chunk_text": {
      "output_type": "dict",
      "structure": {
        "description": "Output from SemanticProcessor.chunk_text",
        "properties": {
          "chunks": {
            "description": "Text chunks",
            "type": "array"
          }
        },
        "type": "object"
      },
      "usage_in_assembly": {
        "merge_strategy": "replace",
        "provides_key": "semantic_processing.chunk_text"
      },
      "validation": {
        "non_empty": true,
        "required": true
      }
    },
    "SemanticProcessor.embed_single": {
      "output_type": "dict",
      "structure": {
        "description": "Output from SemanticProcessor.embed_single",
        "properties": {
          "embedding": {
            "description": "Vector embedding",
            "type": "array"
          }
        },
        "type": "object"
      },
      "usage_in_assembly": {
        "merge_strategy": "replace",
        "provides_key": "semantic_processing.embed_single"
      },
      "validation": {
        "non_empty": true,
        "required": true
      }
    },
    "TextMiningEngine._analyze_link_text": {
      "output_type": "dict",
      "structure": {
        "description": "Output from TextMiningEngine._analyze_link_text",
        "properties": {
          "context_coherence_score": {
            "type": "number"
          },
          "contradicting_evidence": {
            "type": "array"
          },
          "supporting_evidence": {
            "type": "array"
          }
        },
        "type": "object"
      },
      "usage_in_assembly": {
        "merge_strategy": "replace",
        "provides_key": "text_mining.analyze_link_text"
      },
      "validation": {
        "non_empty": true,
        "required": true
      }
    },
    "TextMiningEngine.diagnose_critical_links": {
      "output_type": "dict",
      "structure": {
        "description": "Output from TextMiningEngine.diagnose_critical_links",
        "properties": {
          "context": {
            "description": "Contextual information",
            "type": "object"
          },
          "critical_links": {
            "description": "List of critical causal links",
            "type": "array"
          },
          "link_scores": {
            "description": "Criticality scores for each link",
            "type": "object"
          }
        },
        "type": "object"
      },
      "usage_in_assembly": {
        "merge_strategy": "replace",
        "provides_key": "text_mining.diagnose_critical_links"
      },
      "validation": {
        "non_empty": true,
        "required": true
      }
    }
  },
  "methodological_depth": {
    "methods": [
      {
        "class_name": "EvidenceNexus",
        "epistemological_foundation": {
          "epistemological_stance": "Triangulation across heterogeneous sources with explicit uncertainty.",
          "justification": "Chosen because INSUMOS in PA03 must explain why the plan's commitments are coherent, measurable, and traceable to evidence.",
          "ontological_basis": "Policy plans encode mechanisms via activities, outputs, and indicators; evidence is treated as fallible observations of underlying commitments.",
          "paradigm": "critical_realist",
          "theoretical_framework": [
            "Pearl (2009) causal reasoning (why mechanisms matter)",
            "Pawson & Tilley (1997) realistic evaluation",
            "Results-based management for public policy monitoring"
          ]
        },
        "method_name": "contract_orchestrated_evidence_fusion",
        "priority": 1,
        "role": "evidence_graph_construction_and_synthesis",
        "technical_approach": {
          "algorithm": "pattern extraction + multi-method pipeline + evidence graph synthesis",
          "assumptions": [
            "The source plan contains at least one relevant section or table for this question.",
            "Expected element types map to observable text spans or tabular cells."
          ],
          "complexity": "O(n_patterns × n_text) for bounded regex matching + O(n_nodes + n_edges) for graph propagation.",
          "limitations": [
            "Evidence quality depends on document structure and explicitness of commitments.",
            "Pattern-only extraction is conservative to preserve determinism."
          ],
          "method_type": "graph_based_evidence_fusion",
          "steps": [
            {
              "description": "Extract candidate claims and table structures from the document using contract patterns and policy-area scope.",
              "step": 1
            },
            {
              "description": "Populate method outputs under provides slots and construct evidence nodes aligned to expected_elements.",
              "step": 2
            },
            {
              "description": "Assemble aggregate evidence for elements_found and infer relationships to support synthesis and validation.",
              "step": 3
            },
            {
              "description": "Compute completeness, gaps, and confidence interval for Q091/D1-Q1.",
              "step": 4
            }
          ]
        }
      }
    ]
  },
  "output_contract": {
    "consumer_modules": [
      "src.farfan_core.core.phases.phase2_types.validate_phase2_result",
      "src.farfan_core.core.orchestrator.core.Orchestrator._score_micro_results_async",
      "src.farfan_core.analysis.scoring.MicroQuestionScorer"
    ],
    "human_readable_output": {
      "format": "markdown",
      "methodological_depth": {
        "method_combination_logic": {
          "combination_strategy": "Sequential multi-method pipeline with evidence fusion",
          "confidence_aggregation": "Final confidence per evidence element = weighted_mean([confidence_method1, confidence_method2, ...]) where weights reflect method reliability (e.g., Bayesian methods have higher weight than simple regex).",
          "evidence_fusion": "Evidence from all multiple methods is aggregated by the EvidenceNexus. Overlapping evidence (e.g., same numeric value detected by multiple methods) is deduplicated. Confidence scores are combined via weighted averaging.",
          "execution_order": "Methods execute in priority order (1→17). Later methods can access outputs of earlier methods (e.g., method 7 uses goals extracted by method 6).",
          "rationale": "D1-Q1 requires comprehensive extraction of quantitative baseline data from multiple sources (text, tables, causal links). The multiple methods cover complementary aspects: text mining (methods 1-5), goal extraction (6-7), financial analysis (8-10), contradiction detection (11-13), Bayesian inference (14-15), and semantic processing (16-17).",
          "trade_offs": [
            "Comprehensiveness vs. Complexity: multiple methods ensure thorough coverage but increase computational cost and maintenance burden",
            "Precision vs. Recall: Multiple methods increase recall (find more evidence) but may introduce redundancy; deduplication and confidence weighting mitigate this",
            "Interpretability vs. Sophistication: Bayesian and embedding methods are powerful but less transparent than regex; human_readable_output compensates via methodological documentation"
          ]
        },
        "methods": [
          {
            "class_name": "TextMiningEngine",
            "epistemological_foundation": {
              "epistemological_stance": "Empirical-interpretive: Knowledge about policy mechanisms emerges from detecting linguistic markers of causality in policy documents",
              "justification": "Diagnosing critical causal links in baseline diagnostics reveals whether policymakers understand the causal pathways between gender inequalities and their determinants",
              "ontological_basis": "Texts contain latent causal structures that can be detected through linguistic patterns indicating causal relationships (because, therefore, leads to, etc.)",
              "paradigm": "Critical text mining with causal link detection",
              "theoretical_framework": [
                "Causal discourse analysis: Texts reveal causal beliefs through specific linguistic constructions (Fairclough, 2003)",
                "Theory of change reconstruction: Policy documents implicitly encode theories of change that can be extracted via causal link detection (Weiss, 1995)"
              ]
            },
            "method_name": "diagnose_critical_links",
            "output_interpretation": {
              "actionable_insights": [
                "If few critical links found: Diagnosis lacks causal depth, may be purely descriptive",
                "If many links but low criticality: Diagnosis discusses tangential issues, not core gender inequalities"
              ],
              "interpretation_guide": {
                "high_criticality": "≥0.8: Link directly connects to gender inequality outcomes",
                "low_criticality": "<0.5: Peripheral causal relationship",
                "medium_criticality": "0.5-0.79: Link relates to intermediate factors"
              },
              "output_structure": {
                "critical_links": "List of detected causal links with source/target entities",
                "link_scores": "Criticality scores (0-1) based on relevance to gender outcomes"
              }
            },
            "priority": 1,
            "role": "critical_link_diagnosis",
            "technical_approach": {
              "algorithm": "Multi-pattern regex matching with context window analysis",
              "assumptions": [
                "Causal language reflects causal understanding",
                "Critical links mention gender-related outcomes"
              ],
              "complexity": "O(n*p) where n=sentences, p=causal patterns",
              "limitations": [
                "Cannot detect implicit causality without linguistic markers",
                "May miss causal relationships expressed across distant sentences"
              ],
              "method_type": "pattern_based_causal_link_extraction",
              "steps": [
                {
                  "description": "Identify causal connectors (porque, por lo tanto, conduce a, genera, resulta en)",
                  "step": 1
                },
                {
                  "description": "Extract entities before and after connector (cause → effect)",
                  "step": 2
                },
                {
                  "description": "Classify link criticality based on proximity to gender indicators",
                  "step": 3
                }
              ]
            }
          },
          {
            "class_name": "TextMiningEngine",
            "epistemological_foundation": {
              "epistemological_stance": "Coherentist: A causal link is epistemically justified if it coheres with surrounding textual evidence",
              "justification": "Detecting a causal connector alone is insufficient; analyzing surrounding text validates whether the link is substantive or superficial",
              "ontological_basis": "The meaning and validity of a causal claim depends on its textual context (surrounding sentences, semantic coherence)",
              "paradigm": "Contextual semantic analysis",
              "theoretical_framework": [
                "Semantic coherence theory: Valid claims are embedded in coherent semantic contexts",
                "Evidential reasoning: Context provides supporting or undermining evidence for causal claims"
              ]
            },
            "method_name": "_analyze_link_text",
            "output_interpretation": {
              "actionable_insights": [
                "Low coherence + many links: Document has scattered causal claims without substantiation",
                "High coherence: Causal claims are evidence-based and well-argued"
              ],
              "interpretation_guide": {
                "high_coherence": "≥0.7: Link is well-supported by context",
                "low_coherence": "<0.5: Link may be spurious or poorly contextualized"
              },
              "output_structure": {
                "context_coherence_score": "Semantic coherence metric (0-1)",
                "contradicting_evidence": "Contextual sentences that contradict the link",
                "supporting_evidence": "Contextual sentences that support the link"
              }
            },
            "priority": 2,
            "role": "link_context_analysis",
            "technical_approach": {
              "algorithm": "Extract ±N sentences around link, analyze semantic consistency",
              "assumptions": [
                "Coherent contexts indicate valid causal claims",
                "Context window of ±3 sentences captures relevant information"
              ],
              "complexity": "O(k*w) where k=links, w=context window size",
              "limitations": [
                "May miss long-range dependencies beyond context window",
                "Semantic similarity doesn't guarantee logical validity"
              ],
              "method_type": "context_window_semantic_analysis",
              "steps": [
                {
                  "description": "Extract context window (±3 sentences) around detected causal link",
                  "step": 1
                },
                {
                  "description": "Calculate semantic similarity between link and context using embeddings",
                  "step": 2
                },
                {
                  "description": "Identify supporting/contradicting evidence in context",
                  "step": 3
                }
              ]
            }
          },
          {
            "class_name": "IndustrialPolicyProcessor",
            "epistemological_foundation": {
              "epistemological_stance": "Structuralism: Policy knowledge is organized according to hierarchical structures that determine meaning",
              "justification": "Gender policy baselines must be extracted systematically to ensure comparability across municipalities and policy cycles",
              "ontological_basis": "Policy documents follow recognizable structural patterns (policy pillars, strategic objectives, action lines) that can be systematically extracted",
              "paradigm": "Structured policy analysis with industrial rigor",
              "theoretical_framework": [
                "Policy architecture theory: Effective policies have clear structural organization",
                "Industrial-grade analysis: Systematic, replicable methods ensure consistency across documents"
              ]
            },
            "method_name": "process",
            "output_interpretation": {
              "actionable_insights": [
                "Incomplete structure: Municipality may lack technical capacity for formal policy design",
                "Complete structure but no baseline data: Structural compliance without substantive content"
              ],
              "interpretation_guide": {
                "complete_structure": "≥0.8: Document has well-defined policy architecture",
                "incomplete_structure": "<0.5: Document lacks standard structure, may be disorganized"
              },
              "output_structure": {
                "extracted_segments": "Hierarchical dict of policy structure → content",
                "pattern_matches": "Dict of pattern_type → matched instances",
                "structural_completeness": "Score indicating presence of expected structural elements"
              }
            },
            "priority": 3,
            "role": "industrial_policy_pattern_processing",
            "technical_approach": {
              "algorithm": "Multi-level pattern matching with structural validation",
              "assumptions": [
                "Policy documents follow standard Colombian municipal planning structures",
                "Structural markers are consistently used"
              ],
              "complexity": "O(n*m) where n=document sections, m=pattern types",
              "limitations": [
                "Fails on highly non-standard document structures",
                "Cannot process purely narrative documents without structural markers"
              ],
              "method_type": "hierarchical_pattern_extraction",
              "steps": [
                {
                  "description": "Identify policy structure markers (pillar headers, objective numbering)",
                  "step": 1
                },
                {
                  "description": "Extract content within each structural segment",
                  "step": 2
                },
                {
                  "description": "Match predefined policy patterns (baseline data, targets, indicators)",
                  "step": 3
                },
                {
                  "description": "Validate structural completeness and coherence",
                  "step": 4
                }
              ]
            }
          },
          {
            "class_name": "IndustrialPolicyProcessor",
            "epistemological_foundation": {
              "epistemological_stance": "Linguistic realism: Specific phrases reliably indicate policy elements (e.g., 'línea base' indicates baseline data)",
              "justification": "Sentence-level granularity ensures precise localization of policy elements for traceability and validation",
              "ontological_basis": "Policy commitments and evidence are encoded at sentence level through specific linguistic patterns",
              "paradigm": "Micro-level pattern detection",
              "theoretical_framework": [
                "Information extraction theory: Structured information can be extracted from unstructured text via pattern recognition"
              ]
            },
            "method_name": "_match_patterns_in_sentences",
            "output_interpretation": {
              "actionable_insights": [
                "Sentences with multiple pattern types (e.g., baseline + source + indicator): High-quality evidence",
                "Matches concentrated in few sentences: Evidence is localized, not systemic"
              ],
              "interpretation_guide": {
                "dense_matches": "Many matches per sentence: Rich informational content",
                "sparse_matches": "Few matches: Document may lack expected policy elements"
              },
              "output_structure": {
                "sentence_matches": "List of {sentence_id, pattern_id, matched_text, position}"
              }
            },
            "priority": 4,
            "role": "sentence_level_pattern_matching",
            "technical_approach": {
              "algorithm": "Apply pattern registry to each sentence, collect matches with positions",
              "assumptions": [
                "Sentence boundaries are correctly detected",
                "Patterns comprehensively cover expected phrasings"
              ],
              "complexity": "O(s*p) where s=sentences, p=patterns",
              "limitations": [
                "Misses patterns spanning multiple sentences",
                "Regex cannot handle complex syntactic variations"
              ],
              "method_type": "regex_pattern_matching_per_sentence",
              "steps": [
                {
                  "description": "Segment document into sentences",
                  "step": 1
                },
                {
                  "description": "For each sentence, apply all relevant regex patterns",
                  "step": 2
                },
                {
                  "description": "Record matches with sentence ID, start/end positions, matched text",
                  "step": 3
                }
              ]
            }
          },
          {
            "class_name": "IndustrialPolicyProcessor",
            "epistemological_foundation": {
              "epistemological_stance": "Logical atomism: Understanding complex evidence requires breaking it into elementary propositions",
              "justification": "Extracting point-level evidence allows detailed auditing and prevents conflating distinct claims",
              "ontological_basis": "Complex policy evidence can be decomposed into atomic 'points' (individual data claims, sources, indicators)",
              "paradigm": "Evidence atomization",
              "theoretical_framework": [
                "Evidence granularity theory: Fine-grained evidence units enable precise validation and reuse"
              ]
            },
            "method_name": "_extract_point_evidence",
            "output_interpretation": {
              "actionable_insights": [
                "Many quantitative points + few sources: Data without provenance",
                "Balanced distribution: Comprehensive evidence base"
              ],
              "interpretation_guide": {
                "high_point_count": "Rich evidence base",
                "low_point_count": "Sparse evidence"
              },
              "output_structure": {
                "evidence_points": "List of {type, value, context_snippet, confidence}"
              }
            },
            "priority": 5,
            "role": "point_evidence_extraction",
            "technical_approach": {
              "algorithm": "Identify and isolate individual evidence points (numbers, sources, indicators)",
              "assumptions": [
                "Evidence points can be meaningfully isolated from context",
                "Each point has a primary classification"
              ],
              "complexity": "O(n) where n=detected evidence units",
              "limitations": [
                "Decontextualization may lose nuance",
                "Complex evidence may not fit atomic model"
              ],
              "method_type": "atomic_evidence_extraction",
              "steps": [
                {
                  "description": "Detect evidence units (numeric values, entity names, temporal references)",
                  "step": 1
                },
                {
                  "description": "Extract unit with minimal context (subject, predicate)",
                  "step": 2
                },
                {
                  "description": "Classify unit type (quantitative_indicator, official_source, temporal_marker)",
                  "step": 3
                }
              ]
            }
          },
          {
            "class_name": "CausalExtractor",
            "epistemological_foundation": {
              "epistemological_stance": "Intentionalism: Understanding policy requires identifying intended goals",
              "justification": "Baseline diagnostics should connect data to goals (e.g., 'reduce VBG by X%'); extracting goals enables evaluating this connection",
              "ontological_basis": "Policies are goal-directed interventions; goals are explicitly or implicitly stated desired outcomes",
              "paradigm": "Teleological policy analysis",
              "theoretical_framework": [
                "Means-ends rationality: Policies are structured around goals (ends) and actions (means)",
                "Theory of change: Goals are nodes in causal pathways from inputs to impacts"
              ]
            },
            "method_name": "_extract_goals",
            "output_interpretation": {
              "actionable_insights": [
                "Goals without baseline data: Targets lack empirical foundation",
                "Goals aligned with baseline indicators: Evidence-based planning"
              ],
              "interpretation_guide": {
                "quantified_goals": "Goals with numbers are measurable",
                "vague_goals": "Goals without quantifiers are non-measurable"
              },
              "output_structure": {
                "goals": "List of {goal_verb, target_entity, quantifier, sentence}"
              }
            },
            "priority": 6,
            "role": "goal_extraction",
            "technical_approach": {
              "algorithm": "Identify goal markers (reducir, aumentar, lograr) + target entity + quantifier",
              "assumptions": [
                "Goals are expressed with standard verbs",
                "Goals have identifiable objects and quantifiers"
              ],
              "complexity": "O(n) where n=sentences",
              "limitations": [
                "Misses implicit goals",
                "May confuse aspirational language with concrete goals"
              ],
              "method_type": "goal_phrase_extraction",
              "steps": [
                {
                  "description": "Detect goal verbs (reducir, incrementar, alcanzar)",
                  "step": 1
                },
                {
                  "description": "Extract object of goal verb (what is to be reduced/increased)",
                  "step": 2
                },
                {
                  "description": "Extract quantifier if present (percentage, absolute number)",
                  "step": 3
                }
              ]
            }
          },
          {
            "class_name": "CausalExtractor",
            "epistemological_foundation": {
              "epistemological_stance": "Contextualism: Goal meaning depends on context",
              "justification": "A goal like 'reduce VBG' is ambiguous without context (by how much? by when? who is responsible?)",
              "ontological_basis": "Goals are situated in policy contexts that specify conditions, timeframes, and responsible actors",
              "paradigm": "Contextual goal interpretation",
              "theoretical_framework": [
                "Situated policy analysis: Goals must be understood in their institutional and temporal context"
              ]
            },
            "method_name": "_parse_goal_context",
            "output_interpretation": {
              "actionable_insights": [
                "Goals without temporal context: No deadline, low accountability",
                "Goals without responsible actors: Diffused responsibility"
              ],
              "interpretation_guide": {
                "complete_context": "All three dimensions present: Well-specified goal",
                "incomplete_context": "Missing dimensions: Ambiguous goal"
              },
              "output_structure": {
                "goal_contexts": "Dict mapping goal_id → {temporal, spatial, actor} context"
              }
            },
            "priority": 7,
            "role": "goal_contextualization",
            "technical_approach": {
              "algorithm": "Extract temporal, spatial, and actor context around identified goals",
              "assumptions": [
                "Context is proximal to goal statement",
                "Context markers use standard terminology"
              ],
              "complexity": "O(g*w) where g=goals, w=context window",
              "limitations": [
                "May miss context stated in distant document sections",
                "Cannot infer implicit context"
              ],
              "method_type": "context_parsing_around_goals",
              "steps": [
                {
                  "description": "For each goal, extract surrounding context window",
                  "step": 1
                },
                {
                  "description": "Identify temporal markers (plazo, 2024-2027)",
                  "step": 2
                },
                {
                  "description": "Identify responsible actors (Secretaría de...)",
                  "step": 3
                },
                {
                  "description": "Identify spatial scope (municipal, rural, etc.)",
                  "step": 4
                }
              ]
            }
          },
          {
            "class_name": "FinancialAuditor",
            "epistemological_foundation": {
              "epistemological_stance": "Representationalism: Textual representations of amounts map to objective financial realities",
              "justification": "Gender policies require budget backing; extracting financial amounts enables assessing resource adequacy",
              "ontological_basis": "Financial commitments are expressed as monetary amounts in various formats (text, numbers, currencies)",
              "paradigm": "Financial data extraction and normalization",
              "theoretical_framework": [
                "Financial accountability: Budget transparency requires extracting and verifying financial allocations"
              ]
            },
            "method_name": "_parse_amount",
            "output_interpretation": {
              "actionable_insights": [
                "Many amounts but low confidence: Financial data is poorly formatted",
                "No amounts found: Budget information missing from diagnosis"
              ],
              "interpretation_guide": {
                "high_confidence": "≥0.9: Amount clearly stated and parsed",
                "low_confidence": "<0.7: Ambiguous format, may be misparsed"
              },
              "output_structure": {
                "parsed_amounts": "List of {raw_text, normalized_value, currency, confidence}"
              }
            },
            "priority": 8,
            "role": "financial_amount_parsing",
            "technical_approach": {
              "algorithm": "Detect numeric patterns, parse to float, normalize units (thousands, millions)",
              "assumptions": [
                "Amounts follow Colombian formatting conventions (. as thousands separator)",
                "Units are explicitly stated or inferable from context"
              ],
              "complexity": "O(n) where n=detected amount patterns",
              "limitations": [
                "May fail on non-standard formats",
                "Cannot verify accuracy of stated amounts"
              ],
              "method_type": "numeric_parsing_with_normalization",
              "steps": [
                {
                  "description": "Detect amount patterns ($ X.XXX.XXX, X millones)",
                  "step": 1
                },
                {
                  "description": "Parse numeric component to float",
                  "step": 2
                },
                {
                  "description": "Identify unit (millones, mil millones) and multiply",
                  "step": 3
                },
                {
                  "description": "Normalize to standard unit (e.g., COP)",
                  "step": 4
                }
              ]
            }
          },
          {
            "class_name": "PDETMunicipalPlanAnalyzer",
            "epistemological_foundation": {
              "epistemological_stance": "Domain-specific realism: Financial data extraction must account for PDET-specific terminologies and structures",
              "justification": "PDET municipalities may use specific financial categories (PAC, SGR) requiring specialized extraction",
              "ontological_basis": "PDET plans have specific financial reporting requirements and structures",
              "paradigm": "Context-specific financial analysis for PDET municipalities",
              "theoretical_framework": [
                "Institutional context sensitivity: Policy analysis tools must adapt to specific institutional frameworks (PDET)"
              ]
            },
            "method_name": "_extract_financial_amounts",
            "output_interpretation": {
              "actionable_insights": [
                "Uncategorized amounts: Budget detail without transparency on funding source",
                "Heavy reliance on specific source (e.g., all from SGR): Diversification risk"
              ],
              "interpretation_guide": {
                "categorized": "Amount linked to financial category (e.g., SGR)",
                "uncategorized": "Amount found but no category detected"
              },
              "output_structure": {
                "categorized_amounts": "List of {amount, category, source_type, confidence}"
              }
            },
            "priority": 9,
            "role": "pdet_specific_financial_extraction",
            "technical_approach": {
              "algorithm": "Augment generic financial parsing with PDET-specific patterns",
              "assumptions": [
                "PDET plans mention financial categories near amounts",
                "Category terminology is standardized"
              ],
              "complexity": "O(n*c) where n=amounts, c=categories",
              "limitations": [
                "Limited to known PDET financial categories",
                "Cannot extract amounts from complex table formats without table parsing"
              ],
              "method_type": "domain_specific_financial_extraction",
              "steps": [
                {
                  "description": "Apply generic amount parsing",
                  "step": 1
                },
                {
                  "description": "Detect PDET-specific financial categories (SGR, regalías, PAC)",
                  "step": 2
                },
                {
                  "description": "Link amounts to categories based on proximity",
                  "step": 3
                }
              ]
            }
          },
          {
            "class_name": "PDETMunicipalPlanAnalyzer",
            "epistemological_foundation": {
              "epistemological_stance": "Structural realism: Tables encode structured financial information requiring specialized parsing",
              "justification": "Tables contain dense, structured financial data that cannot be extracted via sentence-level text mining",
              "ontological_basis": "Budget data is often presented in tables with rows (programs/projects) and columns (years/amounts)",
              "paradigm": "Structured data extraction from tabular formats",
              "theoretical_framework": [
                "Information architecture: Tabular formats embed relational structures (rows, columns, cells) that determine information retrieval strategies"
              ]
            },
            "method_name": "_extract_from_budget_table",
            "output_interpretation": {
              "actionable_insights": [
                "Multi-year budget tables: Indicates planning horizon and resource trajectory",
                "Single-year tables: Short-term planning, no long-term resource commitment"
              ],
              "interpretation_guide": {
                "complete_tables": "All expected columns present: Comprehensive budget detail",
                "incomplete_tables": "Missing columns/rows: Incomplete financial information"
              },
              "output_structure": {
                "table_data": "List of {row_label, column_label, amount, table_id}"
              }
            },
            "priority": 10,
            "role": "structured_budget_table_extraction",
            "technical_approach": {
              "algorithm": "Detect table structures, parse rows/columns, extract cell values",
              "assumptions": [
                "Tables follow standard row-column format",
                "Headers are identifiable",
                "Amounts are in consistent column positions"
              ],
              "complexity": "O(r*c) where r=rows, c=columns",
              "limitations": [
                "Fails on complex nested tables",
                "May misparse poorly formatted tables",
                "Cannot extract from image-based tables without OCR"
              ],
              "method_type": "table_parsing_and_cell_extraction",
              "steps": [
                {
                  "description": "Detect table markers (HTML <table>, Markdown tables, aligned whitespace)",
                  "step": 1
                },
                {
                  "description": "Parse table structure (identify headers, rows, columns)",
                  "step": 2
                },
                {
                  "description": "Extract cells containing financial amounts",
                  "step": 3
                },
                {
                  "description": "Link amounts to row labels (program names) and column labels (years)",
                  "step": 4
                }
              ]
            }
          },
          {
            "class_name": "PolicyContradictionDetector",
            "epistemological_foundation": {
              "epistemological_stance": "Logical consistency checking: Policy documents should be internally consistent in their quantitative claims",
              "justification": "Extracting quantitative claims enables detecting contradictions that signal data quality issues or errors",
              "ontological_basis": "Policies make quantitative claims (e.g., 'VBG rate is 15%') that can be contradictory or inconsistent",
              "paradigm": "Claim extraction for contradiction detection",
              "theoretical_framework": [
                "Coherence theory of justification: Contradictory claims undermine policy credibility"
              ]
            },
            "method_name": "_extract_quantitative_claims",
            "output_interpretation": {
              "actionable_insights": [
                "Contradictory claims: Data quality issue or conceptual confusion",
                "Consistent claims: Internal coherence of baseline data"
              ],
              "interpretation_guide": {
                "duplicate_subjects": "Multiple claims about same subject: Check for contradictions",
                "unique_subjects": "Each claim covers distinct topic"
              },
              "output_structure": {
                "claims": "List of {subject, value, unit, sentence_id, confidence}"
              }
            },
            "priority": 11,
            "role": "quantitative_claim_extraction",
            "technical_approach": {
              "algorithm": "Extract statements with quantifiers, normalize for comparison",
              "assumptions": [
                "Quantitative claims have standard subject-predicate-value structure",
                "Same subjects are referred to consistently (no synonyms)"
              ],
              "complexity": "O(n) where n=quantitative statements",
              "limitations": [
                "Cannot resolve synonyms (tasa de VBG vs. violencia de género)",
                "May extract claims that are not meant to be factual (hypothetical scenarios)"
              ],
              "method_type": "quantitative_proposition_extraction",
              "steps": [
                {
                  "description": "Identify quantitative statements (X es Y%, la tasa de X es Y)",
                  "step": 1
                },
                {
                  "description": "Parse subject (X) and value (Y)",
                  "step": 2
                },
                {
                  "description": "Store as structured claim {subject, predicate, value}",
                  "step": 3
                }
              ]
            }
          },
          {
            "class_name": "PolicyContradictionDetector",
            "epistemological_foundation": {
              "epistemological_stance": "Objectivism about numbers: Numeric values have objective meanings independent of representation",
              "justification": "Detecting contradictions requires comparing numbers; parsing ensures accurate comparison",
              "ontological_basis": "Numbers are objective entities that must be correctly parsed for valid comparisons",
              "paradigm": "Numerical data normalization",
              "theoretical_framework": [
                "Formal semantics of numbers: Numerical representations map to abstract mathematical objects"
              ]
            },
            "method_name": "_parse_number",
            "output_interpretation": {
              "actionable_insights": [
                "Low parsing confidence: Document uses non-standard number formatting"
              ],
              "interpretation_guide": {
                "high_confidence": "≥0.95: Unambiguous format",
                "low_confidence": "<0.8: Ambiguous, may be misparsed"
              },
              "output_structure": {
                "parsed_value": "Float representation of number",
                "parsing_confidence": "Confidence in parsing correctness (0-1)"
              }
            },
            "priority": 12,
            "role": "numeric_value_parsing",
            "technical_approach": {
              "algorithm": "Handle multiple numeric formats, convert to standard float",
              "assumptions": [
                "Colombian format conventions (. for thousands, , for decimals or vice versa depending on context)"
              ],
              "complexity": "O(n) where n=numeric strings",
              "limitations": [
                "Ambiguity in separator usage (1.234 could be 1234 or 1.234)",
                "May fail on non-standard formats"
              ],
              "method_type": "robust_numeric_parsing",
              "steps": [
                {
                  "description": "Detect numeric patterns (12.5%, 1.234, 1,234.56)",
                  "step": 1
                },
                {
                  "description": "Normalize separators (. vs , for decimals/thousands)",
                  "step": 2
                },
                {
                  "description": "Convert to float, preserve precision",
                  "step": 3
                }
              ]
            }
          },
          {
            "class_name": "PolicyContradictionDetector",
            "epistemological_foundation": {
              "epistemological_stance": "Statistical inference: Use hypothesis testing to distinguish signal from noise",
              "justification": "Not all numeric discrepancies are meaningful; statistical testing prevents false alarms from minor variations",
              "ontological_basis": "Observed differences between claimed values may be due to sampling error or genuine contradictions",
              "paradigm": "Frequentist hypothesis testing",
              "theoretical_framework": [
                "Frequentist statistics: p-values quantify evidence against null hypothesis (no difference)",
                "Error control: Significance testing controls Type I error (false positive contradictions)"
              ]
            },
            "method_name": "_statistical_significance_test",
            "output_interpretation": {
              "actionable_insights": [
                "Significant contradictions: Data quality problem requiring investigation",
                "Non-significant differences: Likely rounding or measurement error"
              ],
              "interpretation_guide": {
                "p < 0.05": "Statistically significant contradiction",
                "p ≥ 0.05": "Difference not statistically significant"
              },
              "output_structure": {
                "test_results": "List of {claim1_id, claim2_id, difference, p_value, significant}"
              }
            },
            "priority": 13,
            "role": "statistical_significance_testing",
            "technical_approach": {
              "algorithm": "Apply t-test or chi-square test to compare claimed values",
              "assumptions": [
                "Measurement errors are normally distributed",
                "Alpha = 0.05 is appropriate threshold"
              ],
              "complexity": "O(n²) for pairwise comparisons of n claims",
              "limitations": [
                "Requires error estimates (often unavailable in policy documents)",
                "Significance ≠ practical importance"
              ],
              "method_type": "hypothesis_testing_for_contradictions",
              "steps": [
                {
                  "description": "For each pair of claims about same subject, compute difference",
                  "step": 1
                },
                {
                  "description": "Estimate measurement error (if metadata available)",
                  "step": 2
                },
                {
                  "description": "Perform significance test (H0: difference = 0)",
                  "step": 3
                },
                {
                  "description": "Flag contradiction if p < 0.05",
                  "step": 4
                }
              ]
            }
          },
          {
            "class_name": "BayesianNumericalAnalyzer",
            "epistemological_foundation": {
              "epistemological_stance": "Bayesian epistemology: Knowledge is represented as probability distributions updated via Bayes' theorem",
              "justification": "Policy baselines often come from small samples or imperfect data; Bayesian methods quantify uncertainty properly",
              "ontological_basis": "Policy metrics (e.g., VBG rates) are uncertain quantities with probability distributions, not point estimates",
              "paradigm": "Bayesian statistical inference",
              "theoretical_framework": [
                "Bayesian inference: Prior beliefs + data → posterior beliefs",
                "Uncertainty quantification: Bayesian methods explicitly model uncertainty"
              ]
            },
            "method_name": "evaluate_policy_metric",
            "output_interpretation": {
              "actionable_insights": [
                "Wide credible intervals: Baseline estimates unreliable, strengthen data collection",
                "Posterior far from prior: Data strongly updates beliefs"
              ],
              "interpretation_guide": {
                "narrow_CI": "High precision, low uncertainty",
                "wide_CI": "High uncertainty, more data needed"
              },
              "output_structure": {
                "credible_interval": "95% Bayesian credible interval",
                "posterior_distribution": "Full posterior (if needed for downstream analysis)",
                "posterior_mean": "Point estimate of metric"
              }
            },
            "priority": 14,
            "role": "bayesian_metric_evaluation",
            "technical_approach": {
              "algorithm": "Combine prior distribution with observed data to compute posterior",
              "assumptions": [
                "Prior is appropriate for metric domain",
                "Data generation process matches likelihood model"
              ],
              "complexity": "O(MCMC iterations) for non-conjugate, O(1) for conjugate",
              "limitations": [
                "Prior choice affects results (subjective)",
                "Computational cost for non-conjugate models"
              ],
              "method_type": "bayesian_posterior_computation",
              "steps": [
                {
                  "description": "Specify prior distribution for metric (e.g., Beta for rates)",
                  "step": 1
                },
                {
                  "description": "Incorporate observed data (e.g., count of VBG cases, population)",
                  "step": 2
                },
                {
                  "description": "Compute posterior distribution via conjugate updates or MCMC",
                  "step": 3
                },
                {
                  "description": "Extract posterior mean, credible intervals",
                  "step": 4
                }
              ]
            }
          },
          {
            "class_name": "BayesianNumericalAnalyzer",
            "epistemological_foundation": {
              "epistemological_stance": "Probabilistic comparison: Compare posterior distributions, not point estimates",
              "justification": "Ranking municipalities by baseline indicators requires proper uncertainty quantification to avoid spurious rankings",
              "ontological_basis": "Comparing municipalities or policy cycles requires accounting for uncertainty in all estimates",
              "paradigm": "Bayesian comparative analysis",
              "theoretical_framework": [
                "Bayesian model comparison: Quantify probability that policy A > policy B",
                "Decision theory: Bayesian methods support rational decision-making under uncertainty"
              ]
            },
            "method_name": "compare_policies",
            "output_interpretation": {
              "actionable_insights": [
                "High overlap despite different point estimates: Differences not meaningful given uncertainty",
                "Clear separation: Confident ranking"
              ],
              "interpretation_guide": {
                "0.75 < P < 0.95": "Moderate evidence",
                "P > 0.95": "Strong evidence A is better",
                "P ≈ 0.5": "No clear difference"
              },
              "output_structure": {
                "effect_size": "Mean difference in metric",
                "overlap": "Degree of posterior overlap",
                "prob_A_better": "P(metric_A > metric_B | data)"
              }
            },
            "priority": 15,
            "role": "bayesian_policy_comparison",
            "technical_approach": {
              "algorithm": "Compute P(metric_A > metric_B | data) from posterior samples",
              "assumptions": [
                "Posteriors are correctly specified",
                "Sufficient samples for stable probability estimate"
              ],
              "complexity": "O(S) where S=number of posterior samples",
              "limitations": [
                "Requires posterior samples (computational cost)",
                "Interpretation requires Bayesian literacy"
              ],
              "method_type": "posterior_distribution_comparison",
              "steps": [
                {
                  "description": "Obtain posterior distributions for both policies",
                  "step": 1
                },
                {
                  "description": "Draw samples from both posteriors",
                  "step": 2
                },
                {
                  "description": "Compute proportion of samples where policy A > policy B",
                  "step": 3
                },
                {
                  "description": "Report probability and effect size",
                  "step": 4
                }
              ]
            }
          },
          {
            "class_name": "SemanticProcessor",
            "epistemological_foundation": {
              "epistemological_stance": "Pragmatism: Chunking strategies are justified by their utility for downstream tasks (embedding, retrieval)",
              "justification": "Semantic search and embedding require chunked text; good chunking preserves semantic units (paragraphs, sections)",
              "ontological_basis": "Long documents must be divided into chunks that fit neural model context windows while preserving semantic coherence",
              "paradigm": "Semantic preprocessing for neural methods",
              "theoretical_framework": [
                "Information retrieval: Chunk granularity affects retrieval precision and recall",
                "Neural model constraints: Transformer models have fixed context windows requiring chunking"
              ]
            },
            "method_name": "chunk_text",
            "output_interpretation": {
              "actionable_insights": [
                "Chunk boundaries split semantic units: Consider different chunking strategy",
                "Chunks align with document structure: Good chunking"
              ],
              "interpretation_guide": {
                "few_large_chunks": "Document is sparse or unstructured",
                "many_small_chunks": "Document is dense or highly structured"
              },
              "output_structure": {
                "chunks": "List of {chunk_text, start_pos, end_pos, chunk_id}"
              }
            },
            "priority": 16,
            "role": "text_chunking_for_embedding",
            "technical_approach": {
              "algorithm": "Chunk by paragraph/section with overlap to preserve context",
              "assumptions": [
                "Document has clear structural boundaries",
                "Overlap improves retrieval (empirical assumption)"
              ],
              "complexity": "O(n) where n=document length",
              "limitations": [
                "Fixed chunk size may split semantic units",
                "Overlap increases storage and computation"
              ],
              "method_type": "semantic_aware_text_chunking",
              "steps": [
                {
                  "description": "Identify natural boundaries (paragraphs, sections)",
                  "step": 1
                },
                {
                  "description": "Create chunks respecting boundaries, max N tokens",
                  "step": 2
                },
                {
                  "description": "Add overlap (e.g., 50 tokens) between chunks for continuity",
                  "step": 3
                }
              ]
            }
          },
          {
            "class_name": "SemanticProcessor",
            "epistemological_foundation": {
              "epistemological_stance": "Distributional hypothesis: Words/phrases with similar meanings occur in similar contexts, captured by embeddings",
              "justification": "Embeddings enable semantic similarity search and clustering beyond keyword matching",
              "ontological_basis": "Text meaning can be represented as high-dimensional vectors in semantic space",
              "paradigm": "Distributional semantics via neural embeddings",
              "theoretical_framework": [
                "Vector semantics: Meaning is position in vector space",
                "Neural language models: Transformers learn contextual representations"
              ]
            },
            "method_name": "embed_single",
            "output_interpretation": {
              "actionable_insights": [
                "Low similarity between baseline and goals: Diagnostic-planning disconnect",
                "High similarity: Coherent evidence-based planning"
              ],
              "interpretation_guide": {
                "clustering": "High-dimensional clustering reveals thematic groups",
                "cosine_similarity": "Use cosine similarity to compare embeddings"
              },
              "output_structure": {
                "embedding": "Dense vector (e.g., 768-dim float array)",
                "model_id": "Identifier of embedding model used"
              }
            },
            "priority": 17,
            "role": "semantic_embedding",
            "technical_approach": {
              "algorithm": "Pass text through pretrained transformer, extract embedding vector",
              "assumptions": [
                "Pretrained model generalizes to policy domain",
                "Embedding dimensionality (e.g., 768) captures relevant semantics"
              ],
              "complexity": "O(L) where L=sequence length for transformer forward pass",
              "limitations": [
                "Domain shift: Model trained on general text may miss domain-specific nuances",
                "Black box: Embeddings are not directly interpretable"
              ],
              "method_type": "transformer_based_text_embedding",
              "steps": [
                {
                  "description": "Tokenize text for transformer model",
                  "step": 1
                },
                {
                  "description": "Pass through model (e.g., BERT, MPNet)",
                  "step": 2
                },
                {
                  "description": "Extract embedding (e.g., [CLS] token or mean pooling)",
                  "step": 3
                },
                {
                  "description": "Normalize to unit vector",
                  "step": 4
                }
              ]
            }
          }
        ]
      },
      "template": {
        "details": [
          "**Fuentes oficiales identificadas**: {evidence.official_sources_count}",
          "**Indicadores cuantitativos**: {evidence.quantitative_indicators_count}",
          "**Series temporales**: {evidence.temporal_series_count}",
          "**Cobertura territorial**: {evidence.territorial_coverage}"
        ],
        "elements_section": "### Elementos de Evidencia Identificados\n\n{evidence.elements_found_list}\n\n**Elementos críticos faltantes**: {evidence.missing_required_elements}",
        "interpretation": "### Interpretación de Resultados\n\n{methodological_interpretation}",
        "recommendations": "### Recomendaciones\n\n{evidence.recommendations}",
        "score_section": "### Evaluación Cuantitativa\n\n- **Puntaje bruto**: {score}/3.0\n- **Nivel de calidad**: {quality_level}\n- **Confianza promedio**: {evidence.confidence_scores.mean}%\n- **Cobertura de patrones**: {evidence.pattern_matches_count}/14 patrones detectados",
        "summary": "### Resumen Ejecutivo\n\nSe analizó la presencia de **{evidence.elements_found_count}** elementos de evidencia cuantitativa relacionados con la línea base diagnóstica en el área de Derechos de las Mujeres e Igualdad de Género.\n\n**Puntaje**: {score}/3.0 | **Calidad**: {quality_level}",
        "title": "## Q091 | Análisis D1-Q1: Línea Base Cuantitativa en Derechos de las Mujeres | Ambiente sano, cambio climático, prevención y atención a desastres"
      }
    },
    "result_type": "Phase2QuestionResult",
    "schema": {
      "additionalProperties": false,
      "properties": {
        "base_slot": {
          "const": "D1-Q1",
          "description": "Debe coincidir con identity.base_slot.",
          "type": "string"
        },
        "cluster_id": {
          "const": "CL01",
          "description": "Cluster de análisis según el monolith, si aplica.",
          "type": [
            "string",
            "null"
          ]
        },
        "dimension_id": {
          "const": "DIM01",
          "description": "Dimensión canónica, coherente con identity.dimension_id.",
          "type": [
            "string",
            "null"
          ]
        },
        "evidence": {
          "additionalProperties": true,
          "description": "Objeto de evidencia ensamblado por el EvidenceNexus; debe cumplir evidence_assembly.output_schema.",
          "type": [
            "object",
            "null"
          ]
        },
        "metadata": {
          "additionalProperties": true,
          "description": "Metadatos adicionales de la pregunta para consumo posterior.",
          "type": [
            "object",
            "null"
          ]
        },
        "policy_area_id": {
          "const": "PA03",
          "description": "PA canónica, debe ser coherente con identity.policy_area_id.",
          "type": [
            "string",
            "null"
          ]
        },
        "question_global": {
          "const": 91,
          "description": "Índice global de la pregunta (de questionnaire_monolith).",
          "type": "integer"
        },
        "question_id": {
          "const": "Q091",
          "description": "Debe coincidir con identity.question_id.",
          "type": "string"
        },
        "trace": {
          "additionalProperties": true,
          "description": "Información de trazabilidad (provenance, logs) específica de la ejecución.",
          "type": [
            "object",
            "null"
          ]
        },
        "validation": {
          "additionalProperties": true,
          "description": "Resultado de validaciones lógicas de la respuesta (coherencia, integridad, etc.).",
          "type": [
            "object",
            "null"
          ]
        }
      },
      "required": [
        "base_slot",
        "question_id",
        "question_global",
        "evidence",
        "validation"
      ],
      "type": "object"
    }
  },
  "question_context": {
    "dimension_label": "INSUMOS",
    "expected_elements": [
      {
        "minimum": 2,
        "type": "fuentes_oficiales"
      },
      {
        "minimum": 3,
        "type": "indicadores_cuantitativos"
      },
      {
        "minimum": 3,
        "type": "series_temporales_años"
      }
    ],
    "expected_output_type": "score",
    "failure_contract": {
      "abort_if": [
        "missing_required_element",
        "incomplete_text"
      ],
      "emit_code": "ABORT-Q091-REQ"
    },
    "modality": null,
    "patterns": [
      {
        "category": "GENERAL",
        "confidence_weight": 0.85,
        "context_requirement": null,
        "context_scope": "PARAGRAPH",
        "flags": "i",
        "id": "PAT-Q091-000",
        "match_type": "REGEX",
        "pattern": "línea base ambiental|diagnóstico de riesgo|situación inicial",
        "policy_area": "PA03",
        "semantic_expansion": null,
        "specificity": "MEDIUM",
        "validation_rule": null
      },
      {
        "category": "GENERAL",
        "confidence_weight": 0.85,
        "context_requirement": null,
        "context_scope": "PARAGRAPH",
        "flags": "i",
        "id": "PAT-Q091-001",
        "match_type": "REGEX",
        "pattern": "fuente:|según|reporte de|con datos de",
        "policy_area": "PA03",
        "semantic_expansion": null,
        "specificity": "MEDIUM",
        "validation_rule": null
      },
      {
        "category": "GENERAL",
        "confidence_weight": 0.85,
        "context_requirement": null,
        "context_scope": "PARAGRAPH",
        "flags": "i",
        "id": "PAT-Q091-002",
        "match_type": "LITERAL",
        "pattern": "IDEAM|CAR|MinAmbiente|SINA|Parques Nacionales Naturales|PNN",
        "policy_area": "PA03",
        "semantic_expansion": null,
        "specificity": "MEDIUM",
        "validation_rule": null
      },
      {
        "category": "GENERAL",
        "confidence_weight": 0.85,
        "context_requirement": null,
        "context_scope": "PARAGRAPH",
        "flags": "i",
        "id": "PAT-Q091-003",
        "match_type": "LITERAL",
        "pattern": "UNGRD|Servicio Geológico Colombiano|SGC|CMGRD",
        "policy_area": "PA03",
        "semantic_expansion": null,
        "specificity": "MEDIUM",
        "validation_rule": null
      },
      {
        "category": "GENERAL",
        "confidence_weight": 0.85,
        "context_requirement": null,
        "context_scope": "PARAGRAPH",
        "flags": "i",
        "id": "PAT-Q091-004",
        "match_type": "REGEX",
        "pattern": "SUI|SSPD|SIVICAP|Secretaría de Salud",
        "policy_area": "PA03",
        "semantic_expansion": null,
        "specificity": "MEDIUM",
        "validation_rule": null
      },
      {
        "category": "INDICADOR",
        "confidence_weight": 0.85,
        "context_requirement": null,
        "context_scope": "PARAGRAPH",
        "flags": "i",
        "id": "PAT-Q091-005",
        "match_type": "REGEX",
        "pattern": "tasa de deforestación|hectáreas deforestadas|Ha/año",
        "policy_area": "PA03",
        "semantic_expansion": null,
        "specificity": "MEDIUM",
        "validation_rule": null
      },
      {
        "category": "GENERAL",
        "confidence_weight": 0.85,
        "context_requirement": null,
        "context_scope": "PARAGRAPH",
        "flags": "i",
        "id": "PAT-Q091-006",
        "match_type": "REGEX",
        "pattern": "Índice de Riesgo de Calidad del Agua|IRCA",
        "policy_area": "PA03",
        "semantic_expansion": null,
        "specificity": "MEDIUM",
        "validation_rule": null
      },
      {
        "category": "GENERAL",
        "confidence_weight": 0.85,
        "context_requirement": null,
        "context_scope": "PARAGRAPH",
        "flags": "i",
        "id": "PAT-Q091-007",
        "match_type": "REGEX",
        "pattern": "Índice de Calidad del Aire|ICA|PM2.5",
        "policy_area": "PA03",
        "semantic_expansion": null,
        "specificity": "MEDIUM",
        "validation_rule": null
      },
      {
        "category": "GENERAL",
        "confidence_weight": 0.85,
        "context_requirement": null,
        "context_scope": "PARAGRAPH",
        "flags": "i",
        "id": "PAT-Q091-008",
        "match_type": "REGEX",
        "pattern": "toneladas de residuos sólidos generadas|aprovechamiento de residuos",
        "policy_area": "PA03",
        "semantic_expansion": null,
        "specificity": "MEDIUM",
        "validation_rule": null
      },
      {
        "category": "GENERAL",
        "confidence_weight": 0.85,
        "context_requirement": null,
        "context_scope": "PARAGRAPH",
        "flags": "i",
        "id": "PAT-Q091-009",
        "match_type": "REGEX",
        "pattern": "familias en zona de amenaza alta|población expuesta a riesgo",
        "policy_area": "PA03",
        "semantic_expansion": null,
        "specificity": "MEDIUM",
        "validation_rule": null
      },
      {
        "category": "GENERAL",
        "confidence_weight": 0.85,
        "context_requirement": null,
        "context_scope": "PARAGRAPH",
        "flags": "i",
        "id": "PAT-Q091-010",
        "match_type": "REGEX",
        "pattern": "caudal|oferta hídrica|Índice de Uso del Agua|IUA",
        "policy_area": "PA03",
        "semantic_expansion": null,
        "specificity": "MEDIUM",
        "validation_rule": null
      },
      {
        "category": "GENERAL",
        "confidence_weight": 0.85,
        "context_requirement": null,
        "context_scope": "PARAGRAPH",
        "flags": "i",
        "id": "PAT-Q091-011",
        "match_type": "LITERAL",
        "pattern": "(?:19|20)\\\\d{2}|vigencia anterior|serie histórica|cuatrienio anterior",
        "pattern_ref": "PAT-0001",
        "policy_area": "PA03",
        "semantic_expansion": null,
        "specificity": "MEDIUM",
        "validation_rule": null
      },
      {
        "category": "GENERAL",
        "confidence_weight": 0.85,
        "context_requirement": null,
        "context_scope": "PARAGRAPH",
        "flags": "i",
        "id": "PAT-Q091-012",
        "match_type": "REGEX",
        "pattern": "mapa de riesgo|zonificación de amenazas",
        "policy_area": "PA03",
        "semantic_expansion": null,
        "specificity": "MEDIUM",
        "validation_rule": null
      }
    ],
    "policy_area_label": "Ambiente sano, cambio climático, prevención y atención a desastres",
    "question_text": "¿El diagnóstico presenta datos numéricos (hectáreas deforestadas, nivel de IRCA, familias en riesgo) para el área de Ambiente sano, cambio climático y gestión de desastres que sirvan como línea base? Se debe verificar el año y fuentes (ej. IDEAM, CAR, CMGRD).",
    "question_type": "micro",
    "scoring_definition_ref": "scoring_modalities.TYPE_A",
    "scoring_modality": "TYPE_A",
    "validations": {
      "buscar_indicadores_cuantitativos": {
        "minimum_required": 3,
        "patterns": [
          "\\d+%",
          "\\d+\\s*por\\s*\\d+",
          "tasa de",
          "índice de",
          "cobertura de"
        ],
        "specificity": "HIGH"
      },
      "cobertura": {
        "minimum_required": 1,
        "patterns": [
          "departamental",
          "municipal",
          "urbano",
          "rural",
          "territorial",
          "poblacional"
        ],
        "specificity": "HIGH"
      },
      "series_temporales": {
        "minimum_years": 3,
        "patterns": [
          "20\\d{2}",
          "año",
          "periodo",
          "histórico",
          "serie"
        ],
        "specificity": "MEDIUM"
      },
      "unidades_medicion": {
        "minimum_required": 2,
        "patterns": [
          "por 100.000",
          "por 1.000",
          "%",
          "porcentaje",
          "tasa",
          "razón"
        ],
        "specificity": "MEDIUM"
      },
      "verificar_fuentes": {
        "minimum_required": 2,
        "patterns": [
          "fuente:",
          "según",
          "datos de",
          "DANE",
          "DNP",
          "SISPRO",
          "SIVIGILA",
          "Ministerio"
        ],
        "specificity": "MEDIUM"
      }
    }
  },
  "signal_requirements": {
    "mandatory_signals": [
      "baseline_completeness",
      "coverage_data",
      "data_sources",
      "enrollment_rates",
      "quality_indicators"
    ],
    "minimum_signal_threshold": 0.5,
    "note": "Signal requirements are currently under development. Signal IDs will be populated once the signal_registry provides a canonical mapping for PA01/DIM01.",
    "optional_signals": [
      "dropout_rates",
      "geographic_scope",
      "infrastructure_status",
      "teacher_ratios",
      "temporal_coverage"
    ],
    "signal_aggregation": "weighted_mean"
  },
  "test_configuration": {
    "expected_test_coverage": ">=90%",
    "integration_test_required": true,
    "test_document_fixtures": [
      "tests/fixtures/preprocessed_documents/sample_pdet_plan.json"
    ],
    "test_files": [
      "tests/core/orchestrator/test_executors_contract.py",
      "tests/core/phases/test_phase2.py"
    ]
  },
  "traceability": {
    "contract_author": "F.A.R.F.A.N Mechanistic Policy Pipeline",
    "contract_generation_method": "automated_specialization_from_monolith",
    "json_path": "blocks.micro_questions[90]",
    "method_mapping_source": "executor_methods_mapping.json",
    "method_source": "src/farfan_core/core/orchestrator/executors.py:D1_Q1_Executor",
    "ontology_source": "config/canonical_ontologies/policy_areas_and_dimensions.json",
    "provenance_note": "This contract was generated with full multi-method orchestration support. The method_binding.methods array contains all multiple methods from D1_Q1_QuantitativeBaselineExtractor, and human_answer_structure documents the expected evidence output after execution.",
    "source_file": "data/questionnaire_monolith.json",
    "source_hash": "9cbb485065ff803727b9b62408acb7223c0d9ab6dfe83cec6c82a7e4dade7d30",
    "source_question_id": "Q091",
    "specialization_timestamp": "2025-11-28T03:50:13.381444+00:00",
    "specialized_from_base_slot": "D1-Q1"
  },
  "validation_rules": {
    "class_name": "ValidationEngine",
    "engine": "VALIDATION_ENGINE",
    "method_name": "validate",
    "module": "canonic_phases.Phase_two.evidence_nexus",
    "na_policy": "abort_on_critical",
    "rules": [
      {
        "description": "Auto-generated: require all required expected_elements types",
        "field": "elements_found",
        "must_contain": {
          "count": 1,
          "elements": [
            "cobertura_territorial_especificada"
          ]
        },
        "type": "array"
      },
      {
        "description": "Auto-generated: encourage optional evidence types when available",
        "field": "elements_found",
        "should_contain": [
          {
            "elements": [
              "fuentes_oficiales"
            ],
            "minimum": 2
          },
          {
            "elements": [
              "indicadores_cuantitativos"
            ],
            "minimum": 3
          },
          {
            "elements": [
              "series_temporales_años"
            ],
            "minimum": 3
          }
        ],
        "type": "array"
      }
    ]
  }
}
