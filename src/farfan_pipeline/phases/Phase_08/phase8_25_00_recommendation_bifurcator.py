# phase8_25_00_recommendation_bifurcator.py - THE UNIFIED RECOMMENDATION ENGINE v3.0.0
"""
Module: src.farfan_pipeline.phases.Phase_08.phase8_25_00_recommendation_bifurcator
Purpose: UNIFIED recommendation engine with exponential bifurcation amplification
Owner: phase8_core
Stage: 25 (Unified Engine - Generation + Amplification)
Order: 00
Type: UNIFIED (ENG + AMP)
Lifecycle: ACTIVE
Version: 3.0.0
Effective-Date: 2026-01-22

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
              ðŸ”± THE UNIFIED BIFURCATOR - SINGLE RECOMMENDATION ENGINE ðŸ”±
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

CONSOLIDATION (v3.0.0):
    This is now the SINGLE unified recommendation engine for Phase 8.
    - Replaces: phase8_20_00_recommendation_engine.py
    - Replaces: phase8_20_01_recommendation_engine_adapter.py
    - Replaces: phase8_20_02_generic_rule_engine.py
    - Replaces: phase8_30_00_signal_enriched_recommendations.py

    ONE ENGINE THAT DOES EVERYTHING:
    1. Loads rules from JSON (from phase8_20_00)
    2. Generates base recommendations from score data
    3. Applies exponential amplification via bifurcation (from v2.0.0)

ARCHITECTURE:
    - RuleLoader: Loads and validates rules from JSON
    - RecommendationGenerator: Generates base recommendations
    - BifurcationEngine: Amplifies recommendations exponentially
    - UnifiedBifurcator: Main entry point combining all phases

BIFURCATION STRATEGIES:
    1. DIMENSIONAL RESONANCE: When DIM01 improves, related DIMs benefit
    2. CROSS-POLLINATION: When fixing PA01 partially fixes PA03
    3. TEMPORAL CASCADES: Short-term fixes unlock long-term capabilities
    4. SYNERGY DETECTION: Two interventions together > sum of parts

Author: F.A.R.F.A.N Unified Architecture Team
Python: 3.10+
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

from __future__ import annotations

# =============================================================================
# METADATA
# =============================================================================

__version__ = "3.0.0"
__phase__ = 8
__stage__ = 25
__order__ = 0
__author__ = "F.A.R.F.A.N Unified Team"
__created__ = "2026-01-21"
__modified__ = "2026-01-22"
__criticality__ = "HIGH"
__execution_pattern__ = "On-Demand"
__codename__ = "UNIFIED-BIFURCATOR"

# =============================================================================
# IMPORTS
# =============================================================================

import json
import logging
import re
from collections import defaultdict
from dataclasses import dataclass, field
from datetime import datetime, timezone
from functools import lru_cache
from itertools import combinations
from pathlib import Path
from typing import Any, TypeAlias

logger = logging.getLogger(__name__)

# =============================================================================
# PUBLIC API
# =============================================================================

__all__ = [
    # Main engine
    "UnifiedBifurcator",
    "RecommendationBifurcator",  # Alias for backward compatibility
    # Result types
    "BifurcationResult",
    "UnifiedRecommendationResult",
    "CrossPollinationNode",
    "TemporalCascade",
    "SynergyMatrix",
    # Configuration
    "AmplificationConfig",
    # Convenience functions
    "bifurcate_recommendations",
    "generate_recommendations",  # NEW: Unified generation + bifurcation
    "enrich_recommendation_with_bifurcation",
    "integrate_bifurcator_into_recommendation_set",
    # Data models (re-exported for convenience)
    "Recommendation",
    "RecommendationSet",
]

# =============================================================================
# TYPE ALIASES
# =============================================================================

RecommendationDict: TypeAlias = dict[str, Any]
ScoreKey: TypeAlias = str  # Format: "PA##-DIM##"
RuleId: TypeAlias = str
SynergyPair: TypeAlias = tuple[RuleId, RuleId]

# =============================================================================
# CONSTANTS - Input Normalization
# =============================================================================

VALID_LEVELS = frozenset({"MICRO", "MESO", "MACRO"})
DEFAULT_LEVEL = "MICRO"

SCORE_KEY_PATTERN = re.compile(r"^(PA\d{2})-(DIM\d{2})$")

# =============================================================================
# DEFAULT RULE CONFIGURATION
# =============================================================================

# Default score bands (used when JSON rules are not loaded)
DEFAULT_MICRO_SCORE_BANDS = [
    {"code": "CRISIS", "label": "CRISIS", "min": 0.0, "max": 0.81,
     "requires_approval": True, "blocking": True, "horizon_months": 3, "cost_multiplier": 1.4},
    {"code": "CRITICO", "label": "CRÃTICO", "min": 0.81, "max": 1.66,
     "requires_approval": True, "blocking": False, "horizon_months": 6, "cost_multiplier": 1.0},
    {"code": "ACEPTABLE", "label": "ACEPTABLE", "min": 1.66, "max": 2.31,
     "requires_approval": False, "blocking": False, "horizon_months": 9, "cost_multiplier": 0.8},
    {"code": "BUENO", "label": "BUENO", "min": 2.31, "max": 2.71,
     "requires_approval": False, "blocking": False, "horizon_months": 12, "cost_multiplier": 0.6},
    {"code": "EXCELENTE", "label": "EXCELENTE", "min": 2.71, "max": 3.01,
     "requires_approval": False, "blocking": False, "horizon_months": 18, "cost_multiplier": 0.5},
]

# Score band to horizon mapping
BAND_HORIZONS = {
    "CRISIS": 3,
    "CRITICO": 6,
    "ACEPTABLE": 9,
    "BUENO": 12,
    "EXCELENTE": 18,
}

# =============================================================================
# CONSTANTS - Dimensional Resonance Matrix
# =============================================================================

DIMENSIONAL_RESONANCE = {
    "DIM01": {"DIM03": 0.7, "DIM06": 0.5},
    "DIM02": {"DIM05": 0.8, "DIM04": 0.4},
    "DIM03": {"DIM01": 0.6, "DIM04": 0.5},
    "DIM04": {"DIM02": 0.5, "DIM05": 0.7},
    "DIM05": {"DIM02": 0.6, "DIM04": 0.6},
    "DIM06": {"DIM01": 0.8, "DIM03": 0.5},
}

CLUSTER_ECHO_PATTERNS = {
    "CLUSTER_MESO_1": ["PA01", "PA02", "PA03"],
    "CLUSTER_MESO_2": ["PA04", "PA05", "PA06"],
    "CLUSTER_MESO_3": ["PA07", "PA08"],
    "CLUSTER_MESO_4": ["PA09", "PA10"],
}

TEMPORAL_FOLDING = {
    3: {"unlock_12": 0.3, "unlock_18": 0.1},
    6: {"unlock_12": 0.5, "unlock_18": 0.2},
    9: {"unlock_18": 0.4},
    12: {"amplify_existing": 0.3},
}

# =============================================================================
# HELPER FUNCTIONS - Input Normalization
# =============================================================================

def _normalize_level(level: str | None) -> str:
    """Normalize level to one of MICRO/MESO/MACRO."""
    if level is None:
        return DEFAULT_LEVEL
    normalized = str(level).strip().upper()
    return normalized if normalized in VALID_LEVELS else DEFAULT_LEVEL


def _parse_score_key(score_key: str | None) -> tuple[str, str] | None:
    """Parse score_key with format PA##-DIM##."""
    if not score_key:
        return None
    match = SCORE_KEY_PATTERN.match(str(score_key).strip().upper())
    return (match.group(1), match.group(2)) if match else None


def _safe_get_metadata(rec: dict[str, Any]) -> dict[str, Any]:
    """Extract metadata from a recommendation record safely."""
    meta = rec.get("metadata")
    return meta if isinstance(meta, dict) else {}


def _stable_sort_key(rec: dict[str, Any]) -> tuple[str, str]:
    """Generate stable sorting key for recommendations."""
    metadata = _safe_get_metadata(rec)
    score_key = metadata.get("score_key", "")
    rule_id = rec.get("rule_id", "")
    return (score_key, rule_id)


def _ensure_deterministic_input(
    recommendations: list[dict[str, Any]]
) -> list[dict[str, Any]]:
    """Sort recommendations by stable key to guarantee deterministic output."""
    return sorted(recommendations, key=_stable_sort_key)


def _get_score_band(score: float) -> str:
    """Get score band from score value."""
    for band in DEFAULT_MICRO_SCORE_BANDS:
        if band["min"] <= score < band["max"]:
            return band["code"]
    return "EXCELENTE" if score >= 2.71 else "CRISIS"

# =============================================================================
# DATA STRUCTURES - Configuration
# =============================================================================

@dataclass
class AmplificationConfig:
    """Configuration for amplification calculation caps."""
    max_amplification_factor: float | None = 10.0
    max_hidden_value_ratio: float = 5.0
    max_cascade_bonus: float = 2.0
    max_synergy_multiplier: float = 3.0


# =============================================================================
# DATA STRUCTURES - Analysis Results (from v2.0.0)
# =============================================================================

@dataclass
class CrossPollinationNode:
    """Represents a hidden benefit discovered through cross-pollination analysis."""
    source_rule_id: str
    target_key: str
    pollination_strength: float
    mechanism: str
    estimated_bonus_score: float
    evidence: dict[str, Any] = field(default_factory=dict)


@dataclass
class TemporalCascade:
    """Models how a short-term intervention cascades into long-term effects."""
    root_rule_id: str
    order: int
    horizon_months: int
    effect_description: str
    cascade_multiplier: float
    prerequisites: list[str] = field(default_factory=list)

    def get_temporal_id(self) -> str:
        return f"{self.root_rule_id}_T{self.order}_{self.horizon_months}M"


@dataclass
class SynergyMatrix:
    """Encodes which recommendations synergize and their combined effect."""
    synergy_pairs: dict[tuple[str, str], float] = field(default_factory=dict)
    synergy_descriptions: dict[tuple[str, str], str] = field(default_factory=dict)

    def add_synergy(self, rule_id_a: str, rule_id_b: str, strength: float, description: str) -> None:
        key = tuple(sorted([rule_id_a, rule_id_b]))
        self.synergy_pairs[key] = strength
        self.synergy_descriptions[key] = description

    def get_synergy_multiplier(self, rule_ids: set[str]) -> float:
        multiplier = 1.0
        for pair, strength in self.synergy_pairs.items():
            if set(pair).issubset(rule_ids):
                multiplier *= (1.0 + strength)
        return multiplier

    def get_total_synergies(self) -> int:
        return len(self.synergy_pairs)


@dataclass
class BifurcationResult:
    """The complete bifurcation analysis for a recommendation set."""
    original_count: int
    bifurcated_count: int
    cross_pollinations: list[CrossPollinationNode]
    temporal_cascades: list[TemporalCascade]
    synergy_matrix: SynergyMatrix
    amplification_factor: float
    level: str
    analysis_timestamp: str
    hidden_value_score: float = 0.0
    cascade_depth: int = 0
    strongest_synergy: tuple[str, str, float] | None = None

    def to_dict(self) -> dict[str, Any]:
        sorted_cps = sorted(
            self.cross_pollinations,
            key=lambda cp: (cp.source_rule_id, cp.target_key)
        )
        sorted_cascades = sorted(
            self.temporal_cascades,
            key=lambda tc: (tc.root_rule_id, tc.order, tc.horizon_months)
        )
        return {
            "level": self.level,
            "original_count": self.original_count,
            "bifurcated_count": self.bifurcated_count,
            "amplification_factor": round(self.amplification_factor, 3),
            "hidden_value_score": round(self.hidden_value_score, 3),
            "cascade_depth": self.cascade_depth,
            "cross_pollinations_count": len(self.cross_pollinations),
            "temporal_cascades_count": len(self.temporal_cascades),
            "synergies_count": self.synergy_matrix.get_total_synergies(),
            "strongest_synergy": self.strongest_synergy,
            "analysis_timestamp": self.analysis_timestamp,
            "cross_pollinations": [
                {
                    "source": cp.source_rule_id,
                    "target": cp.target_key,
                    "strength": round(cp.pollination_strength, 3),
                    "mechanism": cp.mechanism,
                    "bonus": round(cp.estimated_bonus_score, 4),
                }
                for cp in sorted_cps
            ],
            "temporal_cascades": [
                {
                    "id": tc.get_temporal_id(),
                    "root": tc.root_rule_id,
                    "order": tc.order,
                    "horizon": tc.horizon_months,
                    "multiplier": round(tc.cascade_multiplier, 3),
                    "effect": tc.effect_description,
                }
                for tc in sorted_cascades
            ],
        }


# =============================================================================
# DATA MODELS - Recommendation and RecommendationSet
# =============================================================================

@dataclass
class Recommendation:
    """Structured recommendation with full intervention details."""
    rule_id: str
    level: str
    problem: str
    intervention: str
    indicator: dict[str, Any]
    responsible: dict[str, Any]
    horizon: dict[str, Any]
    verification: list[Any]
    metadata: dict[str, Any] = field(default_factory=dict)
    execution: dict[str, Any] | None = None
    budget: dict[str, Any] | None = None
    template_id: str | None = None
    template_params: dict[str, Any] | None = None

    def to_dict(self) -> dict[str, Any]:
        from dataclasses import asdict
        result = asdict(self)
        return {k: v for k, v in result.items() if v is not None}


@dataclass
class RecommendationSet:
    """Collection of recommendations with metadata."""
    level: str
    recommendations: list[Recommendation]
    generated_at: str
    total_rules_evaluated: int
    rules_matched: int
    metadata: dict[str, Any] = field(default_factory=dict)
    bifurcation_analysis: dict[str, Any] | None = None

    def to_dict(self) -> dict[str, Any]:
        result = {
            "level": self.level,
            "recommendations": [r.to_dict() for r in self.recommendations],
            "generated_at": self.generated_at,
            "total_rules_evaluated": self.total_rules_evaluated,
            "rules_matched": self.rules_matched,
            "metadata": self.metadata,
        }
        if self.bifurcation_analysis:
            result["bifurcation_analysis"] = self.bifurcation_analysis
        return result


@dataclass
class UnifiedRecommendationResult:
    """Complete result from unified engine (generation + bifurcation)."""
    level: str
    base_recommendations: list[RecommendationDict]
    bifurcation_result: BifurcationResult | None
    generated_at: str
    total_rules_evaluated: int
    rules_matched: int

    def to_dict(self) -> dict[str, Any]:
        return {
            "level": self.level,
            "recommendations": self.base_recommendations,
            "generated_at": self.generated_at,
            "total_rules_evaluated": self.total_rules_evaluated,
            "rules_matched": self.rules_matched,
            "bifurcation_analysis": self.bifurcation_result.to_dict() if self.bifurcation_result else None,
        }


# =============================================================================
# RULE LOADER - Loads rules from JSON
# =============================================================================

class RuleLoader:
    """Loads and caches recommendation rules from JSON."""

    def __init__(self, rules_path: str | Path | None = None):
        if rules_path is None:
            rules_path = (
                Path(__file__).resolve().parent
                / "json_phase_eight"
                / "recommendation_rules_enhanced.json"
            )
        self.rules_path = Path(rules_path)
        self._rules_cache: dict[str, Any] | None = None
        self._score_bands: list[dict[str, Any]] = DEFAULT_MICRO_SCORE_BANDS

    def load_rules(self) -> dict[str, Any]:
        """Load rules from JSON file."""
        if self._rules_cache is not None:
            return self._rules_cache

        try:
            if self.rules_path.exists():
                self._rules_cache = json.loads(self.rules_path.read_text(encoding="utf-8"))
                if "micro_score_bands" in self._rules_cache:
                    self._score_bands = self._rules_cache["micro_score_bands"]
                logger.info(f"Loaded rules from {self.rules_path}")
            else:
                logger.warning(f"Rules file not found: {self.rules_path}, using defaults")
                self._rules_cache = {"rules": []}
        except Exception as e:
            logger.error(f"Failed to load rules: {e}, using defaults")
            self._rules_cache = {"rules": []}

        return self._rules_cache

    def get_score_bands(self) -> list[dict[str, Any]]:
        """Get score bands configuration."""
        if self._rules_cache is None:
            self.load_rules()
        return self._score_bands

    @lru_cache(maxsize=1)
    def get_rules_for_level(self, level: str) -> list[dict[str, Any]]:
        """Get all rules for a specific level (cached)."""
        rules = self.load_rules()
        return [r for r in rules.get("rules", []) if r.get("level") == level]


# =============================================================================
# RECOMMENDATION GENERATOR - Generates base recommendations
# =============================================================================

class RecommendationGenerator:
    """Generates base recommendations from score data."""

    def __init__(self, rule_loader: RuleLoader | None = None):
        self.rule_loader = rule_loader or RuleLoader()

    def generate_micro_recommendations(
        self,
        micro_scores: dict[str, float],
        context: dict[str, Any] | None = None,
    ) -> list[RecommendationDict]:
        """Generate MICRO level recommendations from PA-DIM scores."""
        context = context or {}
        recommendations = []
        score_bands = self.rule_loader.get_score_bands()

        for score_key, score in micro_scores.items():
            parsed = _parse_score_key(score_key)
            if parsed is None:
                continue

            pa_id, dim_id = parsed
            band = _get_score_band(score)
            band_info = next((b for b in score_bands if b["code"] == band), score_bands[0])

            # Calculate gap from target (2.5 is target)
            gap = max(2.5 - score, 0)

            rec: RecommendationDict = {
                "rule_id": f"MICRO-{pa_id}-{dim_id}-{band}",
                "level": "MICRO",
                "problem": f"Score {score:.2f} in {pa_id}-{dim_id} is below target",
                "intervention": self._generate_intervention(pa_id, dim_id, band, gap),
                "indicator": {"metric": f"{pa_id}-{dim_id}_score", "current": score, "target": 2.5},
                "responsible": {"entity": "Policy Team", "role": "Implementation Lead"},
                "horizon": {
                    "start": context.get("start_date", "2026-01"),
                    "end": context.get("end_date", f"2026-{1 + band_info['horizon_months']//12:02d}"),
                },
                "verification": ["post_implementation_survey", "score_remeasurement"],
                "metadata": {
                    "score_key": score_key,
                    "score": score,
                    "score_band": band,
                    "gap": gap,
                    "pa_id": pa_id,
                    "dim_id": dim_id,
                },
                "budget": {
                    "estimated_cost": band_info["cost_multiplier"] * gap * 10000,
                    "currency": "USD",
                },
            }
            recommendations.append(rec)

        logger.info(f"Generated {len(recommendations)} MICRO recommendations")
        return recommendations

    def generate_meso_recommendations(
        self,
        cluster_data: dict[str, Any],
        context: dict[str, Any] | None = None,
    ) -> list[RecommendationDict]:
        """Generate MESO level recommendations from cluster data."""
        context = context or {}
        recommendations = []

        for cluster_id, cluster_info in cluster_data.items():
            variance = cluster_info.get("variance", 0)
            weak_areas = cluster_info.get("weak_areas", [])

            if variance < 0.1 and not weak_areas:
                continue

            rec: RecommendationDict = {
                "rule_id": f"MESO-{cluster_id}-IMPROVEMENT",
                "level": "MESO",
                "problem": f"Cluster {cluster_id} shows high variance or weak areas",
                "intervention": f"Strengthen cluster {cluster_id} through targeted improvements",
                "indicator": {"metric": "cluster_variance", "current": variance, "target": 0.05},
                "responsible": {"entity": "Cluster Coordinator", "role": "MESO Lead"},
                "horizon": {
                    "start": context.get("start_date", "2026-01"),
                    "end": context.get("end_date", "2026-12"),
                },
                "verification": ["cluster_assessment", "variance_monitoring"],
                "metadata": {
                    "cluster_id": cluster_id,
                    "variance": variance,
                    "weak_areas": weak_areas,
                },
            }
            recommendations.append(rec)

        logger.info(f"Generated {len(recommendations)} MESO recommendations")
        return recommendations

    def generate_macro_recommendations(
        self,
        macro_data: dict[str, Any],
        context: dict[str, Any] | None = None,
    ) -> list[RecommendationDict]:
        """Generate MACRO level recommendations from macro data."""
        context = context or {}
        recommendations = []

        overall_score = macro_data.get("overall_score", 0)
        if overall_score < 2.0:
            rec: RecommendationDict = {
                "rule_id": "MACRO-SYSTEM-WIDE-IMPROVEMENT",
                "level": "MACRO",
                "problem": f"System score {overall_score:.2f} below acceptable threshold",
                "intervention": "Implement system-wide improvement plan",
                "indicator": {"metric": "overall_score", "current": overall_score, "target": 2.5},
                "responsible": {"entity": "Executive Leadership", "role": "MACRO Lead"},
                "horizon": {
                    "start": context.get("start_date", "2026-01"),
                    "end": context.get("end_date", "2027-01"),
                },
                "verification": ["annual_review", "system_audit"],
                "metadata": {
                    "overall_score": overall_score,
                    "macro_band": _get_score_band(overall_score),
                },
            }
            recommendations.append(rec)

        logger.info(f"Generated {len(recommendations)} MACRO recommendations")
        return recommendations

    def _generate_intervention(self, pa_id: str, dim_id: str, band: str, gap: float) -> str:
        """Generate intervention description based on PA, DIM, and severity."""
        severity_map = {
            "CRISIS": "Immediate emergency intervention required",
            "CRITICO": "Major restructuring needed",
            "ACEPTABLE": "Minor adjustments recommended",
            "BUENO": "Optimization opportunities exist",
            "EXCELENTE": "Maintain best practices",
        }
        base = severity_map.get(band, "Improvement recommended")
        return f"{base} for {pa_id} {dim_id} (gap: {gap:.2f})"


# =============================================================================
# BIFURCATION ENGINE - Amplifies recommendations (from v2.0.0)
# =============================================================================

class BifurcationEngine:
    """Applies exponential amplification to recommendations."""

    def __init__(
        self,
        enable_resonance: bool = True,
        enable_pollination: bool = True,
        enable_cascades: bool = True,
        enable_synergies: bool = True,
        resonance_threshold: float = 0.3,
        pollination_threshold: float = 0.25,
        cascade_max_depth: int = 3,
        amplification_config: AmplificationConfig | None = None,
    ):
        self.enable_resonance = enable_resonance
        self.enable_pollination = enable_pollination
        self.enable_cascades = enable_cascades
        self.enable_synergies = enable_synergies
        self.resonance_threshold = resonance_threshold
        self.pollination_threshold = pollination_threshold
        self.cascade_max_depth = min(max(1, cascade_max_depth), 3)
        self.amplification_config = amplification_config or AmplificationConfig()

    def bifurcate(
        self,
        recommendations: list[dict[str, Any]],
        level: str,
        score_data: dict[str, Any] | None = None,
    ) -> BifurcationResult:
        """Perform bifurcation analysis on a recommendation set."""
        level = _normalize_level(level)
        recommendations = _ensure_deterministic_input(recommendations)

        cross_pollinations: list[CrossPollinationNode] = []
        temporal_cascades: list[TemporalCascade] = []
        synergy_matrix = SynergyMatrix()

        if self.enable_resonance and level == "MICRO":
            resonances = self._detect_dimensional_resonance(recommendations, score_data)
            cross_pollinations.extend(resonances)

        if self.enable_pollination:
            pollinations = self._detect_cross_pollination(recommendations, level)
            cross_pollinations.extend(pollinations)

        if self.enable_cascades:
            cascades = self._build_temporal_cascades(recommendations, level)
            temporal_cascades.extend(cascades)

        if self.enable_synergies:
            synergy_matrix = self._construct_synergy_matrix(recommendations, level)

        rule_ids = {r.get("rule_id", "") for r in recommendations}
        amplification_factor, hidden_value_score, cascade_depth = self._calculate_amplification(
            base_count=len(recommendations),
            cross_pollinations=cross_pollinations,
            temporal_cascades=temporal_cascades,
            synergy_matrix=synergy_matrix,
            rule_ids=rule_ids,
            config=self.amplification_config,
        )

        strongest = None
        max_strength = 0.0
        for pair, strength in synergy_matrix.synergy_pairs.items():
            if strength > max_strength:
                max_strength = strength
                strongest = (pair[0], pair[1], strength)

        return BifurcationResult(
            original_count=len(recommendations),
            bifurcated_count=len(recommendations) + len(cross_pollinations),
            cross_pollinations=cross_pollinations,
            temporal_cascades=temporal_cascades,
            synergy_matrix=synergy_matrix,
            amplification_factor=amplification_factor,
            level=level,
            analysis_timestamp=datetime.now(timezone.utc).isoformat(),
            hidden_value_score=hidden_value_score,
            cascade_depth=cascade_depth,
            strongest_synergy=strongest,
        )

    def _calculate_amplification(
        self,
        base_count: int,
        cross_pollinations: list[CrossPollinationNode],
        temporal_cascades: list[TemporalCascade],
        synergy_matrix: SynergyMatrix,
        rule_ids: set[str],
        config: AmplificationConfig,
    ) -> tuple[float, float, int]:
        base_value = max(base_count, 1)
        raw_hidden = sum(cp.estimated_bonus_score for cp in cross_pollinations)
        hidden_ratio = min(raw_hidden / base_value, config.max_hidden_value_ratio)
        raw_cascade = sum(
            tc.cascade_multiplier - 1.0
            for tc in temporal_cascades
            if tc.order > 1
        )
        cascade_bonus = min(raw_cascade, config.max_cascade_bonus)
        raw_synergy = synergy_matrix.get_synergy_multiplier(rule_ids)
        synergy_mult = min(raw_synergy, config.max_synergy_multiplier)
        amplification = (1.0 + hidden_ratio + cascade_bonus) * synergy_mult
        if config.max_amplification_factor is not None:
            amplification = min(amplification, config.max_amplification_factor)
        hidden_value_score = raw_hidden
        cascade_depth = max((tc.order for tc in temporal_cascades), default=0)
        return amplification, hidden_value_score, cascade_depth

    def _detect_dimensional_resonance(
        self,
        recommendations: list[dict[str, Any]],
        score_data: dict[str, Any] | None,
    ) -> list[CrossPollinationNode]:
        resonances = []
        for rec in recommendations:
            metadata = _safe_get_metadata(rec)
            parsed = _parse_score_key(metadata.get("score_key"))
            if parsed is None:
                continue
            pa_id, dim_id = parsed
            if dim_id not in DIMENSIONAL_RESONANCE:
                continue
            for target_dim, strength in DIMENSIONAL_RESONANCE[dim_id].items():
                if strength < self.resonance_threshold:
                    continue
                raw_gap = metadata.get("gap")
                original_gap = abs(float(raw_gap)) if isinstance(raw_gap, (int, float)) else 0.0
                bonus = original_gap * strength * 0.3
                resonances.append(CrossPollinationNode(
                    source_rule_id=rec.get("rule_id", ""),
                    target_key=f"{pa_id}-{target_dim}",
                    pollination_strength=strength,
                    mechanism=f"dimensional_resonance:{dim_id}â†’{target_dim}",
                    estimated_bonus_score=bonus,
                    evidence={"source_dim": dim_id, "target_dim": target_dim},
                ))
        return resonances

    def _detect_cross_pollination(
        self,
        recommendations: list[dict[str, Any]],
        level: str,
    ) -> list[CrossPollinationNode]:
        pollinations = []
        if level == "MICRO":
            by_dim = defaultdict(list)
            for rec in recommendations:
                metadata = _safe_get_metadata(rec)
                parsed = _parse_score_key(metadata.get("score_key"))
                if parsed is not None:
                    _, dim_id = parsed
                    by_dim[dim_id].append(rec)
            for dim_id, recs in by_dim.items():
                if len(recs) < 2:
                    continue
                for rec_a, rec_b in combinations(recs, 2):
                    metadata_a = _safe_get_metadata(rec_a)
                    key_b = metadata_a.get("score_key", "")
                    pollinations.append(CrossPollinationNode(
                        source_rule_id=rec_a.get("rule_id", ""),
                        target_key=key_b,
                        pollination_strength=0.4,
                        mechanism=f"same_dimension_pollination:{dim_id}",
                        estimated_bonus_score=0.1,
                        evidence={"dimension": dim_id},
                    ))
        elif level == "MESO":
            for rec in recommendations:
                metadata = _safe_get_metadata(rec)
                cluster_id = metadata.get("cluster_id", "")
                if cluster_id in CLUSTER_ECHO_PATTERNS:
                    for pa_id in CLUSTER_ECHO_PATTERNS[cluster_id]:
                        pollinations.append(CrossPollinationNode(
                            source_rule_id=rec.get("rule_id", ""),
                            target_key=f"{pa_id}-ALL",
                            pollination_strength=0.35,
                            mechanism=f"cluster_echo:{cluster_id}â†’{pa_id}",
                            estimated_bonus_score=0.15,
                            evidence={"cluster": cluster_id},
                        ))
        elif level == "MACRO":
            for rec in recommendations:
                pollinations.append(CrossPollinationNode(
                    source_rule_id=rec.get("rule_id", ""),
                    target_key="SYSTEM-WIDE",
                    pollination_strength=0.25,
                    mechanism="macro_system_amplification",
                    estimated_bonus_score=0.5,
                    evidence={"scope": "all_clusters"},
                ))
        return [p for p in pollinations if p.pollination_strength >= self.pollination_threshold]

    def _build_temporal_cascades(
        self,
        recommendations: list[dict[str, Any]],
        level: str,
    ) -> list[TemporalCascade]:
        cascades = []
        for rec in recommendations:
            metadata = _safe_get_metadata(rec)
            score_band = metadata.get("score_band", "")
            base_horizon = BAND_HORIZONS.get(score_band, 6)
            cascades.append(TemporalCascade(
                root_rule_id=rec.get("rule_id", ""),
                order=1,
                horizon_months=base_horizon,
                effect_description=f"Direct intervention completion at {base_horizon}M",
                cascade_multiplier=1.0,
            ))
            if base_horizon in TEMPORAL_FOLDING and self.cascade_max_depth >= 2:
                folds = TEMPORAL_FOLDING[base_horizon]
                for fold_key, multiplier in folds.items():
                    if fold_key.startswith("unlock_"):
                        unlock_horizon = int(fold_key.split("_")[1])
                        cascades.append(TemporalCascade(
                            root_rule_id=rec.get("rule_id", ""),
                            order=2,
                            horizon_months=unlock_horizon,
                            effect_description=f"Unlocked capability at {unlock_horizon}M",
                            cascade_multiplier=1.0 + multiplier,
                        ))
                        if self.cascade_max_depth >= 3:
                            cascades.append(TemporalCascade(
                                root_rule_id=rec.get("rule_id", ""),
                                order=3,
                                horizon_months=unlock_horizon + 6,
                                effect_description=f"Compound growth at {unlock_horizon + 6}M",
                                cascade_multiplier=1.0 + multiplier * 1.5,
                            ))
        return cascades

    def _construct_synergy_matrix(
        self,
        recommendations: list[dict[str, Any]],
        level: str,
    ) -> SynergyMatrix:
        matrix = SynergyMatrix()
        if len(recommendations) < 2:
            return matrix
        if level == "MICRO":
            by_pa = defaultdict(list)
            for rec in recommendations:
                metadata = _safe_get_metadata(rec)
                parsed = _parse_score_key(metadata.get("score_key"))
                if parsed is not None:
                    pa_id, _ = parsed
                    by_pa[pa_id].append(rec)
            for pa_id, recs in by_pa.items():
                for rec_a, rec_b in combinations(recs, 2):
                    metadata_a = _safe_get_metadata(rec_a)
                    metadata_b = _safe_get_metadata(rec_b)
                    key_a = metadata_a.get("score_key", "")
                    key_b = metadata_b.get("score_key", "")
                    parsed_a = _parse_score_key(key_a)
                    parsed_b = _parse_score_key(key_b)
                    if parsed_a is None or parsed_b is None:
                        continue
                    dim_a = parsed_a[1]
                    dim_b = parsed_b[1]
                    if dim_a != dim_b:
                        resonance = DIMENSIONAL_RESONANCE.get(dim_a, {}).get(dim_b, 0)
                        synergy_strength = 0.2 + resonance * 0.3
                        matrix.add_synergy(
                            rec_a.get("rule_id", ""),
                            rec_b.get("rule_id", ""),
                            synergy_strength,
                            f"Same PA ({pa_id}) cross-dimensional synergy"
                        )
        elif level == "MESO":
            for rec_a, rec_b in combinations(recommendations, 2):
                metadata_a = _safe_get_metadata(rec_a)
                metadata_b = _safe_get_metadata(rec_b)
                cluster_a = metadata_a.get("cluster_id", "")
                cluster_b = metadata_b.get("cluster_id", "")
                if cluster_a and cluster_b:
                    try:
                        cluster_a_num = int(cluster_a[-1])
                        cluster_b_num = int(cluster_b[-1])
                        if abs(cluster_a_num - cluster_b_num) == 1:
                            matrix.add_synergy(
                                rec_a.get("rule_id", ""),
                                rec_b.get("rule_id", ""),
                                0.35,
                                f"Adjacent cluster synergy ({cluster_a}â†”{cluster_b})"
                            )
                    except (ValueError, IndexError):
                        pass
        elif level == "MACRO":
            for rec_a, rec_b in combinations(recommendations, 2):
                matrix.add_synergy(
                    rec_a.get("rule_id", ""),
                    rec_b.get("rule_id", ""),
                    0.4,
                    "Strategic system-level synergy"
                )
        return matrix


# =============================================================================
# UNIFIED BIFURCATOR - Main entry point combining generation + amplification
# =============================================================================

class UnifiedBifurcator:
    """
    THE UNIFIED RECOMMENDATION ENGINE (v3.0.0).

    Combines rule generation and exponential amplification into one engine.
    Replaces phase8_20_00, phase8_20_01, phase8_20_02, and phase8_30_00.

    Methods
    -------
    generate_micro_recommendations(micro_scores, context, apply_bifurcation)
        Generate MICRO recommendations with optional bifurcation
    generate_meso_recommendations(cluster_data, context, apply_bifurcation)
        Generate MESO recommendations with optional bifurcation
    generate_macro_recommendations(macro_data, context, apply_bifurcation)
        Generate MACRO recommendations with optional bifurcation
    generate_all_recommendations(micro_scores, cluster_data, macro_data, context, apply_bifurcation)
        Generate all levels with optional bifurcation
    """

    def __init__(
        self,
        rules_path: str | Path | None = None,
        apply_bifurcation: bool = True,
        enable_resonance: bool = True,
        enable_pollination: bool = True,
        enable_cascades: bool = True,
        enable_synergies: bool = True,
        amplification_config: AmplificationConfig | None = None,
    ):
        """
        Initialize the Unified Bifurcator.

        Args:
            rules_path: Path to rules JSON file (optional)
            apply_bifurcation: Whether to apply exponential amplification (default True)
            enable_resonance: Enable dimensional resonance detection
            enable_pollination: Enable cross-pollination analysis
            enable_cascades: Enable temporal cascade identification
            enable_synergies: Enable synergy matrix construction
            amplification_config: Configuration for amplification caps
        """
        self.rule_loader = RuleLoader(rules_path)
        self.generator = RecommendationGenerator(self.rule_loader)
        self.apply_bifurcation = apply_bifurcation
        self.bifurcation_engine = BifurcationEngine(
            enable_resonance=enable_resonance,
            enable_pollination=enable_pollination,
            enable_cascades=enable_cascades,
            enable_synergies=enable_synergies,
            amplification_config=amplification_config,
        ) if apply_bifurcation else None

        logger.info(
            f"ðŸ”± UnifiedBifurcator v3.0.0 initialized: "
            f"bifurcation={'enabled' if apply_bifurcation else 'disabled'}"
        )

    def generate_micro_recommendations(
        self,
        micro_scores: dict[str, float],
        context: dict[str, Any] | None = None,
        apply_bifurcation: bool | None = None,
    ) -> UnifiedRecommendationResult:
        """Generate MICRO recommendations with optional bifurcation."""
        context = context or {}
        should_bifurcate = apply_bifurcation if apply_bifurcation is not None else self.apply_bifurcation

        recommendations = self.generator.generate_micro_recommendations(micro_scores, context)
        bifurcation_result = None

        if should_bifurcate and self.bifurcation_engine:
            bifurcation_result = self.bifurcation_engine.bifurcate(recommendations, "MICRO")

        return UnifiedRecommendationResult(
            level="MICRO",
            base_recommendations=recommendations,
            bifurcation_result=bifurcation_result,
            generated_at=datetime.now(timezone.utc).isoformat(),
            total_rules_evaluated=len(micro_scores),
            rules_matched=len(recommendations),
        )

    def generate_meso_recommendations(
        self,
        cluster_data: dict[str, Any],
        context: dict[str, Any] | None = None,
        apply_bifurcation: bool | None = None,
    ) -> UnifiedRecommendationResult:
        """Generate MESO recommendations with optional bifurcation."""
        context = context or {}
        should_bifurcate = apply_bifurcation if apply_bifurcation is not None else self.apply_bifurcation

        recommendations = self.generator.generate_meso_recommendations(cluster_data, context)
        bifurcation_result = None

        if should_bifurcate and self.bifurcation_engine:
            bifurcation_result = self.bifurcation_engine.bifurcate(recommendations, "MESO")

        return UnifiedRecommendationResult(
            level="MESO",
            base_recommendations=recommendations,
            bifurcation_result=bifurcation_result,
            generated_at=datetime.now(timezone.utc).isoformat(),
            total_rules_evaluated=len(cluster_data),
            rules_matched=len(recommendations),
        )

    def generate_macro_recommendations(
        self,
        macro_data: dict[str, Any],
        context: dict[str, Any] | None = None,
        apply_bifurcation: bool | None = None,
    ) -> UnifiedRecommendationResult:
        """Generate MACRO recommendations with optional bifurcation."""
        context = context or {}
        should_bifurcate = apply_bifurcation if apply_bifurcation is not None else self.apply_bifurcation

        recommendations = self.generator.generate_macro_recommendations(macro_data, context)
        bifurcation_result = None

        if should_bifurcate and self.bifurcation_engine:
            bifurcation_result = self.bifurcation_engine.bifurcate(recommendations, "MACRO")

        return UnifiedRecommendationResult(
            level="MACRO",
            base_recommendations=recommendations,
            bifurcation_result=bifurcation_result,
            generated_at=datetime.now(timezone.utc).isoformat(),
            total_rules_evaluated=1,
            rules_matched=len(recommendations),
        )

    def generate_all_recommendations(
        self,
        micro_scores: dict[str, float] | None = None,
        cluster_data: dict[str, Any] | None = None,
        macro_data: dict[str, Any] | None = None,
        context: dict[str, Any] | None = None,
        apply_bifurcation: bool | None = None,
    ) -> dict[str, UnifiedRecommendationResult]:
        """Generate recommendations at all levels."""
        results = {}
        if micro_scores is not None:
            results["MICRO"] = self.generate_micro_recommendations(micro_scores, context, apply_bifurcation)
        if cluster_data is not None:
            results["MESO"] = self.generate_meso_recommendations(cluster_data, context, apply_bifurcation)
        if macro_data is not None:
            results["MACRO"] = self.generate_macro_recommendations(macro_data, context, apply_bifurcation)
        return results


# =============================================================================
# BACKWARD COMPATIBILITY ALIASES
# =============================================================================

RecommendationBifurcator = UnifiedBifurcator


# =============================================================================
# CONVENIENCE FUNCTIONS
# =============================================================================

def bifurcate_recommendations(
    recommendations: list[dict[str, Any]],
    level: str,
    score_data: dict[str, Any] | None = None,
    **kwargs,
) -> BifurcationResult:
    """Convenience function to bifurcate existing recommendations."""
    engine = BifurcationEngine(**kwargs)
    return engine.bifurcate(recommendations, level, score_data)


def generate_recommendations(
    level: str,
    micro_scores: dict[str, float] | None = None,
    cluster_data: dict[str, Any] | None = None,
    macro_data: dict[str, Any] | None = None,
    context: dict[str, Any] | None = None,
    apply_bifurcation: bool = True,
    **kwargs,
) -> UnifiedRecommendationResult:
    """
    Generate recommendations with optional bifurcation (unified entry point).

    Args:
        level: MICRO, MESO, or MACRO
        micro_scores: PA-DIM score dict (for MICRO)
        cluster_data: Cluster metrics (for MESO)
        macro_data: Macro metrics (for MACRO)
        context: Optional context dict
        apply_bifurcation: Apply exponential amplification
        **kwargs: Passed to UnifiedBifurcator

    Returns:
        UnifiedRecommendationResult with recommendations and optional bifurcation
    """
    bifurcator = UnifiedBifurcator(apply_bifurcation=apply_bifurcation, **kwargs)

    if level == "MICRO":
        return bifurcator.generate_micro_recommendations(micro_scores or {}, context, apply_bifurcation)
    elif level == "MESO":
        return bifurcator.generate_meso_recommendations(cluster_data or {}, context, apply_bifurcation)
    elif level == "MACRO":
        return bifurcator.generate_macro_recommendations(macro_data or {}, context, apply_bifurcation)
    else:
        raise ValueError(f"Invalid level: {level}")


def enrich_recommendation_with_bifurcation(
    recommendation: dict[str, Any],
    bifurcation_result: BifurcationResult,
) -> dict[str, Any]:
    """Enrich a single recommendation with its bifurcation data."""
    rule_id = recommendation.get("rule_id", "")
    relevant_cps = [
        cp for cp in bifurcation_result.cross_pollinations
        if cp.source_rule_id == rule_id
    ]
    relevant_cascades = [
        tc for tc in bifurcation_result.temporal_cascades
        if tc.root_rule_id == rule_id
    ]
    relevant_synergies = {
        pair: strength
        for pair, strength in bifurcation_result.synergy_matrix.synergy_pairs.items()
        if rule_id in pair
    }
    base = 1.0
    cp_bonus = sum(cp.estimated_bonus_score for cp in relevant_cps)
    cascade_mult = max((tc.cascade_multiplier for tc in relevant_cascades), default=1.0)
    synergy_bonus = sum(relevant_synergies.values())
    individual_amplification = (base + cp_bonus) * cascade_mult * (1 + synergy_bonus)
    enrichment = {
        "bifurcation": {
            "enabled": True,
            "individual_amplification": round(individual_amplification, 3),
            "cross_pollinations_count": len(relevant_cps),
            "temporal_cascades_count": len(relevant_cascades),
            "synergies_count": len(relevant_synergies),
            "cross_pollination_targets": [cp.target_key for cp in relevant_cps],
            "max_cascade_order": max((tc.order for tc in relevant_cascades), default=0),
            "synergy_partners": [
                p for pair in relevant_synergies for p in pair if p != rule_id
            ],
        }
    }
    enriched = {**recommendation}
    enriched["metadata"] = {**enriched.get("metadata", {}), **enrichment}
    return enriched


def integrate_bifurcator_into_recommendation_set(
    recommendation_set_dict: dict[str, Any],
) -> dict[str, Any]:
    """Integration hook: Adds bifurcation analysis to a RecommendationSet.to_dict() output."""
    level = recommendation_set_dict.get("level", "MICRO")
    recommendations = recommendation_set_dict.get("recommendations", [])
    if not recommendations:
        recommendation_set_dict["bifurcation_analysis"] = {
            "enabled": True,
            "skipped": True,
            "reason": "no_recommendations",
        }
        return recommendation_set_dict
    result = bifurcate_recommendations(recommendations, level)
    enriched_recs = [
        enrich_recommendation_with_bifurcation(rec, result)
        for rec in recommendations
    ]
    output = {
        **recommendation_set_dict,
        "recommendations": enriched_recs,
        "bifurcation_analysis": result.to_dict(),
    }
    return output


# =============================================================================
# MAIN (Demo/Testing)
# =============================================================================

if __name__ == "__main__":
    print("=" * 80)
    print("ðŸ”± UNIFIED BIFURCATOR v3.0.0 - Single Recommendation Engine")
    print("=" * 80)
    print()
    print("Consolidated engine with:")
    print("  âœ“ Rule generation from JSON (phase8_20_00)")
    print("  âœ“ Exponential amplification (v2.0.0)")
    print("  âœ“ Input normalization (v2.0.0)")
    print("  âœ“ Deterministic output (v2.0.0)")
    print("  âœ“ Amplification control (v2.0.0)")
    print()

    # Demo with mock data
    micro_scores = {
        "PA01-DIM01": 1.2,
        "PA01-DIM03": 1.8,
        "PA02-DIM01": 0.9,
    }

    print("Demo: Generating MICRO recommendations with bifurcation...")
    result = generate_recommendations("MICRO", micro_scores=micro_scores)

    print()
    print(f"Level:              {result.level}")
    print(f"Recommendations:    {len(result.base_recommendations)}")
    if result.bifurcation_result:
        print(f"Amplification:      {result.bifurcation_result.amplification_factor:.2f}x")
        print(f"Cross-pollinations: {len(result.bifurcation_result.cross_pollinations)}")
        print(f"Temporal cascades:  {len(result.bifurcation_result.temporal_cascades)}")
    print()
    print("=" * 80)
