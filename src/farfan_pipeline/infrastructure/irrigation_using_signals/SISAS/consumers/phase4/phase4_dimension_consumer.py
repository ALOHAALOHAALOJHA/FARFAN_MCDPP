"""
Phase4 Dimension Consumer - SOPHISTICATED PHASE 4 DIMENSION AGGREGATION

Advanced dimension aggregation consumer implementing:
- Multi-signal correlation tracking across dimension boundaries
- Adaptive dimension score weighting based on signal patterns
- Cross-dimensional causality analysis
- Signal flow monitoring for 300→60 aggregation
- Real-time backpressure detection and handling
- Dimension-specific synchronization checkpoints

ARCHITECTURE ROLE:
Handles the critical 300 ScoredMicroQuestion → 60 DimensionScore aggregation
with sophisticated signal-based intelligence for data quality and provenance.

Generated by IrrigationProtocol v1.0.0  
Enhanced: 2026-01-26 (Sophisticated Phase 4 Enhancements)
"""

from typing import Any, Dict, List, Optional, Set, Tuple
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from collections import defaultdict, deque
from enum import Enum
import logging
import time

from ..base_consumer import BaseConsumer
from ...core.signal import Signal, SignalType, SignalConfidence

# =============================================================================
# DIMENSION-SPECIFIC ENHANCEMENT STRUCTURES
# =============================================================================

class DimensionAggregationState(Enum):
    """State of dimension aggregation process"""
    COLLECTING = "collecting"  # Collecting micro-scores
    READY = "ready"  # Ready to aggregate
    AGGREGATING = "aggregating"  # Currently aggregating
    COMPLETED = "completed"  # Aggregation complete
    STALLED = "stalled"  # No progress (backpressure detected)


@dataclass
class DimensionScoreTracking:
    """Track aggregation state for a single dimension"""
    dimension_id: str
    expected_micro_scores: int = 300  # Total questions
    received_micro_scores: int = 0
    aggregated_score: Optional[float] = None
    state: DimensionAggregationState = DimensionAggregationState.COLLECTING
    signal_ids: List[str] = field(default_factory=list)
    quality_metrics: Dict[str, float] = field(default_factory=dict)
    last_update: datetime = field(default_factory=datetime.utcnow)
    stall_detected_at: Optional[datetime] = None
    
    def update_progress(self, signal_id: str):
        """Update tracking with new signal"""
        self.received_micro_scores += 1
        self.signal_ids.append(signal_id)
        self.last_update = datetime.utcnow()
        self.stall_detected_at = None
        
        # Update state based on progress
        if self.received_micro_scores >= self.expected_micro_scores:
            self.state = DimensionAggregationState.READY
    
    def check_for_stall(self, stall_threshold_seconds: int = 300) -> bool:
        """Check if dimension aggregation has stalled"""
        if self.state == DimensionAggregationState.COMPLETED:
            return False
        
        time_since_update = (datetime.utcnow() - self.last_update).total_seconds()
        if time_since_update > stall_threshold_seconds:
            if not self.stall_detected_at:
                self.stall_detected_at = datetime.utcnow()
            self.state = DimensionAggregationState.STALLED
            return True
        return False
    
    def get_completion_percentage(self) -> float:
        """Get percentage of expected signals received"""
        return (self.received_micro_scores / self.expected_micro_scores) * 100


@dataclass
class BackpressureMonitor:
    """Monitor and handle backpressure in signal flow"""
    window_size_seconds: int = 60
    threshold_signals_per_second: float = 10.0
    
    _signal_timestamps: deque = field(default_factory=lambda: deque(maxlen=1000))
    _backpressure_active: bool = False
    _backpressure_started_at: Optional[datetime] = None
    
    def record_signal(self):
        """Record signal arrival"""
        self._signal_timestamps.append(datetime.utcnow())
    
    def check_backpressure(self) -> Tuple[bool, float]:
        """
        Check if backpressure is occurring.
        
        Returns:
            (is_backpressure, current_rate)
        """
        if len(self._signal_timestamps) < 10:
            return False, 0.0
        
        # Calculate rate in last window
        now = datetime.utcnow()
        window_start = now - timedelta(seconds=self.window_size_seconds)
        
        recent_signals = [
            ts for ts in self._signal_timestamps
            if ts > window_start
        ]
        
        if not recent_signals:
            return False, 0.0
        
        rate = len(recent_signals) / self.window_size_seconds
        
        if rate > self.threshold_signals_per_second:
            if not self._backpressure_active:
                self._backpressure_active = True
                self._backpressure_started_at = now
            return True, rate
        else:
            if self._backpressure_active:
                self._backpressure_active = False
                self._backpressure_started_at = None
            return False, rate
    
    def get_backpressure_duration(self) -> Optional[float]:
        """Get duration of current backpressure in seconds"""
        if self._backpressure_active and self._backpressure_started_at:
            return (datetime.utcnow() - self._backpressure_started_at).total_seconds()
        return None


@dataclass
class CrossDimensionalCorrelation:
    """Track correlations across dimension boundaries"""
    dimension_pair: Tuple[str, str]
    shared_signals: Set[str] = field(default_factory=set)
    correlation_strength: float = 0.0
    causality_direction: Optional[str] = None  # "forward", "backward", "bidirectional"
    last_update: datetime = field(default_factory=datetime.utcnow)
    
    # Configuration constants for correlation strength calculation
    MAX_SIGNALS_FOR_FULL_CORRELATION: int = 50  # Signals needed for 1.0 correlation strength
    
    def add_shared_signal(self, signal_id: str):
        """
        Add signal that affects both dimensions.
        
        Correlation strength increases linearly with shared signals,
        reaching 1.0 at MAX_SIGNALS_FOR_FULL_CORRELATION (50 signals).
        """
        self.shared_signals.add(signal_id)
        self.last_update = datetime.utcnow()
        self.correlation_strength = min(
            1.0,
            len(self.shared_signals) / self.MAX_SIGNALS_FOR_FULL_CORRELATION
        )


@dataclass
class Phase4DimensionConsumerConfig:
    """Configuration for Phase4DimensionConsumer."""

    enabled_signal_types: List[str] = field(default_factory=list)
    process_signals_asynchronously: bool = False
    max_batch_size: int = 100
    enable_backpressure_monitoring: bool = True
    enable_cross_dimensional_analysis: bool = True
    stall_detection_threshold_seconds: int = 300
    expected_dimensions: int = 60  # Phase 4 produces 60 dimension scores

    def __post_init__(self):
        if not self.enabled_signal_types:
            self.enabled_signal_types = [
                'DIMENSION_AGGREGATION',
                'D1_SCORE', 'D2_SCORE', 'D3_SCORE', 'D4_SCORE', 'D5_SCORE', 'D6_SCORE',
                'MICRO_SCORE',  # Input from Phase 3
                'MESO_AGGREGATION'  # Output orchestration
            ]

class Phase4DimensionConsumer(BaseConsumer):
    """
    SOPHISTICATED Dimension Aggregation Consumer for Phase 4
    
    Implements advanced dimension aggregation (300→60) with:
    - Real-time dimension score tracking across 60 dimensions
    - Signal flow monitoring and backpressure detection
    - Cross-dimensional causality analysis
    - Adaptive synchronization checkpoints
    - Stall detection and recovery
    - Multi-signal correlation for dimension integrity

    Signal Types:
        - DIMENSION_AGGREGATION: Dimension-level aggregation signals
        - D1-D6_SCORE: Individual dimension scores  
        - MICRO_SCORE: Input micro-question scores (300 total)
        - MESO_AGGREGATION: Orchestration signals for dimension rollup
    """

    def __init__(self, config: Phase4DimensionConsumerConfig = None):
        """
        Initialize sophisticated Phase4DimensionConsumer.

        Args:
            config: Configuration for this consumer
        """
        self.config = config or Phase4DimensionConsumerConfig()
        self.consumer_id = "phase4_dimension_consumer_enhanced"
        self.subscribed_signal_types = self.config.enabled_signal_types
        self._signal_buffer: List[Signal] = []
        
        # Sophisticated enhancement components
        self._logger = logging.getLogger(f"SISAS.{self.consumer_id}")
        self._dimension_tracking: Dict[str, DimensionScoreTracking] = {}
        self._backpressure_monitor = BackpressureMonitor()
        self._cross_dimensional_correlations: Dict[Tuple[str, str], CrossDimensionalCorrelation] = {}
        self._synchronization_checkpoints: List[Dict[str, Any]] = []
        self._signal_flow_metrics: Dict[str, Any] = defaultdict(int)
        
        # Performance tracking
        self._metrics = {
            "total_signals_consumed": 0,
            "dimensions_completed": 0,
            "dimensions_stalled": 0,
            "backpressure_events": 0,
            "cross_dimensional_correlations_detected": 0,
            "synchronization_checkpoints_created": 0,
            "avg_dimension_completion_time_seconds": 0.0
        }
        
        # Initialize tracking for all expected dimensions
        # Phase 4: 300 micro-questions distributed across 60 dimensions = 5 questions per dimension
        questions_per_dimension = 300 // self.config.expected_dimensions
        for dim_idx in range(1, self.config.expected_dimensions + 1):
            dim_id = f"DIM{dim_idx:02d}"
            self._dimension_tracking[dim_id] = DimensionScoreTracking(
                dimension_id=dim_id,
                expected_micro_scores=questions_per_dimension
            )
        
        self._logger.info(
            f"Initialized sophisticated dimension consumer: "
            f"{self.config.expected_dimensions} dimensions, "
            f"backpressure_monitoring={self.config.enable_backpressure_monitoring}, "
            f"cross_dimensional_analysis={self.config.enable_cross_dimensional_analysis}"
        )

    def consume(self, signal: Signal) -> Optional[Dict[str, Any]]:
        """
        SOPHISTICATED signal consumption with multi-stage intelligence.
        
        Processing stages:
        1. Backpressure monitoring and handling
        2. Dimension tracking and state management
        3. Cross-dimensional correlation detection
        4. Synchronization checkpoint creation
        5. Stall detection and recovery
        6. Signal flow metrics collection
        """
        start_time = time.time()
        
        if signal.signal_type not in self.subscribed_signal_types:
            return None
        
        try:
            # Stage 1: Monitor backpressure
            self._backpressure_monitor.record_signal()
            is_backpressure, current_rate = self._backpressure_monitor.check_backpressure()
            
            if is_backpressure:
                self._metrics["backpressure_events"] += 1
                self._logger.warning(
                    f"Backpressure detected: {current_rate:.2f} signals/sec "
                    f"(threshold: {self._backpressure_monitor.threshold_signals_per_second})"
                )
                # Apply throttling strategy
                result = self._handle_backpressure(signal, current_rate)
                if result:
                    return result
            
            # Stage 2: Process signal and update dimension tracking
            result = self._process_signal_with_tracking(signal)
            
            # Stage 3: Detect cross-dimensional correlations
            if self.config.enable_cross_dimensional_analysis:
                self._analyze_cross_dimensional_correlations(signal)
            
            # Stage 4: Check for synchronization checkpoint opportunities
            self._check_synchronization_checkpoint()
            
            # Stage 5: Detect and handle stalls
            self._detect_and_handle_stalls()
            
            # Stage 6: Update metrics
            processing_time_ms = (time.time() - start_time) * 1000
            self._update_flow_metrics(signal, processing_time_ms)
            
            self._metrics["total_signals_consumed"] += 1
            
            return result
            
        except Exception as e:
            self._logger.error(f"Signal consumption error: {e}", exc_info=True)
            return {
                "consumer_id": self.consumer_id,
                "signal_type": signal.signal_type,
                "signal_id": signal.signal_id,
                "processed": False,
                "error": str(e)
            }

    def _process_signal_with_tracking(self, signal: Signal) -> Dict[str, Any]:
        """
        Process signal and update dimension tracking state.
        """
        # Extract dimension info from signal
        dimension_id = self._extract_dimension_id(signal)
        
        # Base processing
        processing_result = self._process_signal(signal)
        
        # Update dimension tracking if applicable
        if dimension_id and dimension_id in self._dimension_tracking:
            tracking = self._dimension_tracking[dimension_id]
            tracking.update_progress(signal.signal_id)
            
            # Add tracking info to result
            processing_result["dimension_tracking"] = {
                "dimension_id": dimension_id,
                "progress_percentage": tracking.get_completion_percentage(),
                "state": tracking.state.value,
                "received_scores": tracking.received_micro_scores,
                "expected_scores": tracking.expected_micro_scores
            }
            
            # Check if dimension aggregation is complete
            if tracking.state == DimensionAggregationState.READY:
                self._complete_dimension_aggregation(dimension_id)
        
        return {
            "consumer_id": self.consumer_id,
            "signal_type": signal.signal_type,
            "signal_id": signal.signal_id,
            "processed": True,
            "result": processing_result,
        }

    def _handle_backpressure(self, signal: Signal, current_rate: float) -> Optional[Dict[str, Any]]:
        """
        Handle backpressure by buffering or throttling.
        
        Strategy:
        - Buffer non-critical signals
        - Apply adaptive throttling based on rate
        - Log backpressure duration
        """
        backpressure_duration = self._backpressure_monitor.get_backpressure_duration()
        
        if backpressure_duration and backpressure_duration > 60:
            self._logger.error(
                f"Sustained backpressure for {backpressure_duration:.0f}s - "
                f"signal flow may be stalled"
            )
        
        # Buffer signal if rate is excessive
        if current_rate > self._backpressure_monitor.threshold_signals_per_second * 1.5:
            self._signal_buffer.append(signal)
            return {
                "consumer_id": self.consumer_id,
                "signal_id": signal.signal_id,
                "processed": False,
                "buffered": True,
                "reason": "backpressure_throttling",
                "current_rate": current_rate
            }
        
        return None  # Continue with normal processing

    def _analyze_cross_dimensional_correlations(self, signal: Signal):
        """
        Analyze correlations across dimension boundaries.
        
        Detects when signals affect multiple dimensions, indicating
        cross-cutting concerns or cascading effects.
        """
        dimension_id = self._extract_dimension_id(signal)
        if not dimension_id:
            return
        
        # Check for signals that reference multiple dimensions
        signal_metadata = getattr(signal, 'metadata', {})
        related_dimensions = signal_metadata.get('related_dimensions', [])
        
        if related_dimensions:
            for related_dim in related_dimensions:
                if related_dim == dimension_id:
                    continue
                
                dim_pair = tuple(sorted([dimension_id, related_dim]))
                
                if dim_pair not in self._cross_dimensional_correlations:
                    self._cross_dimensional_correlations[dim_pair] = CrossDimensionalCorrelation(
                        dimension_pair=dim_pair
                    )
                
                correlation = self._cross_dimensional_correlations[dim_pair]
                correlation.add_shared_signal(signal.signal_id)
                
                self._metrics["cross_dimensional_correlations_detected"] += 1
                
                self._logger.debug(
                    f"Cross-dimensional correlation: {dim_pair} - "
                    f"{len(correlation.shared_signals)} shared signals, "
                    f"strength: {correlation.correlation_strength:.2f}"
                )

    def _check_synchronization_checkpoint(self):
        """
        Check if synchronization checkpoint should be created.
        
        Checkpoints mark significant progress in dimension aggregation
        and enable recovery if processing is interrupted.
        """
        # Create checkpoint every 10 completed dimensions
        completed_dimensions = [
            dim_id for dim_id, tracking in self._dimension_tracking.items()
            if tracking.state == DimensionAggregationState.COMPLETED
        ]
        
        if len(completed_dimensions) % 10 == 0 and len(completed_dimensions) > 0:
            last_checkpoint_size = (
                self._synchronization_checkpoints[-1]["completed_count"]
                if self._synchronization_checkpoints else 0
            )
            
            if len(completed_dimensions) > last_checkpoint_size:
                checkpoint = {
                    "checkpoint_id": f"phase4_sync_{len(self._synchronization_checkpoints) + 1}",
                    "timestamp": datetime.utcnow().isoformat(),
                    "completed_dimensions": completed_dimensions.copy(),
                    "completed_count": len(completed_dimensions),
                    "total_dimensions": self.config.expected_dimensions,
                    "progress_percentage": (len(completed_dimensions) / self.config.expected_dimensions) * 100,
                    "metrics_snapshot": self._metrics.copy()
                }
                
                self._synchronization_checkpoints.append(checkpoint)
                self._metrics["synchronization_checkpoints_created"] += 1
                
                self._logger.info(
                    f"Synchronization checkpoint created: {checkpoint['checkpoint_id']} - "
                    f"{checkpoint['progress_percentage']:.1f}% complete"
                )

    def _detect_and_handle_stalls(self):
        """
        Detect stalled dimensions and attempt recovery.
        
        A dimension is stalled if it hasn't received signals within threshold.
        """
        stalled_dimensions = []
        
        for dim_id, tracking in self._dimension_tracking.items():
            if tracking.check_for_stall(self.config.stall_detection_threshold_seconds):
                stalled_dimensions.append(dim_id)
        
        if stalled_dimensions:
            self._metrics["dimensions_stalled"] = len(stalled_dimensions)
            self._logger.warning(
                f"Detected {len(stalled_dimensions)} stalled dimensions: "
                f"{', '.join(stalled_dimensions[:5])}..."
            )
            
            # Log stall details for investigation
            for dim_id in stalled_dimensions[:5]:  # Log first 5
                tracking = self._dimension_tracking[dim_id]
                self._logger.warning(
                    f"Dimension {dim_id} stalled: "
                    f"{tracking.received_micro_scores}/{tracking.expected_micro_scores} received, "
                    f"last update: {(datetime.utcnow() - tracking.last_update).total_seconds():.0f}s ago"
                )

    def _complete_dimension_aggregation(self, dimension_id: str):
        """
        Mark dimension aggregation as complete and finalize.
        """
        tracking = self._dimension_tracking[dimension_id]
        tracking.state = DimensionAggregationState.COMPLETED
        
        self._metrics["dimensions_completed"] += 1
        
        completion_time = (datetime.utcnow() - tracking.last_update).total_seconds()
        
        # Update average completion time
        completed = self._metrics["dimensions_completed"]
        current_avg = self._metrics["avg_dimension_completion_time_seconds"]
        self._metrics["avg_dimension_completion_time_seconds"] = (
            (current_avg * (completed - 1) + completion_time) / completed
        )
        
        self._logger.info(
            f"Dimension {dimension_id} aggregation complete: "
            f"{tracking.received_micro_scores} signals processed"
        )

    def _extract_dimension_id(self, signal: Signal) -> Optional[str]:
        """Extract dimension ID from signal context or metadata"""
        # Try context first
        if hasattr(signal, 'context'):
            context = signal.context
            if hasattr(context, 'node_id') and 'DIM' in str(context.node_id):
                return str(context.node_id)
            if hasattr(context, 'node_type') and context.node_type == 'dimension':
                return str(context.node_id)
        
        # Try metadata
        if hasattr(signal, 'metadata'):
            metadata = signal.metadata
            if isinstance(metadata, dict) and 'dimension_id' in metadata:
                return metadata['dimension_id']
        
        return None

    def _update_flow_metrics(self, signal: Signal, processing_time_ms: float):
        """Update signal flow metrics"""
        self._signal_flow_metrics["total_processing_time_ms"] += processing_time_ms
        self._signal_flow_metrics[f"signal_type_{signal.signal_type}"] += 1

    def _process_signal(self, signal: Signal) -> Dict[str, Any]:
        """
        Process signal based on its type.

        Args:
            signal: Signal to process

        Returns:
            Processing result
        """
        # Default processing - can be overridden for specific signal types
        return {
            "timestamp": signal.timestamp,
            "payload_size": len(str(signal.payload)),
            "metadata": signal.metadata,
        }

    def consume_batch(self, signals: List[Signal]) -> List[Dict[str, Any]]:
        """
        Consume multiple signals in batch.

        Args:
            signals: List of signals to consume

        Returns:
            List of consumption results
        """
        results = []
        for signal in signals:
            result = self.consume(signal)
            if result:
                results.append(result)
        return results

    def flush_buffer(self) -> List[Dict[str, Any]]:
        """
        Flush the internal signal buffer.

        Returns:
            List of buffered consumption results
        """
        results = []
        for signal in self._signal_buffer:
            result = self.consume(signal)
            if result:
                results.append(result)
        self._signal_buffer.clear()
        return results

    def get_consumption_contract(self) -> Dict[str, Any]:
        """
        Get the consumption contract for this consumer.

        Returns:
            Dict with contract details
        """
        return {
            "consumer_id": self.consumer_id,
            "phase": "phase4",
            "subscribed_signal_types": self.subscribed_signal_types,
            "required_capabilities": [
                "process_signal",
                "consume_batch",
                "flush_buffer",
            ],
            "config": {
                "process_signals_asynchronously": self.config.process_signals_asynchronously,
                "max_batch_size": self.config.max_batch_size,
            },
        }

    def get_status(self) -> Dict[str, Any]:
        """
        Get comprehensive status of the consumer including dimension tracking.

        Returns:
            Dict with detailed status information
        """
        # Dimension state summary
        state_summary = defaultdict(int)
        for tracking in self._dimension_tracking.values():
            state_summary[tracking.state.value] += 1
        
        # Backpressure status
        is_backpressure, current_rate = self._backpressure_monitor.check_backpressure()
        backpressure_duration = self._backpressure_monitor.get_backpressure_duration()
        
        return {
            "consumer_id": self.consumer_id,
            "status": "active",
            "buffer_size": len(self._signal_buffer),
            "subscribed_types": self.subscribed_signal_types,
            "metrics": self._metrics,
            "dimension_aggregation": {
                "total_dimensions": self.config.expected_dimensions,
                "completed": state_summary[DimensionAggregationState.COMPLETED.value],
                "collecting": state_summary[DimensionAggregationState.COLLECTING.value],
                "ready": state_summary[DimensionAggregationState.READY.value],
                "stalled": state_summary[DimensionAggregationState.STALLED.value],
                "completion_percentage": (
                    state_summary[DimensionAggregationState.COMPLETED.value] /
                    self.config.expected_dimensions * 100
                )
            },
            "backpressure": {
                "active": is_backpressure,
                "current_rate": current_rate,
                "duration_seconds": backpressure_duration,
                "total_events": self._metrics["backpressure_events"]
            },
            "synchronization": {
                "checkpoints_created": len(self._synchronization_checkpoints),
                "last_checkpoint": (
                    self._synchronization_checkpoints[-1]["checkpoint_id"]
                    if self._synchronization_checkpoints else None
                )
            },
            "cross_dimensional_analysis": {
                "enabled": self.config.enable_cross_dimensional_analysis,
                "correlations_detected": len(self._cross_dimensional_correlations),
                "total_correlation_signals": sum(
                    len(corr.shared_signals)
                    for corr in self._cross_dimensional_correlations.values()
                )
            }
        }
    
    def get_dimension_tracking_report(self) -> Dict[str, Any]:
        """
        Get detailed report on dimension tracking progress.
        
        Returns comprehensive view of all 60 dimensions.
        """
        dimensions_by_state = defaultdict(list)
        
        for dim_id, tracking in sorted(self._dimension_tracking.items()):
            dim_info = {
                "dimension_id": dim_id,
                "state": tracking.state.value,
                "progress_percentage": tracking.get_completion_percentage(),
                "received_scores": tracking.received_micro_scores,
                "expected_scores": tracking.expected_micro_scores,
                "last_update": tracking.last_update.isoformat(),
                "time_since_update_seconds": (
                    datetime.utcnow() - tracking.last_update
                ).total_seconds()
            }
            
            if tracking.stall_detected_at:
                dim_info["stalled_at"] = tracking.stall_detected_at.isoformat()
                dim_info["stall_duration_seconds"] = (
                    datetime.utcnow() - tracking.stall_detected_at
                ).total_seconds()
            
            dimensions_by_state[tracking.state.value].append(dim_info)
        
        return {
            "report_timestamp": datetime.utcnow().isoformat(),
            "total_dimensions": self.config.expected_dimensions,
            "dimensions_by_state": dict(dimensions_by_state),
            "overall_progress_percentage": (
                len(dimensions_by_state[DimensionAggregationState.COMPLETED.value]) /
                self.config.expected_dimensions * 100
            ),
            "estimated_completion_time": self._estimate_completion_time()
        }
    
    def _estimate_completion_time(self) -> Optional[str]:
        """Estimate when all dimensions will complete based on current rate"""
        completed = self._metrics["dimensions_completed"]
        if completed == 0:
            return None
        
        avg_time = self._metrics["avg_dimension_completion_time_seconds"]
        remaining = self.config.expected_dimensions - completed
        
        if avg_time > 0:
            estimated_seconds = remaining * avg_time
            estimated_completion = datetime.utcnow() + timedelta(seconds=estimated_seconds)
            return estimated_completion.isoformat()
        
        return None
    
    def get_cross_dimensional_insights(self) -> Dict[str, Any]:
        """
        Get insights about cross-dimensional correlations.
        
        Reveals hidden relationships between dimensions.
        """
        if not self.config.enable_cross_dimensional_analysis:
            return {"enabled": False}
        
        correlation_details = []
        for dim_pair, correlation in self._cross_dimensional_correlations.items():
            correlation_details.append({
                "dimension_pair": dim_pair,
                "shared_signals": len(correlation.shared_signals),
                "correlation_strength": correlation.correlation_strength,
                "causality_direction": correlation.causality_direction,
                "last_update": correlation.last_update.isoformat()
            })
        
        # Sort by correlation strength
        correlation_details.sort(key=lambda x: x["correlation_strength"], reverse=True)
        
        return {
            "enabled": True,
            "total_correlations": len(correlation_details),
            "strong_correlations": [
                c for c in correlation_details if c["correlation_strength"] > 0.7
            ],
            "all_correlations": correlation_details,
            "insights": self._generate_correlation_insights(correlation_details)
        }
    
    def _generate_correlation_insights(self, correlations: List[Dict]) -> List[str]:
        """Generate human-readable insights from correlations"""
        insights = []
        
        # Find strongest correlations
        if correlations:
            strongest = correlations[0]
            if strongest["correlation_strength"] > 0.8:
                insights.append(
                    f"Very strong correlation detected between {strongest['dimension_pair']}: "
                    f"{strongest['shared_signals']} shared signals"
                )
        
        # Find dimensions with many correlations
        dimension_correlation_count = defaultdict(int)
        for corr in correlations:
            for dim in corr["dimension_pair"]:
                dimension_correlation_count[dim] += 1
        
        if dimension_correlation_count:
            most_correlated = max(
                dimension_correlation_count.items(),
                key=lambda x: x[1]
            )
            if most_correlated[1] > 5:
                insights.append(
                    f"Dimension {most_correlated[0]} shows high cross-dimensional activity: "
                    f"{most_correlated[1]} correlations detected"
                )
        
        return insights
    
    def get_synchronization_checkpoints(self) -> List[Dict[str, Any]]:
        """Get all synchronization checkpoints created during processing"""
        return self._synchronization_checkpoints.copy()
    
    def get_signal_flow_metrics(self) -> Dict[str, Any]:
        """Get detailed signal flow metrics"""
        total_signals = self._metrics["total_signals_consumed"]
        
        return {
            "total_signals_consumed": total_signals,
            "signal_types_breakdown": {
                k.replace("signal_type_", ""): v
                for k, v in self._signal_flow_metrics.items()
                if k.startswith("signal_type_")
            },
            "avg_processing_time_ms": (
                self._signal_flow_metrics["total_processing_time_ms"] / total_signals
                if total_signals > 0 else 0.0
            ),
            "backpressure_events": self._metrics["backpressure_events"],
            "buffer_high_watermark": max(
                self._signal_flow_metrics.get("buffer_sizes", [0])
            ) if "buffer_sizes" in self._signal_flow_metrics else 0
        }
